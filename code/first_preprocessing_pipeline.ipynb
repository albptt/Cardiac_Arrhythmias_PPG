{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TN9rKvunAXF"
      },
      "source": [
        "# Workspace Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define workspace utility functions\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import subprocess\n",
        "from IPython.display import FileLink\n",
        "\n",
        "# Call this function when in need to free RAM space\n",
        "def check_variables():\n",
        "    \"\"\"\n",
        "    Check the memory usage of variables in the workspace.\n",
        "    Print variables and their memory sizes in descending order.\n",
        "    \"\"\"\n",
        "    # Get the memory size of each variable\n",
        "    variable_sizes = {k: sys.getsizeof(v) for k, v in locals().items() if not k.startswith('__')}\n",
        "    # Sort the variables based on their memory size\n",
        "    sorted_variables = sorted(variable_sizes.items(), key=lambda x: x[1], reverse=True)\n",
        "    # Print the variables and their memory sizes in descending order\n",
        "    for var, size in sorted_variables:\n",
        "        print(f\"{var}: {size} bytes\")\n",
        "\n",
        "# Save anything via pickle\n",
        "def save(item, name: str, path=\"/kaggle/working/\"):\n",
        "    \"\"\"\n",
        "    Save an item using pickle.\n",
        "\n",
        "    Parameters:\n",
        "        item: The item to be saved.\n",
        "        name (str): The name of the file.\n",
        "        path (str): The path where the file will be saved (default: \"/kaggle/working/\").\n",
        "    \"\"\"\n",
        "    item_file = path + name\n",
        "    with open(item_file, 'wb') as file:\n",
        "        pickle.dump(item, file)\n",
        "\n",
        "# Download item as zip\n",
        "def download_file(source_path: str, download_file_name: str, output_path=\"/kaggle/working/\"):\n",
        "    \"\"\"\n",
        "    Create a zip file from the specified source path and provide a download link.\n",
        "    \n",
        "    Parameters:\n",
        "        source_path (str): The path to the source file or directory to be zipped.\n",
        "        download_file_name (str): The name of the zip file and download link.\n",
        "        output_path (str): The output path for the zip file (default: \"/kaggle/working/\").\n",
        "    \"\"\"\n",
        "    # Save the current working directory\n",
        "    current_working_directory = os.getcwd()  \n",
        "    os.chdir(output_path)\n",
        "\n",
        "    try:\n",
        "        zip_name = f\"{download_file_name}.zip\"\n",
        "        command = f\"zip {zip_name} {source_path} -r\"\n",
        "        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "        if result.returncode != 0:\n",
        "            raise RuntimeError(f\"Unable to run zip command! Error: {result.stderr}\")\n",
        "\n",
        "        display(FileLink(zip_name))\n",
        "    finally:\n",
        "        # Restore the original working directory\n",
        "        os.chdir(current_working_directory)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkUs4kLvm-qB",
        "outputId": "17334bf6-2cd3-402d-8a58-6765fc8fbda2"
      },
      "outputs": [],
      "source": [
        "# List first content of `input_directory`\n",
        "import os\n",
        "\n",
        "input_directory = \"/kaggle/working/AI project/train\"\n",
        "dir_list = sorted(os.listdir(input_directory))\n",
        "dir_list[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvLzSV2NlvZF",
        "outputId": "66a108ce-a050-477d-9e78-0315fbbe8321"
      },
      "outputs": [],
      "source": [
        "from scipy.io import loadmat\n",
        "\n",
        "def load_data_from_directory(input_directory):\n",
        "    print('Loading data...')\n",
        "    # Get name of label files for fs=128Hz contained in the folder\n",
        "    beat_labeling_128 = []\n",
        "    for f in dir_list:\n",
        "        g = os.path.join(input_directory, f)\n",
        "        if not f.lower().startswith('.') and f.lower().endswith('128_ann.mat') and os.path.isfile(g):\n",
        "            beat_labeling_128.append(g)\n",
        "    num_labels_128 = len(beat_labeling_128)\n",
        "\n",
        "    # Get name of peak location files for fs=128Hz contained in the folder\n",
        "    peak_locations_128 = []\n",
        "    for f in dir_list:\n",
        "        g = os.path.join(input_directory, f)\n",
        "        if not f.lower().startswith('.') and f.lower().endswith('128_spk.mat') and os.path.isfile(g):\n",
        "            peak_locations_128.append(g)\n",
        "    num_peak_locations_128 = len(peak_locations_128)\n",
        "\n",
        "    # Get name of signals fs=128Hz contained in the folder\n",
        "    signals_128 = []\n",
        "    for f in dir_list:\n",
        "        g = os.path.join(input_directory, f)\n",
        "        if not f.lower().startswith('.') and f.lower().endswith('_128.mat') and os.path.isfile(g):\n",
        "            signals_128.append(g)\n",
        "    num_signals_128 = len(signals_128)\n",
        "\n",
        "    # Get name of label files for fs=250Hz contained in the folder\n",
        "    beat_labeling_250 = []\n",
        "    for f in dir_list:\n",
        "        g = os.path.join(input_directory, f)\n",
        "        if not f.lower().startswith('.') and f.lower().endswith('250_ann.mat') and os.path.isfile(g):\n",
        "            beat_labeling_250.append(g)\n",
        "    num_labels_250 = len(beat_labeling_250)\n",
        "\n",
        "    # Get name of peak location files for fs=250Hz contained in the folder\n",
        "    peak_locations_250 = []\n",
        "    for f in dir_list:\n",
        "        g = os.path.join(input_directory, f)\n",
        "        if not f.lower().startswith('.') and f.lower().endswith('250_spk.mat') and os.path.isfile(g):\n",
        "            peak_locations_250.append(g)\n",
        "    num_peak_locations_250 = len(peak_locations_250)\n",
        "\n",
        "    # Get name of signals fs=250Hz contained in the folder\n",
        "    signals_250 = []\n",
        "    for f in dir_list:\n",
        "        g = os.path.join(input_directory, f)\n",
        "        if not f.lower().startswith('.') and f.lower().endswith('_250.mat') and os.path.isfile(g):\n",
        "            signals_250.append(g)\n",
        "    num_signals_250 = len(signals_250)\n",
        "\n",
        "    if((num_signals_128+num_signals_250)==(num_labels_128+num_labels_250)==(num_peak_locations_128+num_peak_locations_250)):\n",
        "        # Create empty list for recordings and header files\n",
        "        recordings_128 = list()\n",
        "        recordings_250 = list()\n",
        "        labels_128 = list()\n",
        "        labels_250 = list()\n",
        "        locations_128 = list()\n",
        "        locations_250 = list()\n",
        "\n",
        "        # Load .mat, _ann.mat and _spk.mat files for each subject using the function \"load_data\"\n",
        "        for i in range(num_signals_128):\n",
        "            # load recordings\n",
        "            recording_128 = loadmat(signals_128[i])\n",
        "            recordings_128.append(recording_128['ppg'])\n",
        "            # load labels\n",
        "            label_128 = loadmat(beat_labeling_128[i])\n",
        "            labels_128.append(label_128['labels'])\n",
        "            # load locations\n",
        "            location_128 = loadmat(peak_locations_128[i])\n",
        "            locations_128.append(location_128['speaks'])\n",
        "            # inform about loading step\n",
        "            print(f\"\\rLoading fs=128Hz file: {i+1}/{num_signals_128}\")\n",
        "\n",
        "        for i in range(num_signals_250):\n",
        "            # load recordings\n",
        "            recording_250 = loadmat(signals_250[i])\n",
        "            recordings_250.append(recording_250['ppg'])\n",
        "            # load labels\n",
        "            label_250 = loadmat(beat_labeling_250[i])\n",
        "            labels_250.append(label_250['labels'])\n",
        "            # load locations\n",
        "            location_250 = loadmat(peak_locations_250[i])\n",
        "            locations_250.append(location_250['speaks'])\n",
        "            # inform about loading step\n",
        "            print(f\"\\rLoading fs=250Hz file: {i+1}/{num_signals_250}\")\n",
        "\n",
        "    else:\n",
        "        print(\"Error while reading files\")\n",
        "\n",
        "    return recordings_128, recordings_250, labels_128, labels_250, locations_128, locations_250\n",
        "\n",
        "# Call the function with the input_directory\n",
        "recordings_128, recordings_250, labels_128, labels_250, locations_128, locations_250 = load_data_from_directory(input_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPNP_Ua71Je6"
      },
      "source": [
        "# Plot label distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFlVoCDge9_f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# get counts for each label\n",
        "def calculate_label_distribution(labels):\n",
        "  tot_count_n = 0\n",
        "  tot_count_s = 0\n",
        "  tot_count_v = 0\n",
        "  for idx in range(len(labels)):\n",
        "    counts_n = np.count_nonzero(labels[idx] == 'N')\n",
        "    counts_s = np.count_nonzero(labels[idx] == 'S')\n",
        "    counts_v = np.count_nonzero(labels[idx] == 'V')\n",
        "    tot_count_n += counts_n\n",
        "    tot_count_s += counts_s\n",
        "    tot_count_v += counts_v\n",
        "  return tot_count_n, tot_count_s, tot_count_v\n",
        "\n",
        "# check label distribution in 128Hz samples\n",
        "tot_count_n_128, tot_count_s_128, tot_count_v_128 = calculate_label_distribution(labels_128)\n",
        "\n",
        "# check label distribution in 250Hz samples\n",
        "tot_count_n_250, tot_count_s_250, tot_count_v_250 = calculate_label_distribution(labels_250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV2QmQp8POKI",
        "outputId": "9f8fafb5-d5db-4fe3-a3c9-8ac082bd6cff"
      },
      "outputs": [],
      "source": [
        "# Check numerosity of classes\n",
        "print(f\"Signals 128Hz: {tot_count_n_128} N beats, {tot_count_s_128} S beats, {tot_count_v_128} V beats\")\n",
        "print(f\"Signals 250Hz: {tot_count_n_250} N beats, {tot_count_s_250} S beats, {tot_count_v_250} V beats\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check class proportions \n",
        "print(\"Tot. proportion of N beats: \", (tot_count_n_128+tot_count_n_250)/(tot_count_n_128+tot_count_n_250+tot_count_s_128+tot_count_s_250+tot_count_v_128+tot_count_v_250))\n",
        "print(\"Tot. proportion of S beats: \", (tot_count_s_128+tot_count_s_250)/(tot_count_n_128+tot_count_n_250+tot_count_s_128+tot_count_s_250+tot_count_v_128+tot_count_v_250))\n",
        "print(\"Tot. proportion of V beats: \", (tot_count_v_128+tot_count_v_250)/(tot_count_n_128+tot_count_n_250+tot_count_s_128+tot_count_s_250+tot_count_v_128+tot_count_v_250))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fYe3UJq-eHm0",
        "outputId": "22a1ccc4-d9ae-4038-ca6c-785faa6e9fec"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_label_distribution(values, title, labels=['N', 'S', 'V']):\n",
        "    plt.bar(labels, values, color=['#add8e6', '#90ee90', '#ffb6c1'])\n",
        "    plt.xlabel('Labels')\n",
        "    plt.ylabel('Counts')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Plot histogram for 128Hz\n",
        "values_128 = [tot_count_n_128, tot_count_s_128, tot_count_v_128]\n",
        "plot_label_distribution(values_128, 'Histogram of Labels for 128Hz recordings')\n",
        "\n",
        "# Plot histogram for 250Hz\n",
        "values_250 = [tot_count_n_250, tot_count_s_250, tot_count_v_250]\n",
        "plot_label_distribution(values_250, 'Histogram of Labels for 250Hz recordings')\n",
        "\n",
        "# Plot histogram for overall distribution\n",
        "values = [tot_count_n_250+tot_count_n_128, tot_count_s_250+tot_count_s_128, tot_count_v_250+tot_count_v_128]\n",
        "plot_label_distribution(values, 'Histogram of Labels for all recordings')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEweQzO8w0h4"
      },
      "source": [
        "# Remove patients having only N type beats\n",
        "\n",
        "Given the large disproportion of classes, patients showing only normal beats are removed as they only carry redundant information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x0e4zAYyHLl",
        "outputId": "370f142a-50eb-4c7b-b7ee-8ed1db4cfad3"
      },
      "outputs": [],
      "source": [
        "# Checking if there are any patient with only 'n' label in the dataset with fs=128Hz\n",
        "only_N_128 = []\n",
        "\n",
        "for idx, label in enumerate(labels_128):\n",
        "    unique_labels = set(label)\n",
        "\n",
        "    if len(unique_labels) == 1 and 'N' in unique_labels:\n",
        "        only_N_128.append(idx)\n",
        "\n",
        "if only_N_128:\n",
        "    print(\"Patients with only 'N' labels found among 128Hz recordings at indices:\", only_N_128)\n",
        "else:\n",
        "    print(\"No patients with only 'N' labels found among 128Hz recordings.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTcMvUz4yPXR",
        "outputId": "d082c5f7-caa2-4c47-df62-f61ad12fe649"
      },
      "outputs": [],
      "source": [
        "# Checking if there are any patient with only 'n' label in the dataset with fs=250Hz\n",
        "only_N_250 = []\n",
        "\n",
        "for idx, label in enumerate(labels_250):\n",
        "  unique_labels = set(label)\n",
        "\n",
        "  if len(unique_labels) == 1 and 'N' in unique_labels:\n",
        "      only_N_250.append(idx)\n",
        "\n",
        "if only_N_250:\n",
        "    print(\"Patients with only 'N' labels found among 250Hz recordings at indices:\", only_N_250)\n",
        "else:\n",
        "    print(\"No patients with only 'N' labels found among 250Hz recordings.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYyqxEch3JMg",
        "outputId": "43a5b9de-555c-4cd9-b2f8-0d920a18b112"
      },
      "outputs": [],
      "source": [
        "# Remove patients from labels, recordings and peak_locations\n",
        "locations_250 = [locations_250[i] for i in range(len(locations_250)) if i not in only_N_250]\n",
        "recordings_250 = [recordings_250[i] for i in range(len(recordings_250)) if i not in only_N_250]\n",
        "labels_250 = [labels_250[i] for i in range(len(labels_250)) if i not in only_N_250]\n",
        "\n",
        "# Check dimensionality\n",
        "print(f\"New peaks locations dim.: {len(locations_250)}\")\n",
        "print(f\"New recordings dim.: {len(recordings_250)}\")\n",
        "print(f\"New labels dim.:{len(labels_250)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEeWkFyHtV3N",
        "outputId": "f56b3d7c-1ffc-4eaf-b74b-9eef1d4a29d6"
      },
      "outputs": [],
      "source": [
        "# Check new label distribution in 250Hz samples\n",
        "tot_count_n_250, tot_count_s_250, tot_count_v_250 = calculate_label_distribution(labels_250)\n",
        "print(f\"Signals 250Hz: {tot_count_n_250} N beats, {tot_count_s_250} S beats, {tot_count_v_250} V beats\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "JRehbsYztaVX",
        "outputId": "6f480875-920d-468b-c3c4-d339ccb5675a"
      },
      "outputs": [],
      "source": [
        "# Visualize the new label distribution\n",
        "\n",
        "# Plot histogram for 250Hz\n",
        "values_250 = [tot_count_n_250, tot_count_s_250, tot_count_v_250]\n",
        "plot_label_distribution(values_250, 'Histogram of Labels for 250Hz recordings')\n",
        "\n",
        "# Plot histogram for overall distribution\n",
        "values = [tot_count_n_250+tot_count_n_128, tot_count_s_250+tot_count_s_128, tot_count_v_250+tot_count_v_128]\n",
        "plot_label_distribution(values, 'Histogram of Labels for all recordings')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZBdAfqbHMFd"
      },
      "source": [
        "# Signal Visualization\n",
        "\n",
        "A rapid signal inspection has shown the presence of many artifacts along the recordings both for 128Hz and 250Hz samples. A further inspection aimed at assessing the labels associated to the peaks in these noisy portions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuRT1bXa80Ig"
      },
      "outputs": [],
      "source": [
        "# define function to plot signals over a given time range\n",
        "def plot_signal(signal, seconds, fs, offset=0):\n",
        "  t = np.arange(offset ,offset+seconds,1/fs)\n",
        "  fig, axs = plt.subplots()\n",
        "  #axs.plot(t, signal[:len(t)], color='C0')\n",
        "  axs.plot(t, signal[offset*fs:(offset+seconds)*fs], color='C0')\n",
        "  axs.set_xlabel(\"Time [s]\")\n",
        "  axs.set_ylabel(\"Amplitude [mV]\")\n",
        "  plt.title('PPG recording')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "9wNvrDfj80K2",
        "outputId": "c8ed258b-242b-4d57-e8e2-fc604670794b"
      },
      "outputs": [],
      "source": [
        "# Show 128Hz signal\n",
        "plot_signal(recordings_128[0],20,128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "AyjUWYzB80M_",
        "outputId": "70b01365-918f-4ce5-f8aa-da53d470a3ad"
      },
      "outputs": [],
      "source": [
        "# Show 250Hz signal\n",
        "plot_signal(recordings_250[0],20,250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7OiPBuUfoHU"
      },
      "outputs": [],
      "source": [
        "# plot the signal with the corresponding peaks\n",
        "def plot_signal_with_peaks(signal, peak_locations, fs):\n",
        "    # Define the time axis\n",
        "    t = np.arange(0, len(signal) / fs, 1 / fs)\n",
        "\n",
        "    # Plot the signal\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(t, signal, color='C0', label='Signal')\n",
        "\n",
        "    # Plot the peak locations\n",
        "    peak_times = np.array(peak_locations) / fs\n",
        "    plt.scatter(peak_times, signal[peak_locations], color='red', label='Peak Locations')\n",
        "\n",
        "    # Set the x-axis label and title\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.title('Signal with Peak Locations')\n",
        "\n",
        "    # Show the legend\n",
        "    plt.legend()\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "C4yu3YRtf0tE",
        "outputId": "ba91c1ad-b0ec-44e1-c448-16960736828f"
      },
      "outputs": [],
      "source": [
        "plot_signal_with_peaks(recordings_250[0][:2000], locations_250[0][:10], 250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex6iLu3NgdNe"
      },
      "outputs": [],
      "source": [
        "# plot the signal with the labelled peaks\n",
        "def plot_signal_with_labelled_peaks(signal, peak_locations, labels, fs):\n",
        "    # Define the time axis\n",
        "    t = np.arange(0, len(signal) / fs, 1 / fs)\n",
        "\n",
        "    # Plot the signal\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(t, signal, color='blue', label='Signal')\n",
        "\n",
        "    # Plot the peak locations with different colors based on the label\n",
        "    for i, peak_loc in enumerate(peak_locations):\n",
        "        if labels[i] == 'N':\n",
        "            color = 'blue'\n",
        "        elif labels[i] == 'V':\n",
        "            color = 'red'\n",
        "        elif labels[i] == 'S':\n",
        "            color = 'green'\n",
        "        else:\n",
        "            color = 'black'\n",
        "        plt.scatter(t[peak_loc], signal[peak_loc], color=color)\n",
        "\n",
        "    # Set the x-axis label and title\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude [mV]')\n",
        "    plt.title('Signal with Labelled Peak Locations')\n",
        "    print('Legend: N = blue, V = red, S = green')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "w044dXWkgeS_",
        "outputId": "2f0cca81-cd05-4005-824c-768c64e93ad2"
      },
      "outputs": [],
      "source": [
        "plot_signal_with_labelled_peaks(recordings_128[1][:1000], locations_128[1][:10],labels_128[1][:10], 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbMd_L0sLdwA"
      },
      "source": [
        "# Signal Pre-Processing\n",
        "\n",
        "Given the different sampling frequencies, resampling is performed to equalize them. In the analysis, __downsampling__ the 250Hz signals to 128Hz is performed for three main reasons:\n",
        "1. It allows to decrease the computational complexity.\n",
        "2. It allows for lower memory requirements.\n",
        "3. The majority of signals are sampled at 128Hz.\n",
        "\n",
        "Note that when downsampling from 250Hz to 128Hz we need to ensure that the original signal does not contain frequencies above 64Hz, in accordance with the sampling theorem. For this reason, the 250Hz signals' periodograms have been analyzed and it was observed that on average most of the frequency content of the signals is contained between 0 and 3 Hz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkQyPoy0SXZe"
      },
      "outputs": [],
      "source": [
        "from scipy.signal import periodogram\n",
        "\n",
        "# plot a signal and the corresponding periodogram\n",
        "def plot_signal_and_periodogram(signal, fs):\n",
        "  frequencies, Pxx = periodogram(signal.flatten(), fs)\n",
        "\n",
        "  # Plot the signal and its periodogram\n",
        "  plt.figure(figsize=(12, 6))\n",
        "\n",
        "  if 1:\n",
        "    # Plot the signal\n",
        "    plt.subplot(2, 1, 1)\n",
        "    t = np.arange(0, len(signal) / fs, 1 / fs)\n",
        "    plt.plot(t, signal)\n",
        "    plt.title('Original Signal')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude [mV]')\n",
        "\n",
        "  # Plot the periodogram\n",
        "  plt.subplot(2, 1, 2)\n",
        "  plt.plot(frequencies, Pxx)\n",
        "  plt.title('Periodogram of the Signal')\n",
        "  plt.xlabel('Frequency (Hz)')\n",
        "  plt.ylabel('Power/Frequency (dB/Hz)')\n",
        "\n",
        "  # Limit the x-axis to 10\n",
        "  plt.xlim(0, 10)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G-mXADVmT2hx",
        "outputId": "0cfc16e1-eddf-4f98-be40-0f175a9b9eae"
      },
      "outputs": [],
      "source": [
        "# Check for 250Hz signals\n",
        "fs = 250\n",
        "for i, signal in enumerate(recordings_250):\n",
        "  print(f\"250Hz signal {i}\")\n",
        "  plot_signal_and_periodogram(signal, fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3Osg5eDW40H"
      },
      "outputs": [],
      "source": [
        "# downsample 250Hz signals to 128Hz\n",
        "from scipy.signal import resample\n",
        "\n",
        "# Define the target sampling frequency\n",
        "fs = 250\n",
        "target_fs = 128\n",
        "\n",
        "# Downsample the signals\n",
        "downsampled_signals = [resample(signal, int(len(signal) * target_fs / fs)) for signal in recordings_250]\n",
        "# Modify locations_250 to match downsampled_signals\n",
        "downsampled_locations = [np.round(location * target_fs / fs).astype(int) for location in locations_250]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "yuTwou7mk2H2",
        "outputId": "ca050d89-d513-4d60-b6d6-41088296d2ef"
      },
      "outputs": [],
      "source": [
        "# Check the downsampled signal with the corresponding peaks\n",
        "plot_signal_with_peaks(downsampled_signals[0][:2000], downsampled_locations[0][:10], 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0eb-fQeoAti"
      },
      "outputs": [],
      "source": [
        "del recordings_250"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxvPFClTSgVX"
      },
      "source": [
        "# Individual signal check\n",
        "Visualization of all the signals composing the dataset is performed to avoid including any \"outlier\", meaning recordings not showing clear waveforms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1_S9cLT2Q6wL",
        "outputId": "7375b120-d51c-4699-9cd7-a63ff0c1837e"
      },
      "outputs": [],
      "source": [
        "# Plot signals in downsampled_signals\n",
        "fs = 128\n",
        "for i, signal in enumerate(downsampled_signals):\n",
        "  print(f\"Downsampled signal {i}\")\n",
        "  plot_signal(signal, 20, fs, offset = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BAg4rAI2Q8Wc",
        "outputId": "fb143d0b-b806-4886-d9f3-3eb1ba846a2e"
      },
      "outputs": [],
      "source": [
        "# Plot signals in recordings_128\n",
        "for i, signal in enumerate(recordings_128):\n",
        "  print(f\"Original 128Hz signal {i}\")\n",
        "  plot_signal(signal, 20, fs, offset = 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETKR4LSfo0xC"
      },
      "source": [
        "## Filtering and artifact removal\n",
        "\n",
        "The periodograms also prove useful to decide the cut-off frequency of filters. Given the low frequency content of PPG signals, initially a bandpass filtering approach was used to deal with artifacts. Nevertheless, as filtering alone could not recostruct the morphology of the PPG waves in the noisy portions, a threshold-based wave removal was performed.\n",
        "\n",
        "Initially 3Hz was chosen as the cutoff frequency, then it was raised to 5Hz as this value seemed to impact less on the morphology of the non-noisy signals. The DC component of the signal is instead filtered out using 0.5 Hz as cutoff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAjkC7r-FLm-"
      },
      "outputs": [],
      "source": [
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "# Define the filter parameters\n",
        "low_cutoff_frequency = 0.5  # Set the low cutoff frequency to 0.5 Hz\n",
        "high_cutoff_frequency = 5  # Set the high cutoff frequency to 5 Hz\n",
        "nyquist_freq = 0.5 * fs  # Nyquist frequency\n",
        "fs = 128  # Set the sampling frequency to 128 Hz\n",
        "filter_order = 2  # Set the filter order\n",
        "\n",
        "# Calculate the normalized cutoff frequencies\n",
        "normalized_low_cutoff_frequency = low_cutoff_frequency / nyquist_freq\n",
        "normalized_high_cutoff_frequency = high_cutoff_frequency / nyquist_freq\n",
        "\n",
        "# Design the Butterworth bandpass filter\n",
        "b, a = butter(filter_order, [normalized_low_cutoff_frequency, normalized_high_cutoff_frequency], btype='band', analog=False, output='ba')\n",
        "\n",
        "# Recordings_128 is a list of 2D arrays\n",
        "flattened_signals_128 = [np.squeeze(signal) for signal in recordings_128]\n",
        "# Apply the filter to the flattened signals\n",
        "filtered_signals_128 = [filtfilt(b, a, signal) for signal in flattened_signals_128]\n",
        "\n",
        "# Downsampled_signals is a list of 2D arrays\n",
        "flattened_downsampled_signals = [np.squeeze(signal) for signal in downsampled_signals]\n",
        "filtered_signals_downsampled = [filtfilt(b, a, signal) for signal in flattened_downsampled_signals]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CDKyl_jz9qR"
      },
      "outputs": [],
      "source": [
        "def plot_signals_overlapped(signal_1, signal_2, seconds, fs,  offset=0):\n",
        "    t = np.arange(offset,offset+seconds,1/fs)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(t, signal_1[offset*fs:(offset+seconds)*fs], label='Signal 1', color='C0')\n",
        "    plt.plot(t, signal_2[offset*fs:(offset+seconds)*fs], label='Signal 2', color='red')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude [mV]')\n",
        "    plt.title('Signal Overlap')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OoQ9Q7fBcCYL",
        "outputId": "87f60df9-2bb3-410f-f297-57e0c0e1960d"
      },
      "outputs": [],
      "source": [
        "# show signal vs. filtered signal\n",
        "fs = 128\n",
        "for i, (signal, filtered_signal) in enumerate(zip(recordings_128,filtered_signals_128)):\n",
        "  print(f\"Original vs Filtered 128Hz signal {i}\")\n",
        "  plot_signals_overlapped(signal, filtered_signal, 20, fs, offset=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C-UVPMykVqUm",
        "outputId": "e190998c-48d6-4c58-dee8-6f77acd3583f"
      },
      "outputs": [],
      "source": [
        "# show signal vs. filtered signal\n",
        "for i, (signal, filtered_signal) in enumerate(zip(downsampled_signals,filtered_signals_downsampled)):\n",
        "  print(f\"Downsampled vs Downsampled and Filtered signal {i}\")\n",
        "  plot_signals_overlapped(signal, filtered_signal, 20, fs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSKeCcJNb4j6"
      },
      "source": [
        "## Standardization\n",
        "\n",
        "Standardization of the signals is performed in order to make them comparable across and within patients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCqu1kwaBoI2"
      },
      "outputs": [],
      "source": [
        "# Perform standardization based on the signal mean and standard deviation\n",
        "def standardize_signals(signals):\n",
        "  standardized_signals = []\n",
        "  for signal in signals:\n",
        "      mean = np.mean(signal)\n",
        "      std = np.std(signal)\n",
        "      standardized_signal = (signal - mean) / std\n",
        "      standardized_signals.append(standardized_signal)\n",
        "  return standardized_signals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5z_DjmSj80SX"
      },
      "outputs": [],
      "source": [
        "# Apply standardization to filtered signals\n",
        "standardized_128 = standardize_signals(filtered_signals_128)\n",
        "standardized_downsampled = standardize_signals(filtered_signals_downsampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg91IXmkuqBK"
      },
      "source": [
        "# Beat Segmentation and Feature Extraction\n",
        "Segmentation is performed in two distinct ways:\n",
        "1. Segmentation of individual beats\n",
        "2. Segmentation to retain 2 consecutive RR intervals\n",
        "\n",
        "In the first case, as the beats are segmented, feature extraction is performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW45-oAuXwSn"
      },
      "outputs": [],
      "source": [
        "# Define a Patient class so that train-validation-test split can be performed easily later\n",
        "class Patient:\n",
        "    def __init__(self,\n",
        "                 single_beats=None, contiguous_beats=None,\n",
        "                 mean=None, std=None, amplitude=None, peak_value=None,\n",
        "                 pre_PP=None, post_PP=None, avg_PP=None, width=None, FWHM=None,\n",
        "                 skewness=None, pre_skew=None, post_skew=None,\n",
        "                 kurtosis=None, pre_kurt=None, post_kurt=None, \n",
        "                 entropy=None, RMS=None, neg_neg_jump=None, \n",
        "                 pre_pos_pos_jump=None, post_pos_pos_jump=None,\n",
        "                 rise_time=None, fall_time=None, area=None, local_hrv=None,\n",
        "                 energy=None, dominant_frequency=None, \n",
        "                 labels=None, peak_locations=None):\n",
        "\n",
        "        # Segmented beats\n",
        "        self.single_beats = single_beats if single_beats is not None else []\n",
        "        self.contiguous_beats = contiguous_beats if contiguous_beats is not None else []\n",
        "        # Features\n",
        "        self.mean = mean if mean is not None else []\n",
        "        self.std = std if std is not None else []\n",
        "        self.amplitude = amplitude if amplitude is not None else []\n",
        "        self.peak_value = peak_value if peak_value is not None else []\n",
        "        self.pre_PP = pre_PP if pre_PP is not None else []\n",
        "        self.post_PP = post_PP if post_PP is not None else []\n",
        "        self.avg_PP = avg_PP if avg_PP is not None else []\n",
        "        self.width = width if width is not None else []\n",
        "        self.FWHM = FWHM if FWHM is not None else []\n",
        "        self.skewness = skewness if skewness is not None else []\n",
        "        self.pre_skew = pre_skew if pre_skew is not None else []\n",
        "        self.post_skew = post_skew if post_skew is not None else []\n",
        "        self.kurtosis = kurtosis if kurtosis is not None else []\n",
        "        self.pre_kurt = pre_kurt if pre_kurt is not None else []\n",
        "        self.post_kurt = post_kurt if post_kurt is not None else []\n",
        "        self.entropy = entropy if entropy is not None else []\n",
        "        self.RMS = RMS if RMS is not None else []\n",
        "        self.neg_neg_jump = neg_neg_jump if neg_neg_jump is not None else []\n",
        "        self.pre_pos_pos_jump = pre_pos_pos_jump if pre_pos_pos_jump is not None else []\n",
        "        self.post_pos_pos_jump = post_pos_pos_jump if post_pos_pos_jump is not None else []\n",
        "        self.rise_time = rise_time if rise_time is not None else []\n",
        "        self.fall_time = fall_time if fall_time is not None else []\n",
        "        self.area = area if area is not None else []\n",
        "        self.local_hrv = local_hrv if local_hrv is not None else []\n",
        "        self.energy = energy if energy is not None else []\n",
        "        self.dominant_frequency = dominant_frequency if dominant_frequency is not None else []\n",
        "        # Labels and peak locations\n",
        "        self.labels = labels if labels is not None else []\n",
        "        self.peak_locations = peak_locations if peak_locations is not None else []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFojtxipXwSn"
      },
      "outputs": [],
      "source": [
        "# Initialize the patient instances as empty lists\n",
        "NUM_PATIENTS = len(standardized_128) + len(standardized_downsampled)\n",
        "\n",
        "patient_instances = [Patient() for _ in range(NUM_PATIENTS)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIdowCUMXwSo"
      },
      "outputs": [],
      "source": [
        "# Define a function to extract single beats from a signal using a dynamic window size\n",
        "def extract_single_beats_dynamically(signal, peak_locations, start_ratio=0.35, end_ratio=0.65):\n",
        "    \"\"\"\n",
        "    Extracts beats from a signal based on the locations of the peaks.\n",
        "\n",
        "    Args:\n",
        "        signal (list): The input signal.\n",
        "        peak_locations (list): The locations of the peaks in the signal.\n",
        "        start_ratio (float, optional): The ratio of the window size to use as the starting point of the beat extraction.\n",
        "            Defaults to 0.35.\n",
        "        end_ratio (float, optional): The ratio of the window size to use as the ending point of the beat extraction.\n",
        "            Defaults to 0.65.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of beats extracted from the signal.\n",
        "        list: A list of peak positions relative to the start of the window.\n",
        "    \"\"\"\n",
        "    beats = []\n",
        "    peak_positions = []\n",
        "    for i in range(len(peak_locations)):\n",
        "        if i == 0:\n",
        "            window_size = peak_locations[i+1] - peak_locations[i]\n",
        "        elif i == len(peak_locations) - 1:\n",
        "            window_size = peak_locations[i] - peak_locations[i-1]\n",
        "        else:\n",
        "            # Compute the average of the previous and the successive peak-to-peak differences\n",
        "            window_size = (peak_locations[i+1] - peak_locations[i] + peak_locations[i] - peak_locations[i-1]) / 2\n",
        "\n",
        "        start = int(max(0, peak_locations[i] - window_size*start_ratio))\n",
        "        end = int(min(len(signal), peak_locations[i] + window_size*end_ratio))\n",
        "        beat = signal[start:end]\n",
        "        beats.append(beat)\n",
        "\n",
        "    return beats, peak_positions\n",
        "\n",
        "\n",
        "# Define a function to extract single beats from a signal using a fixed window size\n",
        "def extract_single_beats_statically(signal, peak_locations, window_size=100, start_ratio=0.35, end_ratio=0.65):\n",
        "    \"\"\"\n",
        "    Extracts single beats from a given signal based on peak locations.\n",
        "\n",
        "    Args:\n",
        "        signal (array-like): The input signal.\n",
        "        peak_locations (array-like): The locations of the peaks in the signal.\n",
        "        window_size (int, optional): The size of the window around each peak to extract the beat.\n",
        "            Defaults to 80.\n",
        "        start_ratio (float, optional): The ratio of the window size to use as the starting point of the beat extraction.\n",
        "            Defaults to 0.35.\n",
        "        end_ratio (float, optional): The ratio of the window size to use as the ending point of the beat extraction.\n",
        "            Defaults to 0.65.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of extracted beats.\n",
        "        list: A list of peak positions relative to the start of the window.\n",
        "    \"\"\"\n",
        "    beats = []\n",
        "    peak_positions = []\n",
        "\n",
        "    # Segment the beats\n",
        "    for peak in peak_locations:\n",
        "        start = int(max(0, peak - window_size*start_ratio))\n",
        "        end = int(min(len(signal), peak + window_size*end_ratio))\n",
        "        beat = signal[start:end]\n",
        "        beats.append(beat)\n",
        "\n",
        "    # Calculate the relative position of the peak within the window\n",
        "        if start > 0:\n",
        "            peak_position = int(window_size * start_ratio)\n",
        "        else:\n",
        "            peak_position = peak\n",
        "        peak_positions.append(peak_position)\n",
        "\n",
        "    return beats, peak_positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEiOyQczXwSo"
      },
      "outputs": [],
      "source": [
        "# Define a function to extract contiguous beats from a signal\n",
        "def extract_contiguous_beats_dynamically(signal, peak_locations):\n",
        "    \"\"\"\n",
        "    Extracts single beats from a given signal based on peak locations.\n",
        "\n",
        "    Args:\n",
        "        signal (array-like): The input signal.\n",
        "        peak_locations (array-like): The locations of the peaks in the signal.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of extracted beats.\n",
        "\n",
        "    \"\"\"\n",
        "    beats = []\n",
        "    for i in range(len(peak_locations)):\n",
        "        if i == 0:\n",
        "            preceding_window_size = peak_locations[i]\n",
        "            succeeding_window_size = peak_locations[i+1] - peak_locations[i]\n",
        "        elif i == len(peak_locations) - 1:\n",
        "            preceding_window_size = peak_locations[i] - peak_locations[i-1]\n",
        "            succeeding_window_size = len(signal) - peak_locations[i]\n",
        "        else:\n",
        "            preceding_window_size = (peak_locations[i] - peak_locations[i-1] + peak_locations[i+1] - peak_locations[i]) / 2\n",
        "            succeeding_window_size = (peak_locations[i+1] - peak_locations[i] + peak_locations[i] - peak_locations[i-1]) / 2\n",
        "\n",
        "        start = int(max(0, peak_locations[i] - preceding_window_size))\n",
        "        end = int(min(len(signal), peak_locations[i] + succeeding_window_size))\n",
        "        beat = signal[start:end]\n",
        "        beats.append(beat)\n",
        "    return beats\n",
        "\n",
        "def extract_contiguous_beats_statically(signal, peak_locations, window_size=165, start_ratio=0.5, end_ratio=0.5):\n",
        "    \"\"\"\n",
        "    Extracts contiguous beats from a given signal based on peak locations.\n",
        "\n",
        "    Args:\n",
        "        signal (array-like): The input signal.\n",
        "        peak_locations (array-like): The locations of the peaks in the signal.\n",
        "        window_size (int, optional): The size of the window around each peak to extract the beat.\n",
        "            Defaults to 200.\n",
        "        start_ratio (float, optional): The ratio of the window size to use as the starting point of the beat extraction.\n",
        "            Defaults to 0.5.\n",
        "        end_ratio (float, optional): The ratio of the window size to use as the ending point of the beat extraction.\n",
        "            Defaults to 0.5.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of extracted beats.\n",
        "\n",
        "    \"\"\"\n",
        "    beats = []\n",
        "    for peak in peak_locations:\n",
        "        start = int(max(0, peak - window_size*start_ratio))\n",
        "        end = int(min(len(signal), peak + window_size*end_ratio))\n",
        "        beat = signal[start:end]\n",
        "        beats.append(beat)\n",
        "    return beats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TsZaHyoXwSo"
      },
      "outputs": [],
      "source": [
        "# Define a function for the visualization of beats\n",
        "def plot_beat_with_peak(beats, positions, idx=None):\n",
        "    \"\"\"\n",
        "    Plots a beat with the peak location.\n",
        "\n",
        "    Args:\n",
        "        beats (array-like): The input beats.\n",
        "        positions (array-like): The locations of the peaks in the beats.\n",
        "        idx (int, optional): The index of the beat to plot. Defaults to None.\n",
        "    \"\"\"\n",
        "    if idx is None:\n",
        "        idx = np.random.randint(len(beats))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(beats[idx])\n",
        "    plt.scatter(positions[idx], beats[idx][positions[idx]], color='red')\n",
        "    plt.xlabel('Samples')\n",
        "    plt.ylabel('Amplitude [mV]')\n",
        "    plt.title(f'Beat {idx} with Peak Location')\n",
        "    plt.show()\n",
        "\n",
        "# Define a function to plot a beat of a specific label\n",
        "def plot_beat_of_given_label(beats, labels, label, idx=None):\n",
        "    \"\"\"\n",
        "    Plots a beat of a given label.\n",
        "\n",
        "    Args:\n",
        "        beats (array-like): The input beats.\n",
        "        labels (array-like): The labels of the beats.\n",
        "        label (str): The label to plot.\n",
        "        idx (int, optional): The index of the beat to plot. Defaults to None.\n",
        "    \"\"\"\n",
        "    if idx is None:\n",
        "        idx = np.random.randint(len(beats))\n",
        "    while labels[idx] != label:\n",
        "        idx = np.random.randint(len(beats))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(beats[idx])\n",
        "    plt.xlabel('Samples')\n",
        "    plt.ylabel('Amplitude [mV]')\n",
        "    plt.title(f'Beat {idx} with Label {label}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNhUuGoRXwSo"
      },
      "outputs": [],
      "source": [
        "# Feature extraction\n",
        "def get_beat_mean(beats):\n",
        "    \"\"\"\n",
        "    Calculates the mean of the beats.\n",
        "\n",
        "    Args:\n",
        "        beats (array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        array-like: The mean of the beats.\n",
        "    \"\"\"\n",
        "    mean_array = []\n",
        "    for beat in beats:\n",
        "        mean = np.mean(beat)\n",
        "        mean_array.append(mean)\n",
        "    return mean_array\n",
        "\n",
        "def get_beat_std(beats):\n",
        "    \"\"\"\n",
        "    Calculates the standard deviation of the beats.\n",
        "\n",
        "    Args:\n",
        "        beats (array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        array-like: The standard deviation of the beats.\n",
        "    \"\"\"\n",
        "    std_array = []\n",
        "    for beat in beats:\n",
        "        std = np.std(beat)\n",
        "        std_array.append(std)\n",
        "    return std_array\n",
        "\n",
        "def get_beat_amplitude(beats):\n",
        "    \"\"\"\n",
        "    Calculates the amplitude of the beats.\n",
        "\n",
        "    Args:\n",
        "        beats (array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        array-like: The amplitude of the beats.\n",
        "    \"\"\"\n",
        "    amplitudes = []\n",
        "    for beat in beats:\n",
        "        amplitudes.append(np.max(beat) - np.min(beat))\n",
        "    return amplitudes\n",
        "\n",
        "def get_beat_peak_value(beats):\n",
        "    \"\"\"\n",
        "    Calculates the peak value of the beats.\n",
        "\n",
        "    Args:\n",
        "        beats (array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        array-like: The peak value of the beats.\n",
        "    \"\"\"\n",
        "    peak_values = []\n",
        "    for beat in beats:\n",
        "        peak_values.append(max(beat))\n",
        "    return peak_values\n",
        "\n",
        "def get_beat_pre_post_PP(peak_locations):\n",
        "    \"\"\"\n",
        "    Calculates the peak to peak distances of the beats.\n",
        "\n",
        "    Args:\n",
        "        peak_locations (array-like): The locations of the peaks in the beats.\n",
        "\n",
        "    Returns:\n",
        "        array-like: The pre-PP and post-PP of the beats in seconds.\n",
        "    \"\"\"\n",
        "    fs = 128\n",
        "    pre_PPs = []\n",
        "    post_PPs = []\n",
        "\n",
        "    for i in range(len(peak_locations)):\n",
        "        if i == 0:\n",
        "            pre_PP = None\n",
        "        else:\n",
        "            pre_PP = float(peak_locations[i] - peak_locations[i-1])/fs\n",
        "\n",
        "        if i == len(peak_locations) - 1:\n",
        "            post_PP = None\n",
        "        else:\n",
        "            post_PP = float(peak_locations[i+1] - peak_locations[i])/fs\n",
        "\n",
        "        if(pre_PP is None):\n",
        "            pre_PP = post_PP\n",
        "        elif(post_PP is None):\n",
        "            post_PP = pre_PP\n",
        "\n",
        "        pre_PPs.append(pre_PP)\n",
        "        post_PPs.append(post_PP)\n",
        "\n",
        "    return pre_PPs, post_PPs\n",
        "\n",
        "def get_beat_width(beats):\n",
        "    \"\"\"\n",
        "    Calculates the duration of the beats.\n",
        "\n",
        "    Args:\n",
        "        beats (array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        array-like: The width value of the beats in seconds.\n",
        "    \"\"\"\n",
        "    fs = 128\n",
        "    widths = []\n",
        "    for beat in beats:\n",
        "        widths.append(len(beat)/fs)\n",
        "    return widths\n",
        "\n",
        "def get_beat_FWHM(beats):\n",
        "    \"\"\"\n",
        "    Calculates the Full Width at Half Maximum (FWHM) of the beats.\n",
        "\n",
        "    Args:\n",
        "        beats (array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        array-like: The FWHM value of the beats in seconds.\n",
        "    \"\"\"\n",
        "    fs = 128\n",
        "    widths = []\n",
        "    for beat in beats:\n",
        "        max_val = np.max(beat)\n",
        "        half_max = max_val / 2.\n",
        "        indices = np.where(beat > half_max)[0]\n",
        "        if len(indices) > 0:  # Check if there are any indices found\n",
        "            fwhm = (indices[-1] - indices[0] + 1) / fs\n",
        "            widths.append(fwhm)\n",
        "        else:\n",
        "            widths.append(0)  # If no indices found, append 0\n",
        "    return widths\n",
        "\n",
        "def compute_rise_times(beats):\n",
        "    \"\"\"\n",
        "    Calculates the rise time of each beat in the list.\n",
        "\n",
        "    Args:\n",
        "        beats (list of array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        list of float: The rise times of the beats in seconds.\n",
        "    \"\"\"\n",
        "    fs = 128  # Sampling frequency\n",
        "    rise_times = []\n",
        "    for beat in beats:\n",
        "        max_val = np.max(beat)\n",
        "        max_index = np.argmax(beat)\n",
        "        low_val = 0.1 * max_val\n",
        "        high_val = 0.9 * max_val\n",
        "\n",
        "        indices = np.where((beat[:max_index] >= low_val) & (beat[:max_index] <= high_val))[0]\n",
        "        if len(indices) > 0:  # Check if there are any indices found\n",
        "            rise_time = (indices[-1] - indices[0] + 1) / fs\n",
        "            rise_times.append(rise_time)\n",
        "        else:\n",
        "            rise_times.append(0)  # If no indices found, append 0\n",
        "    return rise_times\n",
        "\n",
        "def compute_fall_times(beats):\n",
        "    \"\"\"\n",
        "    Calculates the fall time of each beat in the list.\n",
        "\n",
        "    Args:\n",
        "        beats (list of array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        list of float: The fall times of the beats in seconds.\n",
        "    \"\"\"\n",
        "    fs = 128  # Sampling frequency\n",
        "    fall_times = []\n",
        "    for beat in beats:\n",
        "        max_val = np.max(beat)\n",
        "        max_index = np.argmax(beat)\n",
        "        high_val = 0.9 * max_val\n",
        "        low_val = 0.1 * max_val\n",
        "\n",
        "        # Reverse the beat to calculate fall time\n",
        "        reversed_beat = beat[max_index:][::-1]\n",
        "        indices = np.where((reversed_beat >= low_val) & (reversed_beat <= high_val))[0]\n",
        "        if len(indices) > 0:  # Check if there are any indices found\n",
        "            fall_time = (indices[-1] - indices[0] + 1) / fs\n",
        "            fall_times.append(fall_time)\n",
        "        else:\n",
        "            fall_times.append(0)  # If no indices found, append 0\n",
        "    return fall_times\n",
        "\n",
        "def compute_negative_to_negative_peak_jump(beats):\n",
        "    \"\"\"\n",
        "    Calculates the difference between the beat onset and beat end values of each beat in the list.\n",
        "\n",
        "    Args:\n",
        "        beats (list of array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        list of float: The negative to negative peak jump.\n",
        "    \"\"\"\n",
        "    neg_jumps = []\n",
        "    for beat in beats:\n",
        "        max_index = np.argmax(beat)\n",
        "        # Split the beat into two halves\n",
        "        first_half = beat[:max_index]\n",
        "        second_half = beat[max_index:]        \n",
        "        # Find the minimum value in each half\n",
        "        min_first_half = min(first_half) if first_half.size > 0 else beat[0]\n",
        "        min_second_half = min(second_half) if second_half.size > 0 else beat[-1]\n",
        "        # Calculate the negative to negative peak jump\n",
        "        jump = min_second_half - min_first_half\n",
        "        neg_jumps.append(jump)\n",
        "\n",
        "    return neg_jumps\n",
        "\n",
        "def compute_positive_to_positive_peak_jump(beats):\n",
        "    \"\"\"\n",
        "    Calculates the difference between successive peak values for each beat in the list.\n",
        "\n",
        "    Args:\n",
        "        beats (list of array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        list of float: The negative to negative peak jump.\n",
        "    \"\"\"\n",
        "    pre_pos_jumps = []\n",
        "    post_pos_jumps = []\n",
        "    for beat in beats:\n",
        "        # Compute the peak value of the current beat\n",
        "        peak_current = np.max(beats[i])\n",
        "        # Compute the peak value of the previous beat\n",
        "        peak_prev = np.max(beats[i - 1])\n",
        "        # Compute the peak value of the next beat\n",
        "        peak_next = np.max(beats[i + 1])\n",
        "        # Compute the difference in peak values\n",
        "        pre_diff = peak_current - peak_prev\n",
        "        post_diff = peak_next - peak_current\n",
        "        \n",
        "        pre_pos_jumps.append(pre_diff)\n",
        "        post_pos_jumps.append(post_diff)\n",
        "\n",
        "    return pre_pos_jumps, post_pos_jumps\n",
        "\n",
        "def compute_areas(beats):\n",
        "    \"\"\"\n",
        "    Calculates the area under each beat in the list.\n",
        "\n",
        "    Args:\n",
        "        beats (list of array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        list of float: The areas under the beats.\n",
        "    \"\"\"\n",
        "    areas = []\n",
        "    for beat in beats:\n",
        "        area = np.trapz(beat)\n",
        "        areas.append(area)\n",
        "    return areas\n",
        "\n",
        "def compute_energy(beats):\n",
        "    \"\"\"\n",
        "    Calculates the total energy of each beat in the list.\n",
        "\n",
        "    Args:\n",
        "        beats (list of array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        list of float: The total energy of the beats.\n",
        "    \"\"\"\n",
        "    energies = []\n",
        "    for beat in beats:\n",
        "        energy = np.sum(np.square(beat))\n",
        "        energies.append(energy)\n",
        "    return energies\n",
        "\n",
        "from scipy.stats import skew\n",
        "\n",
        "def compute_skewness(beats):\n",
        "    \"\"\"\n",
        "    Calculates the skewness of each beat in the list.\n",
        "\n",
        "    Args:\n",
        "        beats (list of array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        list of float: The skewness of the beats.\n",
        "    \"\"\"\n",
        "    skewness_values = []\n",
        "    for beat in beats:\n",
        "        skewness = skew(beat)\n",
        "        skewness_values.append(skewness)\n",
        "    return skewness_values\n",
        "\n",
        "def compute_skewness_diff(beats):\n",
        "    \"\"\"\n",
        "    Calculates the difference in skewness between each beat and its previous and next beat.\n",
        "\n",
        "    Args:\n",
        "        beats (list of array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        list of tuples: Each tuple contains the difference in skewness with the previous beat and the next beat.\n",
        "    \"\"\"\n",
        "    skewness_values = [skew(beat) for beat in beats]\n",
        "    skewness_diffs_pre = []\n",
        "    skewness_diffs_post = []\n",
        "\n",
        "    for i in range(len(skewness_values)):\n",
        "        if i == 0:  # first beat, no previous beat\n",
        "            prev_diff = None\n",
        "        else:\n",
        "            prev_diff = skewness_values[i] - skewness_values[i-1]\n",
        "\n",
        "        if i == len(skewness_values) - 1:  # last beat, no next beat\n",
        "            next_diff = None\n",
        "        else:\n",
        "            next_diff = skewness_values[i] - skewness_values[i+1]\n",
        "\n",
        "        if(prev_diff is None):\n",
        "            prev_diff = next_diff\n",
        "        elif(next_diff is None):\n",
        "            next_diff = prev_diff\n",
        "        \n",
        "        skewness_diffs_pre.append(prev_diff)\n",
        "        skewness_diffs_post.append(next_diff)\n",
        "\n",
        "    return skewness_diffs_pre, skewness_diffs_post\n",
        "\n",
        "from scipy.stats import kurtosis\n",
        "\n",
        "def compute_kurtosis(beats):\n",
        "    \"\"\"\n",
        "    Calculates the kurtosis of each beat in the list.\n",
        "\n",
        "    Args:\n",
        "        beats (list of array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        list of float: The kurtosis of the beats.\n",
        "    \"\"\"\n",
        "    kurtosis_values = []\n",
        "    for beat in beats:\n",
        "        kurt = kurtosis(beat)\n",
        "        kurtosis_values.append(kurt)\n",
        "    return kurtosis_values\n",
        "\n",
        "def compute_kurtosis_diff(beats):\n",
        "    \"\"\"\n",
        "    Calculates the difference in kurtosis between each beat and its previous and next beat.\n",
        "\n",
        "    Args:\n",
        "        beats (list of array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        Two lists: Each list contains the difference in kurtosis with the previous beat and the next beat.\n",
        "    \"\"\"\n",
        "    kurtosis_values = [kurtosis(beat) for beat in beats]\n",
        "    kurtosis_diffs_pre = []\n",
        "    kurtosis_diffs_post = []\n",
        "\n",
        "    for i in range(len(kurtosis_values)):\n",
        "        if i == 0:  # first beat, no previous beat\n",
        "            prev_diff = None\n",
        "        else:\n",
        "            prev_diff = kurtosis_values[i] - kurtosis_values[i-1]\n",
        "\n",
        "        if i == len(kurtosis_values) - 1:  # last beat, no next beat\n",
        "            next_diff = None\n",
        "        else:\n",
        "            next_diff = kurtosis_values[i] - kurtosis_values[i+1]\n",
        "\n",
        "        if(prev_diff is None):\n",
        "            prev_diff = next_diff\n",
        "        elif(next_diff is None):\n",
        "            next_diff = prev_diff\n",
        "        \n",
        "        kurtosis_diffs_pre.append(prev_diff)\n",
        "        kurtosis_diffs_post.append(next_diff)\n",
        "\n",
        "    return kurtosis_diffs_pre, kurtosis_diffs_post\n",
        "\n",
        "import nolds\n",
        "\n",
        "def compute_entropy(beats):\n",
        "    \"\"\"\n",
        "    Calculates the sample entropy of each beat.\n",
        "\n",
        "    Args:\n",
        "        beats (list of array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        list of float: The sample entropy of each beat.\n",
        "    \"\"\"\n",
        "    entropy_values = []\n",
        "    for beat in beats:\n",
        "        # Compute the sample entropy of the beat\n",
        "        e = nolds.sampen(beat)\n",
        "        entropy_values.append(e)\n",
        "\n",
        "    return entropy_values\n",
        "\n",
        "def compute_rms(beats):\n",
        "    \"\"\"\n",
        "    Calculates the root mean square (RMS) of each beat in the list.\n",
        "\n",
        "    Args:\n",
        "        beats (list of array-like): The input beats.\n",
        "\n",
        "    Returns:\n",
        "        list of float: The RMS of the beats.\n",
        "    \"\"\"\n",
        "    rms_values = []\n",
        "    for beat in beats:\n",
        "        rms = np.sqrt(np.mean(np.square(beat)))\n",
        "        rms_values.append(rms)\n",
        "    return rms_values\n",
        "\n",
        "def calculate_hrv(peak_locations, window_size=4):\n",
        "    \"\"\"\n",
        "    Calculate Heart Rate Variability (HRV) for each peak location within a given window size.\n",
        "\n",
        "    Parameters:\n",
        "    peak_locations (list): List of peak locations.\n",
        "    window_size (int): The size of the window to consider for each peak. Default is 4.\n",
        "\n",
        "    Returns:\n",
        "    list: HRV measures for each peak.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize an empty list to store HRV measures\n",
        "    hrv_measures = []\n",
        "\n",
        "    # Calculate half window size for creating centered window\n",
        "    half_window = window_size // 2\n",
        "\n",
        "    # Loop over each peak location\n",
        "    for i in range(len(peak_locations)):\n",
        "        # Define the start and end of the window centered on the current peak\n",
        "        window_start = max(0, i - half_window)\n",
        "        window_end = min(len(peak_locations), i + half_window + 1)\n",
        "\n",
        "        # Get the peak locations within this window\n",
        "        window_peaks = peak_locations[window_start:window_end]\n",
        "        window_peaks_1d = window_peaks.flatten()\n",
        "        # Calculate differences between successive peaks to get PP intervals\n",
        "        rr_intervals = np.diff(window_peaks_1d)\n",
        "\n",
        "        # Calculate HRV measure for this window\n",
        "        # Here we use the standard deviation of PP intervals as the HRV measure\n",
        "        hrv = np.std(rr_intervals)\n",
        "\n",
        "        # Append the calculated HRV measure to the list\n",
        "        hrv_measures.append(hrv)\n",
        "\n",
        "    # Return the list of HRV measures\n",
        "    return hrv_measures\n",
        "\n",
        "def compute_avg_peak_to_peak_distance(peak_locations, window_size=4):\n",
        "    \"\"\"\n",
        "    Calculates the average peak-to-peak distance for each peak within a sliding window.\n",
        "\n",
        "    Args:\n",
        "        peak_locations (list of int): The locations of the peaks.\n",
        "        window_size (int, optional): The size of the sliding window. Defaults to 4.\n",
        "\n",
        "    Returns:\n",
        "        list of float: The average peak-to-peak distance for each peak.\n",
        "    \"\"\"\n",
        "    fs = 128\n",
        "    avg_PP_distances = []\n",
        "    for i in range(len(peak_locations)):\n",
        "        # Determine the start and end of the sliding window\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(i + window_size, len(peak_locations))\n",
        "\n",
        "        # Extract the peak locations within the sliding window\n",
        "        window_peaks = peak_locations[start:end]\n",
        "        window_peaks_1d = window_peaks.flatten()\n",
        "        # Compute the peak-to-peak distances\n",
        "        distances = np.diff(window_peaks_1d) / fs\n",
        "\n",
        "        # Compute the average distance\n",
        "        avg_distance = np.mean(distances) if distances.size else 0\n",
        "        avg_PP_distances.append(avg_distance)\n",
        "\n",
        "    return avg_PP_distances\n",
        "\n",
        "from scipy.fftpack import fft\n",
        "\n",
        "def compute_dominant_frequency(beats, sample_rate):\n",
        "    \"\"\"\n",
        "    Calculates the dominant frequency of each beat in the list.\n",
        "\n",
        "    Args:\n",
        "        beats (list of array-like): The input beats.\n",
        "        sample_rate (float): The sample rate of the beats.\n",
        "\n",
        "    Returns:\n",
        "        list of float: The dominant frequency of the beats.\n",
        "    \"\"\"\n",
        "    dominant_frequencies = []\n",
        "    for beat in beats:\n",
        "        # Compute FFT\n",
        "        fft_vals = fft(beat)\n",
        "\n",
        "        # Compute absolute value of FFT\n",
        "        abs_fft_vals = np.abs(fft_vals)\n",
        "\n",
        "        # Find the frequency where the absolute value of FFT is maximum\n",
        "        dominant_frequency = np.argmax(abs_fft_vals) * sample_rate / len(beat)\n",
        "        dominant_frequencies.append(dominant_frequency)\n",
        "    return dominant_frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaBVTiqEXwSp",
        "outputId": "d2498b23-c2bc-43d9-9d02-5ade37ae5c3d"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "# Extract beats and features from the signals\n",
        "print(\"Extracting beats from 128Hz signals...\")\n",
        "for i, patient_instance in tqdm.tqdm(enumerate(patient_instances[:len(standardized_128)]), total=len(standardized_128)):\n",
        "    # Extract single beats from the signal\n",
        "    single_beats, peak_locations = extract_single_beats_statically(standardized_128[i],\n",
        "                                                                   locations_128[i],\n",
        "                                                                   window_size=100)\n",
        "    # Extract contiguous beats from the signal\n",
        "    contiguous_beats = extract_contiguous_beats_statically(standardized_128[i],\n",
        "                                                            locations_128[i],\n",
        "                                                            window_size=200)\n",
        "    # Store the beats and peak locations in the patient instance\n",
        "    patient_instance.single_beats = single_beats\n",
        "    patient_instance.contiguous_beats = contiguous_beats\n",
        "    patient_instance.peak_locations = peak_locations\n",
        "    # Store the labels in the patient instance\n",
        "    patient_instance.labels = labels_128[i]\n",
        "    # Calculate the features and store them in the patient instance\n",
        "    single_beats_dynamic, peak_locations_ = extract_single_beats_dynamically(filtered_signals_128[i],\n",
        "                                                                             locations_128[i])\n",
        "    patient_instance.mean = get_beat_mean(single_beats_dynamic)\n",
        "    patient_instance.std = get_beat_std(single_beats_dynamic)\n",
        "    patient_instance.amplitude = get_beat_amplitude(single_beats_dynamic)\n",
        "    patient_instance.peak_value = get_beat_peak_value(single_beats_dynamic)\n",
        "    patient_instance.pre_PP, patient_instance.post_PP = get_beat_pre_post_PP(locations_128[i])\n",
        "    patient_instance.avg_PP = compute_avg_peak_to_peak_distance(locations_128[i])\n",
        "    patient_instance.width = get_beat_width(single_beats_dynamic)\n",
        "    patient_instance.FWHM = get_beat_FWHM(single_beats_dynamic)\n",
        "    patient_instance.rise_time = compute_rise_times(single_beats_dynamic)\n",
        "    patient_instance.fall_time = compute_fall_times(single_beats_dynamic)\n",
        "    patient_instance.area = compute_areas(single_beats_dynamic)\n",
        "    patient_instance.skewness = compute_skewness(single_beats_dynamic)\n",
        "    patient_instance.pre_skew, patient_instance.post_skew = compute_skewness_diff(single_beats_dynamic)\n",
        "    patient_instance.kurtosis = compute_kurtosis(single_beats_dynamic)\n",
        "    patient_instance.pre_kurt, patient_instance.post_kurt = compute_kurtosis_diff(single_beats_dynamic)\n",
        "    patient_instance.entropy = compute_entropy(single_beats_dynamic)\n",
        "    patient_instance.RMS = compute_rms(single_beats_dynamic)\n",
        "    patient_instance.neg_neg_jump = compute_negative_to_negative_peak_jump(single_beats_dynamic)\n",
        "    patient_instance.pre_pos_pos_jump, patient_instance.post_pos_pos_jump = compute_positive_to_positive_peak_jump(single_beats_dynamic)\n",
        "    patient_instance.local_hrv = calculate_hrv(locations_128[i])\n",
        "    patient_instance.energy = compute_energy(single_beats_dynamic)\n",
        "    patient_instance.dominant_frequency = compute_dominant_frequency(single_beats_dynamic, 128)\n",
        "\n",
        "\n",
        "print(\"Extracting beats from downsampled 250Hz signals...\")\n",
        "for i, patient_instance in tqdm.tqdm(enumerate(patient_instances[len(standardized_128):]), total=len(standardized_downsampled)):\n",
        "    # Extract single beats from the signal\n",
        "    single_beats, peak_locations = extract_single_beats_statically(standardized_downsampled[i],\n",
        "                                                                   downsampled_locations[i],\n",
        "                                                                   window_size=100)\n",
        "    # Extract also beats form non-standardized signals to be used for feature extraction\n",
        "    # single_beats_non_standardized, peak_locations_non_standardized = extract_single_beats_statically(filtered_signals_downsampled[i],\n",
        "    #                                                                                                  downsampled_locations[i],\n",
        "    #                                                                                                  window_size=100)\n",
        "    # Extract contiguous beats from the signal\n",
        "    contiguous_beats = extract_contiguous_beats_statically(standardized_downsampled[i],\n",
        "                                                            downsampled_locations[i],\n",
        "                                                            window_size=200)\n",
        "    # Store the beats and peak locations in the patient instance\n",
        "    patient_instance.single_beats = single_beats\n",
        "    patient_instance.contiguous_beats = contiguous_beats\n",
        "    patient_instance.peak_locations = peak_locations\n",
        "    # Store the labels in the patient instance\n",
        "    patient_instance.labels = labels_250[i]\n",
        "    # Calculate the features and store them in the patient instance\n",
        "    single_beats_dynamic, peak_locations_ = extract_single_beats_dynamically(filtered_signals_downsampled[i],\n",
        "                                                                             downsampled_locations[i])\n",
        "    patient_instance.mean = get_beat_mean(single_beats_dynamic)\n",
        "    patient_instance.std = get_beat_std(single_beats_dynamic)\n",
        "    patient_instance.amplitude = get_beat_amplitude(single_beats_dynamic)\n",
        "    patient_instance.peak_value = get_beat_peak_value(single_beats_dynamic)\n",
        "    patient_instance.pre_PP, patient_instance.post_PP = get_beat_pre_post_PP(downsampled_locations[i])\n",
        "    patient_instance.avg_PP = compute_avg_peak_to_peak_distance(downsampled_locations[i])\n",
        "    patient_instance.width = get_beat_width(single_beats_dynamic)\n",
        "    patient_instance.FWHM = get_beat_FWHM(single_beats_dynamic)\n",
        "    patient_instance.rise_time = compute_rise_times(single_beats_dynamic)\n",
        "    patient_instance.fall_time = compute_fall_times(single_beats_dynamic)\n",
        "    patient_instance.area = compute_areas(single_beats_dynamic)\n",
        "    patient_instance.skewness = compute_skewness(single_beats_dynamic)\n",
        "    patient_instance.pre_skew, patient_instance.post_skew = compute_skewness_diff(single_beats_dynamic)\n",
        "    patient_instance.kurtosis = compute_kurtosis(single_beats_dynamic)\n",
        "    patient_instance.pre_kurt, patient_instance.post_kurt = compute_kurtosis_diff(single_beats_dynamic)\n",
        "    patient_instance.entropy = compute_entropy(single_beats_dynamic)\n",
        "    patient_instance.RMS = compute_rms(single_beats_dynamic)\n",
        "    patient_instance.neg_neg_jump = compute_negative_to_negative_peak_jump(single_beats_dynamic)\n",
        "    patient_instance.pre_pos_pos_jump, patient_instance.post_pos_pos_jump = compute_positive_to_positive_peak_jump(single_beats_dynamic)\n",
        "    patient_instance.local_hrv = calculate_hrv(downsampled_locations[i])\n",
        "    patient_instance.energy = compute_energy(single_beats_dynamic)\n",
        "    patient_instance.dominant_frequency = compute_dominant_frequency(single_beats_dynamic, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcuxJbMLXwSp"
      },
      "outputs": [],
      "source": [
        "# Plot beat of label N\n",
        "plot_beat_of_given_label(patient_instances[0].single_beats, patient_instances[0].labels, 'N')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHEB4_dEXwSp"
      },
      "outputs": [],
      "source": [
        "# Plot beat of label S\n",
        "plot_beat_of_given_label(patient_instances[0].single_beats, patient_instances[0].labels, 'S')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCqZYhv0XwSq"
      },
      "outputs": [],
      "source": [
        "# Plot beat of label V\n",
        "plot_beat_of_given_label(patient_instances[0].single_beats, patient_instances[0].labels, 'V')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwcbppTgXwSq"
      },
      "source": [
        "# Beat Cleaning\n",
        "The goal is to remove the noisy beats belonging to class 'N' and to reconstruct those belonging to classes 'S' and 'V' in order to maximize the amount of data available for the minority classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsNAoqHoXwSq"
      },
      "outputs": [],
      "source": [
        "# Inspect beats based on amplitude threshold\n",
        "def get_noisy_beats(patients, threshold=1.5):\n",
        "    \"\"\"\n",
        "    Gets the indices of the beats that are considered noisy.\n",
        "\n",
        "    Args:\n",
        "        patients (list): The list of patient instances.\n",
        "        threshold (float, optional): The threshold to use to determine if a beat is noisy. Defaults to 1.5.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of indices of noisy beats.\n",
        "    \"\"\"\n",
        "    noisy_beats = []\n",
        "    for i, patient in enumerate(patients):\n",
        "        for j in range(len(patient.single_beats)):\n",
        "            if (np.max(patient.single_beats[j])-np.min(patient.single_beats[j])) > threshold:\n",
        "                noisy_beats.append((i, j))\n",
        "    return noisy_beats\n",
        "\n",
        "# Plot noisy beats\n",
        "def plot_noisy_beats(patients, noisy_beats, num_beats=5):\n",
        "    \"\"\"\n",
        "    Plots the noisy beats.\n",
        "\n",
        "    Args:\n",
        "        patients (list): The list of patient instances.\n",
        "        noisy_beats (list): The list of indices of noisy beats.\n",
        "        num_beats (int, optional): The number of beats to plot. Defaults to 5.\n",
        "    \"\"\"\n",
        "    for i, j in noisy_beats[:num_beats]:\n",
        "        plot_beat_with_peak(patients[i].single_beats, patients[i].peak_locations, idx=j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NJ1fzfbXwSq"
      },
      "outputs": [],
      "source": [
        "# Get the indices of the noisy beats\n",
        "noisy_beats = get_noisy_beats(patient_instances)\n",
        "# Check noisy beats labels\n",
        "check_noisy_beats_labels = [patient_instances[i].labels[j] for i, j in noisy_beats]\n",
        "unique, counts = np.unique(check_noisy_beats_labels, return_counts=True)\n",
        "print(f\"Labels of noisy beats: {dict(zip(unique, counts))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk6-dtTCXwSq"
      },
      "outputs": [],
      "source": [
        "# Compute label distribution\n",
        "tot_count_n, tot_count_s, tot_count_v = calculate_label_distribution([patient.labels for patient in patient_instances])\n",
        "print(f\"Label Distribution: {tot_count_n} N beats, {tot_count_s} S beats, {tot_count_v} V beats\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0Ya9yugXwSr"
      },
      "outputs": [],
      "source": [
        "# Separate beats based on label\n",
        "noisy_beats_N = [(i,j) for i, j in noisy_beats if patient_instances[i].labels[j] == 'N']\n",
        "noisy_beats_S = [(i,j) for i, j in noisy_beats if patient_instances[i].labels[j] == 'S']\n",
        "noisy_beats_V = [(i,j) for i, j in noisy_beats if patient_instances[i].labels[j] == 'V']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8bO4KxYXwSr"
      },
      "outputs": [],
      "source": [
        "# Visualize a batch of N noisy beats\n",
        "plot_noisy_beats(patient_instances, noisy_beats_N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-zjCjVyXwSr"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def group_noisy_beats(noisy_beats_N):\n",
        "    \"\"\"\n",
        "    Groups the noisy beats based on the first index.\n",
        "\n",
        "    Args:\n",
        "        noisy_beats_N (list of tuples): The noisy beats to group.\n",
        "\n",
        "    Returns:\n",
        "        list of list: The grouped noisy beats.\n",
        "    \"\"\"\n",
        "    groups = defaultdict(list)\n",
        "    for beat in noisy_beats_N:\n",
        "        groups[beat[0]].append(beat)\n",
        "\n",
        "    return list(groups.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFvXtWUZXwSr"
      },
      "outputs": [],
      "source": [
        "# Group N noisy beats by patients\n",
        "grouped_noisy_beats_N = group_noisy_beats(noisy_beats_N)\n",
        "noisy_N_list = []\n",
        "for group in grouped_noisy_beats_N:\n",
        "    noisy_N_list.append([item[1] for item in group])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ohhzCdeXwSs"
      },
      "outputs": [],
      "source": [
        "# Check if noisy beats have been found for each patient\n",
        "patient_indexes = []\n",
        "for group in grouped_noisy_beats_N:\n",
        "    idx = [item[0] for item in group]\n",
        "    unique_idx = set(idx)\n",
        "    if unique_idx not in patient_indexes:\n",
        "        patient_indexes.append(unique_idx)\n",
        "\n",
        "print(f\"Patient indexes: {patient_indexes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsvxlkuiXwSs"
      },
      "outputs": [],
      "source": [
        "# Remove noisy beats labelled as 'N'\n",
        "for i, patient in enumerate(patient_instances):\n",
        "    patient.single_beats = [beat for j, beat in enumerate(patient.single_beats) if j not in noisy_N_list[i]]\n",
        "    # Note that in this way the noisy beats is removed from the contiguous beats as well\n",
        "    # Yet the previous and the following beats to the noisy one are not removed (this may need to be done)\n",
        "    patient.contiguous_beats = [beat for j, beat in enumerate(patient.contiguous_beats) if j not in noisy_N_list[i]]\n",
        "    patient.mean = [mean for j, mean in enumerate(patient.mean) if j not in noisy_N_list[i]]\n",
        "    patient.std = [std for j, std in enumerate(patient.std) if j not in noisy_N_list[i]]\n",
        "    patient.amplitude = [amplitude for j, amplitude in enumerate(patient.amplitude) if j not in noisy_N_list[i]]\n",
        "    patient.peak_value = [peak_value for j, peak_value in enumerate(patient.peak_value) if j not in noisy_N_list[i]]\n",
        "    patient.pre_PP = [pre_PP for j, pre_PP in enumerate(patient.pre_PP) if j not in noisy_N_list[i]]\n",
        "    patient.post_PP = [post_PP for j, post_PP in enumerate(patient.post_PP) if j not in noisy_N_list[i]]\n",
        "    patient.avg_PP = [avg_PP for j, avg_PP in enumerate(patient.avg_PP) if j not in noisy_N_list[i]]\n",
        "    patient.width = [width for j, width in enumerate(patient.width) if j not in noisy_N_list[i]]\n",
        "    patient.labels = [label for j, label in enumerate(patient.labels) if j not in noisy_N_list[i]]\n",
        "    patient.peak_locations = [peak_location for j, peak_location in enumerate(patient.peak_locations) if j not in noisy_N_list[i]]\n",
        "    patient.FWHM = [FWHM for j, FWHM in enumerate(patient.FWHM) if j not in noisy_N_list[i]]\n",
        "    patient.skewness = [skewness for j, skewness in enumerate(patient.skewness) if j not in noisy_N_list[i]]\n",
        "    patient.pre_skew = [pre_skew for j, pre_skew in enumerate(patient.pre_skew) if j not in noisy_N_list[i]]\n",
        "    patient.post_skew = [post_skew for j, post_skew in enumerate(patient.post_skew) if j not in noisy_N_list[i]]\n",
        "    patient.kurtosis = [kurtosis for j, kurtosis in enumerate(patient.kurtosis) if j not in noisy_N_list[i]]\n",
        "    patient.pre_kurt = [pre_kurt for j, pre_kurt in enumerate(patient.pre_kurt) if j not in noisy_N_list[i]]\n",
        "    patient.post_kurt = [post_kurt for j, post_kurt in enumerate(patient.post_kurt) if j not in noisy_N_list[i]]\n",
        "    patient.entropy = [entropy for j, entropy in enumerate(patient.entropy) if j not in noisy_N_list[i]]\n",
        "    patient.RMS = [RMS for j, RMS in enumerate(patient.RMS) if j not in noisy_N_list[i]]\n",
        "    patient.neg_neg_jump = [neg_neg_jump for j, neg_neg_jump in enumerate(patient.neg_neg_jump) if j not in noisy_N_list[i]]\n",
        "    patient.pre_pos_pos_jump = [pre_pos_pos_jump for j, pre_pos_pos_jump in enumerate(patient.pre_pos_pos_jump) if j not in noisy_N_list[i]]\n",
        "    patient.post_pos_pos_jump = [post_pos_pos_jump for j, post_pos_pos_jump in enumerate(patient.post_pos_pos_jump) if j not in noisy_N_list[i]]\n",
        "    patient.rise_time = [rise_time for j, rise_time in enumerate(patient.rise_time) if j not in noisy_N_list[i]]\n",
        "    patient.fall_time = [fall_time for j, fall_time in enumerate(patient.fall_time) if j not in noisy_N_list[i]]\n",
        "    patient.area = [area for j, area in enumerate(patient.area) if j not in noisy_N_list[i]]\n",
        "    patient.local_hrv = [local_hrv for j, local_hrv in enumerate(patient.local_hrv) if j not in noisy_N_list[i]]\n",
        "    patient.energy = [energy for j, energy in enumerate(patient.energy) if j not in noisy_N_list[i]]\n",
        "    patient.dominant_frequency = [dominant_frequency for j, dominant_frequency in enumerate(patient.dominant_frequency) if j not in noisy_N_list[i]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGvF6QLaXwSs"
      },
      "outputs": [],
      "source": [
        "# Define a function to check class distribution\n",
        "def calculate_class_distribution(patient_instances):\n",
        "    \"\"\"\n",
        "    Calculates the class distribution of the labels.\n",
        "\n",
        "    Args:\n",
        "        patient_instances (list): The list of patient instances.\n",
        "    \"\"\"\n",
        "    tot_count_n = 0\n",
        "    tot_count_s = 0\n",
        "    tot_count_v = 0\n",
        "    for patient in patient_instances:\n",
        "        count_n, count_s, count_v = calculate_label_distribution(patient.labels)\n",
        "        tot_count_n += count_n\n",
        "        tot_count_s += count_s\n",
        "        tot_count_v += count_v\n",
        "    print(f\"Label Distribution: {tot_count_n} N beats, {tot_count_s} S beats, {tot_count_v} V beats\")\n",
        "\n",
        "# Check class distribution\n",
        "calculate_class_distribution(patient_instances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_mmQ3wJXwSs"
      },
      "source": [
        "## Minority Classes Noisy Beats Inspection\n",
        "\n",
        "As N noisy beats have been removed indexes for the S and V noisy beats must be re-computed.\n",
        "In this subsection the goal is to try to reconstruct the beats associated to the minority class labels via autoencoders.\n",
        "As some of the beats show labelled peak positions far from the actual peak, this discrepancy must be taken into account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuYdG3h9XwSs"
      },
      "outputs": [],
      "source": [
        "# Define a function to check if the peak position is correct\n",
        "def check_peak_position(patients, threshold=10):\n",
        "    \"\"\"\n",
        "    Checks if the peak position is correct.\n",
        "\n",
        "    Args:\n",
        "        patients (list): The list of patient instances.\n",
        "        threhshold (int, optional): The threshold to use to determine if the peak position is correct. Defaults to 5.\n",
        "    Returns:\n",
        "        list: A list of incorrect peak positions.\n",
        "        Each element of the list is a tuple containing the patient index and the incorrect peak positions.\n",
        "    \"\"\"\n",
        "    incorrect_peak_positions = []\n",
        "    for patient_id, patient in enumerate(patients):\n",
        "        for beat_id, (beat, peak_location)in enumerate(zip(patient.single_beats, patient.peak_locations)):\n",
        "            peak_pos = np.argmax(beat)\n",
        "            # Check if the peak is at the beginning or at the end of the beat\n",
        "            if peak_pos in [0, len(beat)-1]:\n",
        "                continue\n",
        "            # Check if the peak is correct\n",
        "            if not (peak_pos - threshold <= peak_location <= peak_pos + threshold):\n",
        "                incorrect_peak_positions.append((patient_id, beat_id))\n",
        "\n",
        "    return incorrect_peak_positions\n",
        "\n",
        "# Check incorrect peak positions\n",
        "incorrect_peak_positions = check_peak_position(patient_instances)\n",
        "# Check dimensionality of mislabelled peaks.\n",
        "# Note that many, but not necessarily all of these will correspond to noisy beats\n",
        "print(f\"Num. mislabelled peaks: {len(incorrect_peak_positions)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1f2oz2KXwSs"
      },
      "outputs": [],
      "source": [
        "# Group incorrect_peak_positions by patients\n",
        "grouped_incorrect_peak_positions = group_noisy_beats(incorrect_peak_positions)\n",
        "incorrect_peak_positions_list = []\n",
        "for group in grouped_incorrect_peak_positions:\n",
        "    incorrect_peak_positions_list.append([item[1] for item in group])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjHd8bANXwSt"
      },
      "outputs": [],
      "source": [
        "# Check if incorrect_peak_positions have been found for each patient\n",
        "patient_indexes = []\n",
        "for group in grouped_incorrect_peak_positions:\n",
        "    idx = [item[0] for item in group]\n",
        "    unique_idx = set(idx)\n",
        "    if unique_idx not in patient_indexes:\n",
        "        patient_indexes.append(unique_idx)\n",
        "\n",
        "print(f\"Patient indexes: {patient_indexes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79T1d1r7XwSt"
      },
      "outputs": [],
      "source": [
        "# Remove mislabelled peaks\n",
        "for i, patient in enumerate(patient_instances):\n",
        "    patient.single_beats = [beat for j, beat in enumerate(patient.single_beats) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.contiguous_beats = [beat for j, beat in enumerate(patient.contiguous_beats) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.mean = [mean for j, mean in enumerate(patient.mean) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.std = [std for j, std in enumerate(patient.std) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.amplitude = [amplitude for j, amplitude in enumerate(patient.amplitude) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.peak_value = [peak_value for j, peak_value in enumerate(patient.peak_value) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.pre_PP = [pre_PP for j, pre_PP in enumerate(patient.pre_PP) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.post_PP = [post_PP for j, post_PP in enumerate(patient.post_PP) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.avg_PP = [avg_PP for j, avg_PP in enumerate(patient.avg_PP) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.width = [width for j, width in enumerate(patient.width) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.labels = [label for j, label in enumerate(patient.labels) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.peak_locations = [peak_location for j, peak_location in enumerate(patient.peak_locations) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.FWHM = [FWHM for j, FWHM in enumerate(patient.FWHM) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.skewness = [skewness for j, skewness in enumerate(patient.skewness) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.pre_skew = [pre_skew for j, pre_skew in enumerate(patient.pre_skew) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.post_skew = [post_skew for j, post_skew in enumerate(patient.post_skew) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.kurtosis = [kurtosis for j, kurtosis in enumerate(patient.kurtosis) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.pre_kurt = [pre_kurt for j, pre_kurt in enumerate(patient.pre_kurt) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.post_kurt = [post_kurt for j, post_kurt in enumerate(patient.post_kurt) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.entropy = [entropy for j, entropy in enumerate(patient.entropy) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.RMS = [RMS for j, RMS in enumerate(patient.RMS) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.neg_neg_jump = [neg_neg_jump for j, neg_neg_jump in enumerate(patient.neg_neg_jump) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.pre_pos_pos_jump = [pre_pos_pos_jump for j, pre_pos_pos_jump in enumerate(patient.pre_pos_pos_jump) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.post_pos_pos_jump = [post_pos_pos_jump for j, post_pos_pos_jump in enumerate(patient.post_pos_pos_jump) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.rise_time = [rise_time for j, rise_time in enumerate(patient.rise_time) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.fall_time = [fall_time for j, fall_time in enumerate(patient.fall_time) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.area = [area for j, area in enumerate(patient.area) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.local_hrv = [local_hrv for j, local_hrv in enumerate(patient.local_hrv) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.energy = [energy for j, energy in enumerate(patient.energy) if j not in incorrect_peak_positions_list[i]]\n",
        "    patient.dominant_frequency = [dominant_frequency for j, dominant_frequency in enumerate(patient.dominant_frequency) if j not in incorrect_peak_positions_list[i]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WILmzyalXwSt"
      },
      "outputs": [],
      "source": [
        "# Compute new class distribution\n",
        "calculate_class_distribution(patient_instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MdHqFLxXwSt"
      },
      "outputs": [],
      "source": [
        "# Get the indices of the noisy beats\n",
        "noisy_beats = get_noisy_beats(patient_instances)\n",
        "# Check noisy beats labels\n",
        "check_noisy_beats_labels = [patient_instances[i].labels[j] for i, j in noisy_beats]\n",
        "unique, counts = np.unique(check_noisy_beats_labels, return_counts=True)\n",
        "print(f\"Labels of noisy beats: {dict(zip(unique, counts))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YviEevNyXwSu"
      },
      "outputs": [],
      "source": [
        "# Separate beats based on label\n",
        "noisy_beats_S = [(i,j) for i, j in noisy_beats if patient_instances[i].labels[j] == 'S']\n",
        "noisy_beats_V = [(i,j) for i, j in noisy_beats if patient_instances[i].labels[j] == 'V']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVA94qfDXwSu"
      },
      "outputs": [],
      "source": [
        "# Visualize a batch of S noisy beats\n",
        "plot_noisy_beats(patient_instances, noisy_beats_S)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iky34WjjXwSu"
      },
      "outputs": [],
      "source": [
        "# Visualize a batch of V noisy beats\n",
        "plot_noisy_beats(patient_instances, noisy_beats_V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h11s3qFmXwSu"
      },
      "source": [
        "Notice how few the noisy S and V beats are. For this reason, reconstructing the actual beats may be unfeasible.\n",
        "So, noisy beats are completely removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYzdKYf_XwSu"
      },
      "outputs": [],
      "source": [
        "# Group noisy_beats_S by patients\n",
        "grouped_noisy_beats_S = group_noisy_beats(noisy_beats_S)\n",
        "noisy_beats_S_list = []\n",
        "for group in grouped_noisy_beats_S:\n",
        "    noisy_beats_S_list.append([item[1] for item in group])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmWOkMF-XwSu"
      },
      "outputs": [],
      "source": [
        "# Check if incorrect_peak_positions have been found for each patient\n",
        "patient_indexes = []\n",
        "for group in grouped_noisy_beats_S:\n",
        "    idx = [item[0] for item in group]\n",
        "    unique_idx = set(idx)\n",
        "    if unique_idx not in patient_indexes:\n",
        "        patient_indexes.append(unique_idx)\n",
        "\n",
        "print(f\"Patient indexes: {patient_indexes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvdVIvABXwSv"
      },
      "outputs": [],
      "source": [
        "# As noisy 'S' beats are not found for all the patients the removal must include an additional check\n",
        "# Extract the indexes from the sets\n",
        "patient_indexes = [list(s)[0] for s in patient_indexes]\n",
        "# Remove the noisy beats labelled as 'S'\n",
        "for i, patient in enumerate(patient_indexes):\n",
        "    patient_instances[patient].single_beats = [beat for j, beat in enumerate(patient_instances[patient].single_beats) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].contiguous_beats = [beat for j, beat in enumerate(patient_instances[patient].contiguous_beats) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].mean = [mean for j, mean in enumerate(patient_instances[patient].mean) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].std = [std for j, std in enumerate(patient_instances[patient].std) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].amplitude = [amplitude for j, amplitude in enumerate(patient_instances[patient].amplitude) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].peak_value = [peak_value for j, peak_value in enumerate(patient_instances[patient].peak_value) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].pre_PP = [pre_PP for j, pre_PP in enumerate(patient_instances[patient].pre_PP) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].post_PP = [post_PP for j, post_PP in enumerate(patient_instances[patient].post_PP) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].avg_PP = [avg_PP for j, avg_PP in enumerate(patient_instances[patient].avg_PP) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].width = [width for j, width in enumerate(patient_instances[patient].width) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].labels = [label for j, label in enumerate(patient_instances[patient].labels) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].peak_locations = [peak_location for j, peak_location in enumerate(patient_instances[patient].peak_locations) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].FWHM = [FWHM for j, FWHM in enumerate(patient_instances[patient].FWHM) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].skewness = [skewness for j, skewness in enumerate(patient_instances[patient].skewness) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].pre_skew = [pre_skew for j, pre_skew in enumerate(patient_instances[patient].pre_skew) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].post_skew = [post_skew for j, post_skew in enumerate(patient_instances[patient].post_skew) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].kurtosis = [kurtosis for j, kurtosis in enumerate(patient_instances[patient].kurtosis) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].pre_kurt = [pre_kurt for j, pre_kurt in enumerate(patient_instances[patient].pre_kurt) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].post_kurt = [post_kurt for j, post_kurt in enumerate(patient_instances[patient].post_kurt) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].entropy = [entropy for j, entropy in enumerate(patient_instances[patient].entropy) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].RMS = [RMS for j, RMS in enumerate(patient_instances[patient].RMS) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].neg_neg_jump = [neg_neg_jump for j, neg_neg_jump in enumerate(patient_instances[patient].neg_neg_jump) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].pre_pos_pos_jump = [pre_pos_pos_jump for j, pre_pos_pos_jump in enumerate(patient_instances[patient].pre_pos_pos_jump) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].post_pos_pos_jump = [post_pos_pos_jump for j, post_pos_pos_jump in enumerate(patient_instances[patient].post_pos_pos_jump) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].rise_time = [rise_time for j, rise_time in enumerate(patient_instances[patient].rise_time) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].fall_time = [fall_time for j, fall_time in enumerate(patient_instances[patient].fall_time) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].area = [area for j, area in enumerate(patient_instances[patient].area) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].local_hrv = [local_hrv for j, local_hrv in enumerate(patient_instances[patient].local_hrv) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].energy = [energy for j, energy in enumerate(patient_instances[patient].energy) if j not in noisy_beats_S_list[i]]\n",
        "    patient_instances[patient].dominant_frequency = [dominant_frequency for j, dominant_frequency in enumerate(patient_instances[patient].dominant_frequency) if j not in noisy_beats_S_list[i]]\n",
        "    \n",
        "# Check new label distribution\n",
        "calculate_class_distribution(patient_instances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-O77P5SXwSv"
      },
      "source": [
        "As we removed the noisy 'S' beats then the indexes for the noisy 'V' beats must be re-computed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYabAY3wXwSv"
      },
      "outputs": [],
      "source": [
        "# Get the indices of the noisy beats\n",
        "noisy_beats = get_noisy_beats(patient_instances)\n",
        "# Check noisy beats labels\n",
        "check_noisy_beats_labels = [patient_instances[i].labels[j] for i, j in noisy_beats]\n",
        "unique, counts = np.unique(check_noisy_beats_labels, return_counts=True)\n",
        "print(f\"Labels of noisy beats: {dict(zip(unique, counts))}\")\n",
        "# Separate beats based on label\n",
        "noisy_beats_V = [(i,j) for i, j in noisy_beats if patient_instances[i].labels[j] == 'V']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAmzSWeXXwSv"
      },
      "outputs": [],
      "source": [
        "# Group noisy_beats_V by patients\n",
        "grouped_noisy_beats_V = group_noisy_beats(noisy_beats_V)\n",
        "noisy_beats_V_list = []\n",
        "for group in grouped_noisy_beats_V:\n",
        "    noisy_beats_V_list.append([item[1] for item in group])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLRYRPW_XwSw"
      },
      "outputs": [],
      "source": [
        "# Check if incorrect_peak_positions have been found for each patient\n",
        "patient_indexes = []\n",
        "for group in grouped_noisy_beats_V:\n",
        "    idx = [item[0] for item in group]\n",
        "    unique_idx = set(idx)\n",
        "    if unique_idx not in patient_indexes:\n",
        "        patient_indexes.append(unique_idx)\n",
        "\n",
        "print(f\"Patient indexes: {patient_indexes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZHCpGx2XwSw"
      },
      "outputs": [],
      "source": [
        "# As noisy 'V' beats are not found for all the patients the removal must include an additional check\n",
        "# Extract the indexes from the sets\n",
        "patient_indexes = [list(s)[0] for s in patient_indexes]\n",
        "# Remove the noisy beats labelled as 'V'\n",
        "for i, patient in enumerate(patient_indexes):\n",
        "    patient_instances[patient].single_beats = [beat for j, beat in enumerate(patient_instances[patient].single_beats) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].contiguous_beats = [beat for j, beat in enumerate(patient_instances[patient].contiguous_beats) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].mean = [mean for j, mean in enumerate(patient_instances[patient].mean) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].std = [std for j, std in enumerate(patient_instances[patient].std) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].amplitude = [amplitude for j, amplitude in enumerate(patient_instances[patient].amplitude) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].peak_value = [peak_value for j, peak_value in enumerate(patient_instances[patient].peak_value) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].pre_PP = [pre_PP for j, pre_PP in enumerate(patient_instances[patient].pre_PP) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].post_PP = [post_PP for j, post_PP in enumerate(patient_instances[patient].post_PP) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].avg_PP = [avg_PP for j, avg_PP in enumerate(patient_instances[patient].avg_PP) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].width = [width for j, width in enumerate(patient_instances[patient].width) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].labels = [label for j, label in enumerate(patient_instances[patient].labels) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].peak_locations = [peak_location for j, peak_location in enumerate(patient_instances[patient].peak_locations) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].FWHM = [FWHM for j, FWHM in enumerate(patient_instances[patient].FWHM) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].skewness = [skewness for j, skewness in enumerate(patient_instances[patient].skewness) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].pre_skew = [pre_skew for j, pre_skew in enumerate(patient_instances[patient].pre_skew) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].post_skew = [post_skew for j, post_skew in enumerate(patient_instances[patient].post_skew) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].kurtosis = [kurtosis for j, kurtosis in enumerate(patient_instances[patient].kurtosis) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].pre_kurt = [pre_kurt for j, pre_kurt in enumerate(patient_instances[patient].pre_kurt) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].post_kurt = [post_kurt for j, post_kurt in enumerate(patient_instances[patient].post_kurt) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].entropy = [entropy for j, entropy in enumerate(patient_instances[patient].entropy) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].RMS = [RMS for j, RMS in enumerate(patient_instances[patient].RMS) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].neg_neg_jump = [neg_neg_jump for j, neg_neg_jump in enumerate(patient_instances[patient].neg_neg_jump) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].pre_pos_pos_jump = [pre_pos_pos_jump for j, pre_pos_pos_jump in enumerate(patient_instances[patient].pre_pos_pos_jump) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].post_pos_pos_jump = [post_pos_pos_jump for j, post_pos_pos_jump in enumerate(patient_instances[patient].post_pos_pos_jump) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].rise_time = [rise_time for j, rise_time in enumerate(patient_instances[patient].rise_time) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].fall_time = [fall_time for j, fall_time in enumerate(patient_instances[patient].fall_time) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].area = [area for j, area in enumerate(patient_instances[patient].area) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].local_hrv = [local_hrv for j, local_hrv in enumerate(patient_instances[patient].local_hrv) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].energy = [energy for j, energy in enumerate(patient_instances[patient].energy) if j not in noisy_beats_V_list[i]]\n",
        "    patient_instances[patient].dominant_frequency = [dominant_frequency for j, dominant_frequency in enumerate(patient_instances[patient].dominant_frequency) if j not in noisy_beats_V_list[i]]\n",
        "\n",
        "# Check new label distribution\n",
        "calculate_class_distribution(patient_instances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igTangY_ifRg"
      },
      "source": [
        "# Split patients into train-validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYMY2JrqifRg"
      },
      "outputs": [],
      "source": [
        "# Define a function to compute class proportions\n",
        "def calculate_class_proportions(patient_instances):\n",
        "    \"\"\"\n",
        "    Calculates the class proportions of the labels.\n",
        "\n",
        "    Args:\n",
        "        patient_instances (list): The list of patient instances.\n",
        "    \"\"\"\n",
        "    tot_count_n = 0\n",
        "    tot_count_s = 0\n",
        "    tot_count_v = 0\n",
        "    for patient in patient_instances:\n",
        "        count_n, count_s, count_v = calculate_label_distribution(patient.labels)\n",
        "        tot_count_n += count_n\n",
        "        tot_count_s += count_s\n",
        "        tot_count_v += count_v\n",
        "    n_ratio = tot_count_n / (tot_count_n + tot_count_v + tot_count_s)\n",
        "    v_ratio = tot_count_v / (tot_count_n + tot_count_v + tot_count_s)\n",
        "    s_ratio = tot_count_s / (tot_count_n + tot_count_v + tot_count_s)\n",
        "    print(f\"Label proportions: {round(n_ratio, 4)} N beats, {round(v_ratio, 4)} V beats, {round(s_ratio, 4)} S beats\")\n",
        "    return n_ratio, v_ratio, s_ratio\n",
        "\n",
        "# Check class proportions\n",
        "n_ratio, v_ratio, s_ratio = calculate_class_proportions(patient_instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpYwKNljifRg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initialize variables\n",
        "n_ratio_train = 0\n",
        "s_ratio_train = 0\n",
        "v_ratio_train = 0\n",
        "\n",
        "n_ratio_val = 0\n",
        "s_ratio_val = 0\n",
        "v_ratio_val = 0\n",
        "\n",
        "n_ratio_test = 0\n",
        "s_ratio_test = 0\n",
        "v_ratio_test = 0\n",
        "random_state = 999\n",
        "best_random_state = random_state\n",
        "max_iterations = 100\n",
        "iteration = 0\n",
        "\n",
        "# Initialize variables for the best split\n",
        "best_diff = float('inf')\n",
        "best_split = None\n",
        "\n",
        "# Loop until desired conditions are met\n",
        "while((abs(n_ratio_train - n_ratio) > 0.001 or abs(s_ratio_train - s_ratio) > 0.001 or abs(v_ratio_train - v_ratio) > 0.001 or\n",
        "       abs(n_ratio_val - n_ratio) > 0.001 or abs(s_ratio_val - s_ratio) > 0.001 or abs(v_ratio_val - v_ratio) > 0.001 or\n",
        "       abs(n_ratio_test - n_ratio) > 0.001 or abs(s_ratio_test - s_ratio) > 0.001 or abs(v_ratio_test - v_ratio) > 0.001) and iteration < max_iterations):\n",
        "\n",
        "    # Split the data into train, validation and test sets\n",
        "    X_train_val, X_test = train_test_split(patient_instances, test_size=0.15, random_state=random_state)\n",
        "    X_train, X_val = train_test_split(X_train_val, test_size=len(X_test), random_state=random_state)\n",
        "\n",
        "    # Check label distribution in train set\n",
        "    print(\"Train set:\")\n",
        "    calculate_class_distribution(X_train)\n",
        "    n_ratio_train, v_ratio_train, s_ratio_train = calculate_class_proportions(X_train)\n",
        "\n",
        "    # Check label distribution in validation set\n",
        "    print(\"Validation set:\")\n",
        "    calculate_class_distribution(X_val)\n",
        "    n_ratio_val, v_ratio_val, s_ratio_val = calculate_class_proportions(X_val)\n",
        "\n",
        "    # Check label distribution in test set\n",
        "    print(\"Test set:\")\n",
        "    calculate_class_distribution(X_test)\n",
        "    n_ratio_test, v_ratio_test, s_ratio_test = calculate_class_proportions(X_test)\n",
        "\n",
        "    # Calculate the total difference between the ratios\n",
        "    total_diff = abs(n_ratio_train - n_ratio) + abs(s_ratio_train - s_ratio) + abs(v_ratio_train - v_ratio) + \\\n",
        "                abs(n_ratio_val - n_ratio) + abs(s_ratio_val - s_ratio) + abs(v_ratio_val - v_ratio) + \\\n",
        "                abs(n_ratio_test - n_ratio) + abs(s_ratio_test - s_ratio) + abs(v_ratio_test - v_ratio)\n",
        "\n",
        "    # If this split is better than the previous best, update the best split\n",
        "    if total_diff < best_diff:\n",
        "        best_random_state = random_state\n",
        "        best_diff = total_diff\n",
        "        best_split = (X_train, X_val, X_test)\n",
        "\n",
        "    random_state += 1\n",
        "    iteration += 1\n",
        "\n",
        "# After the loop, best_split contains the best split found\n",
        "if(iteration >= max_iterations):\n",
        "    print(\"Max iterations reached\")\n",
        "    X_train, X_val, X_test = best_split\n",
        "    print(f\"Best Random State: {best_random_state}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fmHVReWifRh"
      },
      "outputs": [],
      "source": [
        "# Check the class proportions of the sets\n",
        "print(\"Train set:\")\n",
        "calculate_class_proportions(X_train)\n",
        "print(\"Validation set:\")\n",
        "calculate_class_proportions(X_val)\n",
        "print(\"Test set:\")\n",
        "calculate_class_proportions(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGtV43JPXwSw"
      },
      "source": [
        "# Build Network Input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmHL_ku4XwSw"
      },
      "outputs": [],
      "source": [
        "# Build train, validation and test sets\n",
        "X_train_single_beats = [beat for patient in X_train for beat in patient.single_beats]\n",
        "X_val_single_beats = [beat for patient in X_val for beat in patient.single_beats]\n",
        "X_test_single_beats = [beat for patient in X_test for beat in patient.single_beats]\n",
        "\n",
        "X_train_contiguous_beats = [beat for patient in X_train for beat in patient.contiguous_beats]\n",
        "X_val_contiguous_beats = [beat for patient in X_val for beat in patient.contiguous_beats]\n",
        "X_test_contiguous_beats = [beat for patient in X_test for beat in patient.contiguous_beats]\n",
        "\n",
        "# Build train, validation and test labels\n",
        "y_train = [label for patient in X_train for label in patient.labels]\n",
        "y_val = [label for patient in X_val for label in patient.labels]\n",
        "y_test = [label for patient in X_test for label in patient.labels]\n",
        "\n",
        "# Check dimensionality of train, validation and test sets\n",
        "print(\"-> Single Beats\")\n",
        "print(f\"Train dim.: {len(X_train_single_beats)}\")\n",
        "print(f\"Validation dim.: {len(X_val_single_beats)}\")\n",
        "print(f\"Test dim.: {len(X_test_single_beats)}\")\n",
        "print(\"-> Contiguous Beats\")\n",
        "print(f\"Train dim.: {len(X_train_contiguous_beats)}\")\n",
        "print(f\"Validation dim.: {len(X_val_contiguous_beats)}\")\n",
        "print(f\"Test dim.: {len(X_test_contiguous_beats)}\")\n",
        "\n",
        "# Check dimensionality of labels\n",
        "print(\"-> Labels\")\n",
        "print(f\"Train labels dim.: {len(y_train)}\")\n",
        "print(f\"Validation labels dim.: {len(y_val)}\")\n",
        "print(f\"Test labels dim.: {len(y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86ch02iniFIB"
      },
      "source": [
        "As the train-validation-test split was performed, it was seen that some beats (10 for the normal beats and 1 for the abnormal ones) where not 100 samples in length. This is due to the fact that these beats were in proximity to the signal truncations so in the case of the N signal they are removed, for the abnormal signals padding is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nys95kFIifRi"
      },
      "outputs": [],
      "source": [
        "def separate_short_beats(beats, labels, target_len=100):\n",
        "    # Initialize empty lists for each label\n",
        "    short_beats_N = []\n",
        "    short_beats_S = []\n",
        "    short_beats_V = []\n",
        "\n",
        "    # Iterate over the beats and append them to the corresponding list based on their label\n",
        "    for i, beat in enumerate(beats):\n",
        "        label = labels[i]\n",
        "        if len(beat) < target_len:\n",
        "            if label == 'N':\n",
        "                print(f\"Beat {i} is a {len(beat)} N beat\")\n",
        "                short_beats_N.append(i)\n",
        "            elif label == 'S':\n",
        "                print(f\"Beat {i} is a {len(beat)} S beat\")\n",
        "                short_beats_S.append(i)\n",
        "            elif label == 'V':\n",
        "                print(f\"Beat {i} is a {len(beat)} V beat\")\n",
        "                short_beats_V.append(i)\n",
        "\n",
        "    return short_beats_N, short_beats_S, short_beats_V\n",
        "\n",
        "# Check the number of short beats for each label\n",
        "SINGLE_BEAT_LEN = 100\n",
        "CONT_BEAT_LEN = 200\n",
        "\n",
        "print(\"--> Single Beats\")\n",
        "print(\"Train set:\")\n",
        "short_beats_N_train, short_beats_S_train, short_beats_V_train = separate_short_beats(X_train_single_beats, y_train, target_len=SINGLE_BEAT_LEN)\n",
        "print(\"Validation set:\")\n",
        "short_beats_N_val, short_beats_S_val, short_beats_V_val = separate_short_beats(X_val_single_beats, y_val, target_len=SINGLE_BEAT_LEN)\n",
        "print(\"Test set:\")\n",
        "short_beats_N_test, short_beats_S_test, short_beats_V_test = separate_short_beats(X_test_single_beats, y_test, target_len=SINGLE_BEAT_LEN)\n",
        "\n",
        "print(\"--> Contiguous Beats\")\n",
        "print(\"Train set:\")\n",
        "short_contiguous_beats_N_train, short_contiguous_beats_S_train, short_contiguous_beats_V_train = separate_short_beats(X_train_contiguous_beats, y_train, target_len=CONT_BEAT_LEN)\n",
        "print(\"Validation set:\")\n",
        "short_contiguous_beats_N_val, short_contiguous_beats_S_val, short_contiguous_beats_V_val = separate_short_beats(X_val_contiguous_beats, y_val, target_len=CONT_BEAT_LEN)\n",
        "print(\"Test set:\")\n",
        "short_contiguous_beats_N_test, short_contiguous_beats_S_test, short_contiguous_beats_V_test = separate_short_beats(X_test_contiguous_beats, y_test, target_len=CONT_BEAT_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di8vT5Cda30h"
      },
      "outputs": [],
      "source": [
        "# Define a function to pad the sequence by repeating its last value\n",
        "def pad_sequence(seq, target_length):\n",
        "    pad_size = target_length - len(seq)\n",
        "    if pad_size <= 0:\n",
        "        return seq\n",
        "    else:\n",
        "        return np.pad(seq, (0, pad_size), 'constant', constant_values=seq[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_YGBExlifRi"
      },
      "outputs": [],
      "source": [
        "# Apply padding to the short V sequence in Single beats\n",
        "X_train_beats = [pad_sequence(beat, SINGLE_BEAT_LEN) if i in short_beats_V_train else beat for i, beat in enumerate(X_train_single_beats)]\n",
        "X_val_beats = [pad_sequence(beat, SINGLE_BEAT_LEN) if i in short_beats_V_val else beat for i, beat in enumerate(X_val_single_beats)]\n",
        "X_test_beats = [pad_sequence(beat, SINGLE_BEAT_LEN) if i in short_beats_V_test else beat for i, beat in enumerate(X_test_single_beats)]\n",
        "\n",
        "# Apply padding to the short V sequence in Contiguous beats\n",
        "X_train_beats_cont = [pad_sequence(beat, CONT_BEAT_LEN) if i in short_contiguous_beats_V_train else beat for i, beat in enumerate(X_train_contiguous_beats)]\n",
        "X_val_beats_cont = [pad_sequence(beat, CONT_BEAT_LEN) if i in short_contiguous_beats_V_val else beat for i, beat in enumerate(X_val_contiguous_beats)]\n",
        "X_test_beats_cont = [pad_sequence(beat, CONT_BEAT_LEN) if i in short_contiguous_beats_V_test else beat for i, beat in enumerate(X_test_contiguous_beats)]\n",
        "\n",
        "# Apply padding to the short S sequence in Contiguous beats\n",
        "X_train_beats_cont = [pad_sequence(beat, CONT_BEAT_LEN) if i in short_contiguous_beats_S_train else beat for i, beat in enumerate(X_train_beats_cont)]\n",
        "X_val_beats_cont = [pad_sequence(beat, CONT_BEAT_LEN) if i in short_contiguous_beats_S_val else beat for i, beat in enumerate(X_val_beats_cont)]\n",
        "X_test_beats_cont = [pad_sequence(beat, CONT_BEAT_LEN) if i in short_contiguous_beats_S_test else beat for i, beat in enumerate(X_test_beats_cont)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f9zyaU-T_bf"
      },
      "outputs": [],
      "source": [
        "print(\"--> Single Beats\")\n",
        "print(\"Train set:\")\n",
        "short_beats_N_train, short_beats_S_train, short_beats_V_train = separate_short_beats(X_train_beats, y_train)\n",
        "print(\"Validation set:\")\n",
        "short_beats_N_val, short_beats_S_val, short_beats_V_val = separate_short_beats(X_val_beats, y_val)\n",
        "print(\"Test set:\")\n",
        "short_beats_N_test, short_beats_S_test, short_beats_V_test = separate_short_beats(X_test_beats, y_test)\n",
        "\n",
        "print(\"--> Contiguous Beats\")\n",
        "print(\"Train set:\")\n",
        "short_contiguous_beats_N_train, short_contiguous_beats_S_train, short_contiguous_beats_V_train = separate_short_beats(X_train_beats_cont, y_train, target_len=CONT_BEAT_LEN)\n",
        "print(\"Validation set:\")\n",
        "short_contiguous_beats_N_val, short_contiguous_beats_S_val, short_contiguous_beats_V_val = separate_short_beats(X_val_beats_cont, y_val, target_len=CONT_BEAT_LEN)\n",
        "print(\"Test set:\")\n",
        "short_contiguous_beats_N_test, short_contiguous_beats_S_test, short_contiguous_beats_V_test = separate_short_beats(X_test_beats_cont, y_test, target_len=CONT_BEAT_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx6T0CaPi-ss"
      },
      "outputs": [],
      "source": [
        "# Remove the short N beats from all the sets\n",
        "# Single Beats\n",
        "X_train_single = [beat for i, beat in enumerate(X_train_beats) if i not in short_beats_N_train]\n",
        "X_val_single = [beat for i, beat in enumerate(X_val_beats) if i not in short_beats_N_val]\n",
        "X_test_single = [beat for i, beat in enumerate(X_test_beats) if i not in short_beats_N_test]\n",
        "# Contiguous Beats\n",
        "X_train_contiguous = [beat for i, beat in enumerate(X_train_beats_cont) if i not in short_contiguous_beats_N_train]\n",
        "X_val_contiguous = [beat for i, beat in enumerate(X_val_beats_cont) if i not in short_contiguous_beats_N_val]\n",
        "X_test_contiguous = [beat for i, beat in enumerate(X_test_beats_cont) if i not in short_contiguous_beats_N_test]\n",
        "# Single Beats labels\n",
        "y_train_single = [label for i, label in enumerate(y_train) if i not in short_beats_N_train]\n",
        "y_val_single = [label for i, label in enumerate(y_val) if i not in short_beats_N_val]\n",
        "y_test_single = [label for i, label in enumerate(y_test) if i not in short_beats_N_test]\n",
        "# Contiguous Beats labels\n",
        "y_train_contiguous = [label for i, label in enumerate(y_train) if i not in short_contiguous_beats_N_train]\n",
        "y_val_contiguous = [label for i, label in enumerate(y_val) if i not in short_contiguous_beats_N_val]\n",
        "y_test_contiguous = [label for i, label in enumerate(y_test) if i not in short_contiguous_beats_N_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyHtiCr20CmQ"
      },
      "outputs": [],
      "source": [
        "# Single Beats\n",
        "# Convert to NumPy array\n",
        "X_train_single = np.array(X_train_single)\n",
        "X_val_single = np.array(X_val_single)\n",
        "X_test_single = np.array(X_test_single)\n",
        "# Check dimensionality\n",
        "print(\"Single Beats\")\n",
        "print(X_train_single.shape,X_val_single.shape,X_test_single.shape)\n",
        "# Contiguous Beats\n",
        "# Convert to NumPy array\n",
        "X_train_contiguous = np.array(X_train_contiguous)\n",
        "X_val_contiguous = np.array(X_val_contiguous)\n",
        "X_test_contiguous = np.array(X_test_contiguous)\n",
        "# Check dimensionality\n",
        "print(\"Contiguous Beats\")\n",
        "print(X_train_contiguous.shape,X_val_contiguous.shape,X_test_contiguous.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EV-tOR9V0OdD"
      },
      "outputs": [],
      "source": [
        "# Single Beats\n",
        "# Convert to NumPy array\n",
        "y_train_single = np.array(y_train_single)\n",
        "y_val_single = np.array(y_val_single)\n",
        "y_test_single = np.array(y_test_single)\n",
        "# Check dimensionality\n",
        "print(\"Single Beats\")\n",
        "print(y_train_single.shape,y_val_single.shape,y_test_single.shape)\n",
        "# Contiguous Beats\n",
        "# Convert to NumPy array\n",
        "y_train_contiguous = np.array(y_train_contiguous)\n",
        "y_val_contiguous = np.array(y_val_contiguous)\n",
        "y_test_contiguous = np.array(y_test_contiguous)\n",
        "# Check dimensionality\n",
        "print(\"Contiguous Beats\")\n",
        "print(y_train_contiguous.shape,y_val_contiguous.shape,y_test_contiguous.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49iJ2vk31lyo"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# One hot encode labels\n",
        "num_classes = 3\n",
        "encoder = LabelEncoder()\n",
        "one_hot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "\n",
        "# Single Beats\n",
        "y_train_single_encoded = encoder.fit_transform(y_train_single).reshape(-1, 1)\n",
        "y_val_single_encoded = encoder.transform(y_val_single).reshape(-1, 1)\n",
        "y_test_single_encoded = encoder.transform(y_test_single).reshape(-1, 1)\n",
        "\n",
        "y_train_single = one_hot_encoder.fit_transform(y_train_single_encoded)\n",
        "y_val_single = one_hot_encoder.transform(y_val_single_encoded)\n",
        "y_test_single = one_hot_encoder.transform(y_test_single_encoded)\n",
        "\n",
        "# Contiguous Beats\n",
        "y_train_contiguous_encoded = encoder.fit_transform(y_train_contiguous).reshape(-1, 1)\n",
        "y_val_contiguous_encoded = encoder.transform(y_val_contiguous).reshape(-1, 1)\n",
        "y_test_contiguous_encoded = encoder.transform(y_test_contiguous).reshape(-1, 1)\n",
        "\n",
        "y_train_contiguous = one_hot_encoder.fit_transform(y_train_contiguous_encoded)\n",
        "y_val_contiguous = one_hot_encoder.transform(y_val_contiguous_encoded)\n",
        "y_test_contiguous = one_hot_encoder.transform(y_test_contiguous_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2DM6Gp7A7Nh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Build Train dataframe with extracted features\n",
        "X_train_feat = pd.DataFrame(columns=['mean', 'std', 'amplitude', 'peak_value', \n",
        "                                     'pre_PP','post_PP', 'avg_PP',\n",
        "                                     'width', 'FWHM', \n",
        "                                     'skewness', 'pre_skew', 'post_skew',\n",
        "                                     'kurtosis', 'pre_kurt', 'post_kurt',\n",
        "                                     'entropy', 'RMS', \n",
        "                                     'neg_neg_jump', 'pre_pos_pos_jump', 'post_pos_pos_jump',\n",
        "                                     'rise_time', 'fall_time', 'area',\n",
        "                                     'local_hrv', 'energy', 'dominant_frequency'])\n",
        "for patient in tqdm.tqdm(X_train, desc=\"Building Train Feature Dataframe\",\n",
        "                         total=len(X_train)):\n",
        "  for i, beat in enumerate(patient.single_beats):\n",
        "        row = [patient.mean[i], patient.std[i], patient.amplitude[i], patient.peak_value[i], \n",
        "               patient.pre_PP[i],patient.post_PP[i], patient.avg_PP[i], \n",
        "               patient.width[i], patient.FWHM[i], \n",
        "               patient.skewness[i], patient.pre_skew[i], patient.post_skew[i],\n",
        "               patient.kurtosis[i], patient.pre_kurt[i], patient.post_kurt[i],\n",
        "               patient.entropy[i], patient.RMS[i], \n",
        "               patient.neg_neg_jump[i], patient.pre_pos_pos_jump[i], patient.post_pos_pos_jump[i],\n",
        "               patient.rise_time[i], patient.fall_time[i], patient.area[i],\n",
        "               patient.local_hrv[i], patient.energy[i], patient.dominant_frequency[i]]\n",
        "        X_train_feat.loc[len(X_train_feat)] = row\n",
        "\n",
        "X_train_feat.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-AVusV0KJ6S"
      },
      "outputs": [],
      "source": [
        "# Inspect data\n",
        "X_train_feat.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XEt_ocOKdNy"
      },
      "outputs": [],
      "source": [
        "#check the presence of missing values as 'NaN'\n",
        "print(\"The number of missing values per attribute is the following:\")\n",
        "X_train_feat.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#check the presence of 'Inf' values\n",
        "print(\"The number of Inf values per attribute is the following:\")\n",
        "print(np.isinf(X_train_feat).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjkfrfcoDiHu"
      },
      "outputs": [],
      "source": [
        "# Build Val dataframe with extracted features\n",
        "X_val_feat = pd.DataFrame(columns=['mean', 'std', 'amplitude', 'peak_value', \n",
        "                                     'pre_PP','post_PP', 'avg_PP',\n",
        "                                     'width', 'FWHM', \n",
        "                                     'skewness', 'pre_skew', 'post_skew',\n",
        "                                     'kurtosis', 'pre_kurt', 'post_kurt',\n",
        "                                     'entropy', 'RMS', \n",
        "                                     'neg_neg_jump', 'pre_pos_pos_jump', 'post_pos_pos_jump',\n",
        "                                     'rise_time', 'fall_time', 'area',\n",
        "                                     'local_hrv', 'energy', 'dominant_frequency'])\n",
        "for patient in tqdm.tqdm(X_val, desc=\"Building Val Feature Dataframe\",\n",
        "                         total=len(X_val)):\n",
        "  for i, beat in enumerate(patient.single_beats):\n",
        "        row = [patient.mean[i], patient.std[i], patient.amplitude[i], patient.peak_value[i], \n",
        "               patient.pre_PP[i],patient.post_PP[i], patient.avg_PP[i], \n",
        "               patient.width[i], patient.FWHM[i], \n",
        "               patient.skewness[i], patient.pre_skew[i], patient.post_skew[i],\n",
        "               patient.kurtosis[i], patient.pre_kurt[i], patient.post_kurt[i],\n",
        "               patient.entropy[i], patient.RMS[i], \n",
        "               patient.neg_neg_jump[i], patient.pre_pos_pos_jump[i], patient.post_pos_pos_jump[i],\n",
        "               patient.rise_time[i], patient.fall_time[i], patient.area[i],\n",
        "               patient.local_hrv[i], patient.energy[i], patient.dominant_frequency[i]]\n",
        "        X_val_feat.loc[len(X_val_feat)] = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Czt7kAI6akdq"
      },
      "outputs": [],
      "source": [
        "# Concatenate the two dataframes in case ML approch is considered\n",
        "# This is done since the aim is to perform Cross-Validation over the entire train-validation set\n",
        "ML_MODEL = 0\n",
        "\n",
        "if ML_MODEL:\n",
        "  X_train_feat = pd.concat([X_train_feat, X_val_feat], axis=0)\n",
        "  X_train_feat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuFsTtEDD-Oi"
      },
      "outputs": [],
      "source": [
        "# Build test dataframe with extracted features\n",
        "X_test_feat = pd.DataFrame(columns=['mean', 'std', 'amplitude', 'peak_value', \n",
        "                                     'pre_PP','post_PP', 'avg_PP',\n",
        "                                     'width', 'FWHM', \n",
        "                                     'skewness', 'pre_skew', 'post_skew',\n",
        "                                     'kurtosis', 'pre_kurt', 'post_kurt',\n",
        "                                     'entropy', 'RMS', \n",
        "                                     'neg_neg_jump', 'pre_pos_pos_jump', 'post_pos_pos_jump',\n",
        "                                     'rise_time', 'fall_time', 'area',\n",
        "                                     'local_hrv', 'energy', 'dominant_frequency'])\n",
        "for patient in tqdm.tqdm(X_test, desc=\"Building test Feature Dataframe\",\n",
        "                         total=len(X_test)):\n",
        "  for i, beat in enumerate(patient.single_beats):\n",
        "        row = [patient.mean[i], patient.std[i], patient.amplitude[i], patient.peak_value[i], \n",
        "               patient.pre_PP[i],patient.post_PP[i], patient.avg_PP[i], \n",
        "               patient.width[i], patient.FWHM[i], \n",
        "               patient.skewness[i], patient.pre_skew[i], patient.post_skew[i],\n",
        "               patient.kurtosis[i], patient.pre_kurt[i], patient.post_kurt[i],\n",
        "               patient.entropy[i], patient.RMS[i], \n",
        "               patient.neg_neg_jump[i], patient.pre_pos_pos_jump[i], patient.post_pos_pos_jump[i],\n",
        "               patient.rise_time[i], patient.fall_time[i], patient.area[i],\n",
        "               patient.local_hrv[i], patient.energy[i], patient.dominant_frequency[i]]\n",
        "        X_test_feat.loc[len(X_test_feat)] = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qm95gYj6sxK"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "# Visualize correlation among features\n",
        "def plot_correlation_matrix(df):\n",
        "    correlation_matrix = df.corr()\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "    plt.title('Correlation Matrix Heatmap', fontsize=13)\n",
        "    plt.show()\n",
        "    # Compute the average correlation of each feature with all the others\n",
        "    average_correlation = correlation_matrix.mean()\n",
        "    average_correlation_sorted = average_correlation.sort_values(ascending=False)\n",
        "    print(\"-> Average correlation values for each feature:\")\n",
        "    print(average_correlation_sorted)\n",
        "\n",
        "# Plot correlation matrix\n",
        "plot_correlation_matrix(X_train_feat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the correlation matrix we notice that some features show high Pearson correlation values (>0.9). This is a case of multicollinearity and one of the two variables in the highly correlated pair should be removed to prevent feeding the model with redundant information. Removal is therefore performed by considering the average correlation of each feature with respect to all others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHx4OoNQIuML"
      },
      "outputs": [],
      "source": [
        "# Drop highly correlated features\n",
        "X_train_feat.drop('peak_value', axis=1, inplace=True)\n",
        "X_test_feat.drop('peak_value', axis=1, inplace=True)\n",
        "\n",
        "X_train_feat.drop('amplitude', axis=1, inplace=True)\n",
        "X_test_feat.drop('amplitude', axis=1, inplace=True)\n",
        "\n",
        "X_train_feat.drop('width', axis=1, inplace=True)\n",
        "X_test_feat.drop('width', axis=1, inplace=True)\n",
        "\n",
        "X_train_feat.drop('avg_PP', axis=1, inplace=True)\n",
        "X_test_feat.drop('avg_PP', axis=1, inplace=True)\n",
        "\n",
        "X_train_feat.drop('std', axis=1, inplace=True)\n",
        "X_test_feat.drop('std', axis=1, inplace=True)\n",
        "\n",
        "X_train_feat.drop('mean', axis=1, inplace=True)\n",
        "X_test_feat.drop('mean', axis=1, inplace=True)\n",
        "\n",
        "X_train_feat.drop('RMS', axis=1, inplace=True)\n",
        "X_test_feat.drop('RMS', axis=1, inplace=True)\n",
        "\n",
        "if not ML_MODEL:\n",
        "    X_val_feat.drop('peak_value', axis=1, inplace=True)\n",
        "    X_val_feat.drop('amplitude', axis=1, inplace=True)\n",
        "    X_val_feat.drop('width', axis=1, inplace=True)\n",
        "    X_val_feat.drop('avg_PP', axis=1, inplace=True)\n",
        "    X_val_feat.drop('std', axis=1, inplace=True)\n",
        "    X_val_feat.drop('mean', axis=1, inplace=True)\n",
        "    X_val_feat.drop('RMS', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot new correlation matrix\n",
        "plot_correlation_matrix(X_train_feat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMk9q_iKJMp7"
      },
      "outputs": [],
      "source": [
        "# Min-Max Normalization of Features\n",
        "max_df = X_train_feat.max()\n",
        "min_df = X_train_feat.min()\n",
        "\n",
        "X_train_feat = (X_train_feat - min_df)/(max_df - min_df)\n",
        "X_test_feat = (X_test_feat - min_df)/(max_df - min_df)\n",
        "\n",
        "if not ML_MODEL:\n",
        "  X_val_feat = (X_val_feat - min_df)/(max_df - min_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Three class split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYX1nwFvMTX3"
      },
      "outputs": [],
      "source": [
        "# Encode labels\n",
        "num_classes = 3\n",
        "# Label Encoder\n",
        "y_train_encoded = encoder.fit_transform(y_train_single)\n",
        "y_val_encoded = encoder.transform(y_val_single)\n",
        "y_test_encoded = encoder.transform(y_test_single)\n",
        "\n",
        "if ML_MODEL:\n",
        "  y_train_encoded = np.concatenate((y_train_encoded, y_val_encoded), axis=0)\n",
        "else:\n",
        "  # One Hot Encoding\n",
        "  from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "  one_hot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "  y_train_enc = y_train_encoded.reshape(-1, 1)\n",
        "  y_val_enc = y_val_encoded.reshape(-1, 1)\n",
        "  y_test_enc = y_test_encoded.reshape(-1, 1)\n",
        "  \n",
        "  y_train_feat = one_hot_encoder.fit_transform(y_train_enc)\n",
        "  y_val_feat = one_hot_encoder.transform(y_val_enc)\n",
        "  y_test_feat = one_hot_encoder.transform(y_test_enc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature dataframes and labels\n",
        "save(X_train_feat, 'X_train_feat')\n",
        "save(y_train_feat, 'y_train_feat')\n",
        "save(X_test_feat, 'X_test_feat')\n",
        "save(y_test_feat, 'y_test_feat')\n",
        "\n",
        "if not ML_MODEL:\n",
        "  save(X_val_feat, 'X_val_feat')\n",
        "  save(y_val_feat, 'y_val_feat')\n",
        "\n",
        "# Beat dataframes and labels\n",
        "save(X_train_single, 'X_train_single')\n",
        "save(y_train_single, 'y_train_single')\n",
        "save(X_test_single, 'X_test_single')\n",
        "save(y_test_single, 'y_test_single')\n",
        "\n",
        "save(X_train_contiguous, 'X_train_contiguous')\n",
        "save(y_train_contiguous, 'y_train_contiguous')\n",
        "save(X_test_contiguous, 'X_test_contiguous')\n",
        "save(y_test_contiguous, 'y_test_contiguous')\n",
        "\n",
        "if not ML_MODEL:\n",
        "  save(X_val_single, 'X_val_single')\n",
        "  save(y_val_single, 'y_val_single')\n",
        "  save(X_val_contiguous, 'X_val_contiguous')\n",
        "  save(y_val_contiguous, 'y_val_contiguous')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Two Class Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_encoded[y_train_encoded == 2] = 1\n",
        "y_val_encoded[y_val_encoded == 2] = 1\n",
        "y_test_encoded[y_test_encoded == 2] = 1\n",
        "\n",
        "# Save the encoded labels\n",
        "save(y_test_encoded, 'y_test_encoded_binary')\n",
        "\n",
        "if not ML_MODEL:\n",
        "  save(y_val_encoded, 'y_val_encoded_binary')\n",
        "else:\n",
        "  y_train_encoded = np.concatenate((y_train_encoded, y_val_encoded), axis=0)\n",
        "  save(y_train_encoded, 'y_train_encoded_binary')\n",
        "\n",
        "# One hot encode the labels\n",
        "y_train_encoded = one_hot_encoder.fit_transform(y_train_encoded.reshape(-1, 1))\n",
        "y_test_encoded = one_hot_encoder.transform(y_test_encoded.reshape(-1, 1))\n",
        "save(y_train_encoded, 'y_train_one_hot_binary')\n",
        "save(y_test_encoded, 'y_test_one_hot_binary')\n",
        "\n",
        "if not ML_MODEL:\n",
        "  y_val_encoded = one_hot_encoder.transform(y_val_encoded.reshape(-1, 1))\n",
        "  save(y_val_encoded, 'y_val_one_hot_binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Two Class Split for autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get 'N' beats\n",
        "norm_beats = [beat for i, patient in enumerate(patient_instances)\n",
        "              for j, (beat, label) in enumerate(zip(patient.single_beats, patient.labels))\n",
        "              if label == 'N']\n",
        "# Get abnormal beats\n",
        "abnorm_beats = [beat for i, patient in enumerate(patient_instances)\n",
        "                for j, (beat, label) in enumerate(zip(patient.single_beats, patient.labels))\n",
        "                if label != 'N']\n",
        "# Check dimensionality\n",
        "print(f\"Normal beats dim.: {len(norm_beats)}\")\n",
        "print(f\"Abnormal beats dim.: {len(abnorm_beats)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply padding to all elements in abnorm_beats\n",
        "abnorm_beats = np.array([pad_sequence(seq, 100) for seq in abnorm_beats])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reshape the array\n",
        "abnorm_beats = np.expand_dims(abnorm_beats, axis=-1)\n",
        "abnorm_beats.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove the short beats from norm_beats\n",
        "norm_beats = np.array([beat for beat in norm_beats if len(beat)==100])\n",
        "norm_beats = np.expand_dims(norm_beats,axis=-1)\n",
        "norm_beats.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the 'N' beats into train, validation and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_norm, X_test_norm = train_test_split(norm_beats, test_size=0.1, random_state=99)\n",
        "X_train_norm, X_val_norm = train_test_split(X_train_norm, test_size=0.1, random_state=99)\n",
        "\n",
        "# Check dimensionality\n",
        "print(f\"Normal beats Train dim.: {X_train_norm.shape}\")\n",
        "print(f\"Normal beats Validation dim.: {X_val_norm.shape}\")\n",
        "print(f\"Normal beats Test dim.: {X_test_norm.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save \n",
        "save(X_train_norm, 'X_train_norm')\n",
        "save(X_val_norm, 'X_val_norm')\n",
        "save(X_test_norm, 'X_test_norm')\n",
        "save(abnorm_beats, 'abnorm_beats')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
