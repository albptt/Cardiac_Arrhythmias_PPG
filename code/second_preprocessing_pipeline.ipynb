{"cells":[{"cell_type":"markdown","metadata":{"id":"eEoA85QoONyI"},"source":["# Workspace Set-Up"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define workspace utility functions\n","import os\n","import sys\n","import pickle\n","import subprocess\n","from IPython.display import FileLink\n","\n","# Call this function when in need to free RAM space\n","def check_variables():\n","    \"\"\"\n","    Check the memory usage of variables in the workspace.\n","    Print variables and their memory sizes in descending order.\n","    \"\"\"\n","    # Get the memory size of each variable\n","    variable_sizes = {k: sys.getsizeof(v) for k, v in locals().items() if not k.startswith('__')}\n","    # Sort the variables based on their memory size\n","    sorted_variables = sorted(variable_sizes.items(), key=lambda x: x[1], reverse=True)\n","    # Print the variables and their memory sizes in descending order\n","    for var, size in sorted_variables:\n","        print(f\"{var}: {size} bytes\")\n","\n","# Save anything via pickle\n","def save(item, name: str, path=\"/kaggle/working/\"):\n","    \"\"\"\n","    Save an item using pickle.\n","\n","    Parameters:\n","        item: The item to be saved.\n","        name (str): The name of the file.\n","        path (str): The path where the file will be saved (default: \"/kaggle/working/\").\n","    \"\"\"\n","    item_file = path + name\n","    with open(item_file, 'wb') as file:\n","        pickle.dump(item, file)\n","\n","# Download item as zip\n","def download_file(source_path: str, download_file_name: str, output_path=\"/kaggle/working/\"):\n","    \"\"\"\n","    Create a zip file from the specified source path and provide a download link.\n","    \n","    Parameters:\n","        source_path (str): The path to the source file or directory to be zipped.\n","        download_file_name (str): The name of the zip file and download link.\n","        output_path (str): The output path for the zip file (default: \"/kaggle/working/\").\n","    \"\"\"\n","    # Save the current working directory\n","    current_working_directory = os.getcwd()  \n","    os.chdir(output_path)\n","\n","    try:\n","        zip_name = f\"{download_file_name}.zip\"\n","        command = f\"zip {zip_name} {source_path} -r\"\n","        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n","        if result.returncode != 0:\n","            raise RuntimeError(f\"Unable to run zip command! Error: {result.stderr}\")\n","\n","        display(FileLink(zip_name))\n","    finally:\n","        # Restore the original working directory\n","        os.chdir(current_working_directory)  "]},{"cell_type":"markdown","metadata":{"id":"-TN9rKvunAXF"},"source":["# Import Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-01T19:43:57.404539Z","iopub.status.busy":"2024-02-01T19:43:57.403975Z","iopub.status.idle":"2024-02-01T19:43:57.523321Z","shell.execute_reply":"2024-02-01T19:43:57.522352Z","shell.execute_reply.started":"2024-02-01T19:43:57.404497Z"},"executionInfo":{"elapsed":3428,"status":"ok","timestamp":1706388733016,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"UkUs4kLvm-qB","outputId":"abdc9552-a66b-4e01-bde7-3ab2f9cfd5df","trusted":true},"outputs":[],"source":["# List first content of `input_directory`\n","import os\n","\n","input_directory = '/kaggle/input/ai-train/train'\n","dir_list = sorted(os.listdir(input_directory))\n","dir_list[0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-01T19:43:57.525735Z","iopub.status.busy":"2024-02-01T19:43:57.525400Z","iopub.status.idle":"2024-02-01T19:44:04.293033Z","shell.execute_reply":"2024-02-01T19:44:04.292118Z","shell.execute_reply.started":"2024-02-01T19:43:57.525710Z"},"executionInfo":{"elapsed":15998,"status":"ok","timestamp":1706388748999,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"VvLzSV2NlvZF","outputId":"006ac2ce-0d13-4281-d7fc-9260026c1709","trusted":true},"outputs":[],"source":["from scipy.io import loadmat\n","\n","def load_data_from_directory(input_directory):\n","    print('Loading data...')\n","    # Get name of label files for fs=128Hz contained in the folder\n","    beat_labeling_128 = []\n","    for f in dir_list:\n","        g = os.path.join(input_directory, f)\n","        if not f.lower().startswith('.') and f.lower().endswith('128_ann.mat') and os.path.isfile(g):\n","            beat_labeling_128.append(g)\n","    num_labels_128 = len(beat_labeling_128)\n","\n","    # Get name of peak location files for fs=128Hz contained in the folder\n","    peak_locations_128 = []\n","    for f in dir_list:\n","        g = os.path.join(input_directory, f)\n","        if not f.lower().startswith('.') and f.lower().endswith('128_spk.mat') and os.path.isfile(g):\n","            peak_locations_128.append(g)\n","    num_peak_locations_128 = len(peak_locations_128)\n","\n","    # Get name of signals fs=128Hz contained in the folder\n","    signals_128 = []\n","    for f in dir_list:\n","        g = os.path.join(input_directory, f)\n","        if not f.lower().startswith('.') and f.lower().endswith('_128.mat') and os.path.isfile(g):\n","            signals_128.append(g)\n","    num_signals_128 = len(signals_128)\n","\n","    # Get name of label files for fs=250Hz contained in the folder\n","    beat_labeling_250 = []\n","    for f in dir_list:\n","        g = os.path.join(input_directory, f)\n","        if not f.lower().startswith('.') and f.lower().endswith('250_ann.mat') and os.path.isfile(g):\n","            beat_labeling_250.append(g)\n","    num_labels_250 = len(beat_labeling_250)\n","\n","    # Get name of peak location files for fs=250Hz contained in the folder\n","    peak_locations_250 = []\n","    for f in dir_list:\n","        g = os.path.join(input_directory, f)\n","        if not f.lower().startswith('.') and f.lower().endswith('250_spk.mat') and os.path.isfile(g):\n","            peak_locations_250.append(g)\n","    num_peak_locations_250 = len(peak_locations_250)\n","\n","    # Get name of signals fs=250Hz contained in the folder\n","    signals_250 = []\n","    for f in dir_list:\n","        g = os.path.join(input_directory, f)\n","        if not f.lower().startswith('.') and f.lower().endswith('_250.mat') and os.path.isfile(g):\n","            signals_250.append(g)\n","    num_signals_250 = len(signals_250)\n","\n","    if((num_signals_128+num_signals_250)==(num_labels_128+num_labels_250)==(num_peak_locations_128+num_peak_locations_250)):\n","        # Create empty list for recordings and header files\n","        recordings_128 = list()\n","        recordings_250 = list()\n","        labels_128 = list()\n","        labels_250 = list()\n","        locations_128 = list()\n","        locations_250 = list()\n","\n","        # Load .mat, _ann.mat and _spk.mat files for each subject using the function \"load_data\"\n","        for i in range(num_signals_128):\n","            # load recordings\n","            recording_128 = loadmat(signals_128[i])\n","            recordings_128.append(recording_128['ppg'])\n","            # load labels\n","            label_128 = loadmat(beat_labeling_128[i])\n","            labels_128.append(label_128['labels'])\n","            # load locations\n","            location_128 = loadmat(peak_locations_128[i])\n","            locations_128.append(location_128['speaks'])\n","            # inform about loading step\n","            print(f\"\\rLoading fs=128Hz file: {i+1}/{num_signals_128}\")\n","\n","        for i in range(num_signals_250):\n","            # load recordings\n","            recording_250 = loadmat(signals_250[i])\n","            recordings_250.append(recording_250['ppg'])\n","            # load labels\n","            label_250 = loadmat(beat_labeling_250[i])\n","            labels_250.append(label_250['labels'])\n","            # load locations\n","            location_250 = loadmat(peak_locations_250[i])\n","            locations_250.append(location_250['speaks'])\n","            # inform about loading step\n","            print(f\"\\rLoading fs=250Hz file: {i+1}/{num_signals_250}\")\n","\n","    else:\n","        print(\"Error while reading files\")\n","\n","    return recordings_128, recordings_250, labels_128, labels_250, locations_128, locations_250\n","\n","# Call the function with the input_directory\n","recordings_128, recordings_250, labels_128, labels_250, locations_128, locations_250 = load_data_from_directory(input_directory)"]},{"cell_type":"markdown","metadata":{"id":"TPNP_Ua71Je6"},"source":["# Plot label distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:04.295120Z","iopub.status.busy":"2024-02-01T19:44:04.294393Z","iopub.status.idle":"2024-02-01T19:44:04.305784Z","shell.execute_reply":"2024-02-01T19:44:04.305042Z","shell.execute_reply.started":"2024-02-01T19:44:04.295081Z"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1706388749000,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"UFlVoCDge9_f","trusted":true},"outputs":[],"source":["import numpy as np\n","# get counts for each label\n","def calculate_label_distribution(labels):\n","  tot_count_n = 0\n","  tot_count_s = 0\n","  tot_count_v = 0\n","  for idx in range(len(labels)):\n","    counts_n = np.count_nonzero(labels[idx] == 'N')\n","    counts_s = np.count_nonzero(labels[idx] == 'S')\n","    counts_v = np.count_nonzero(labels[idx] == 'V')\n","    tot_count_n += counts_n\n","    tot_count_s += counts_s\n","    tot_count_v += counts_v\n","  return tot_count_n, tot_count_s, tot_count_v\n","\n","# check label distribution in 128Hz samples\n","tot_count_n_128, tot_count_s_128, tot_count_v_128 = calculate_label_distribution(labels_128)\n","\n","# check label distribution in 250Hz samples\n","tot_count_n_250, tot_count_s_250, tot_count_v_250 = calculate_label_distribution(labels_250)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-01T19:44:04.307964Z","iopub.status.busy":"2024-02-01T19:44:04.307683Z","iopub.status.idle":"2024-02-01T19:44:04.314086Z","shell.execute_reply":"2024-02-01T19:44:04.313107Z","shell.execute_reply.started":"2024-02-01T19:44:04.307939Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706388749000,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"rV2QmQp8POKI","outputId":"3c32df51-484e-4be7-afec-7fe15c9e2d9c","trusted":true},"outputs":[],"source":["# Check numerosity of classes\n","print(f\"Signals 128Hz: {tot_count_n_128} N beats, {tot_count_s_128} S beats, {tot_count_v_128} V beats\")\n","print(f\"Signals 250Hz: {tot_count_n_250} N beats, {tot_count_s_250} S beats, {tot_count_v_250} V beats\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2024-02-01T19:44:04.315319Z","iopub.status.busy":"2024-02-01T19:44:04.315052Z","iopub.status.idle":"2024-02-01T19:44:04.811229Z","shell.execute_reply":"2024-02-01T19:44:04.810299Z","shell.execute_reply.started":"2024-02-01T19:44:04.315272Z"},"executionInfo":{"elapsed":1651,"status":"ok","timestamp":1706388750648,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"fYe3UJq-eHm0","outputId":"dc97837c-5fe7-4f56-9f06-32c7de06801f","trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def plot_label_distribution(values, title, labels=['N', 'S', 'V']):\n","    plt.bar(labels, values, color=['blue', 'green', 'red'])\n","    plt.xlabel('Labels')\n","    plt.ylabel('Counts')\n","    plt.title(title)\n","    plt.show()\n","\n","# Plot histogram for 128Hz\n","values_128 = [tot_count_n_128, tot_count_s_128, tot_count_v_128]\n","plot_label_distribution(values_128, 'Histogram of Labels for 128Hz recordings')\n","\n","# Plot histogram for 250Hz\n","values_250 = [tot_count_n_250, tot_count_s_250, tot_count_v_250]\n","plot_label_distribution(values_250, 'Histogram of Labels for 250Hz recordings')\n","\n","# Plot histogram for overall distribution\n","values = [tot_count_n_250+tot_count_n_128, tot_count_s_250+tot_count_s_128, tot_count_v_250+tot_count_v_128]\n","plot_label_distribution(values, 'Histogram of Labels for all recordings')"]},{"cell_type":"markdown","metadata":{"id":"lEweQzO8w0h4"},"source":["# Remove patients having only N type beats\n","\n","Given the large disproportion of classes, patients showing only normal beats are removed as they only carry redundant information."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-01T19:44:04.813258Z","iopub.status.busy":"2024-02-01T19:44:04.812674Z","iopub.status.idle":"2024-02-01T19:44:04.882090Z","shell.execute_reply":"2024-02-01T19:44:04.881219Z","shell.execute_reply.started":"2024-02-01T19:44:04.813222Z"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1706388750649,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"7x0e4zAYyHLl","outputId":"67128237-9479-4350-a83f-52dc86c3149f","trusted":true},"outputs":[],"source":["# Checking if there are any patient with only 'n' label in the dataset with fs=128Hz\n","only_N_128 = []\n","\n","for idx, label in enumerate(labels_128):\n","    unique_labels = set(label)\n","\n","    if len(unique_labels) == 1 and 'N' in unique_labels:\n","        only_N_128.append(idx)\n","\n","if only_N_128:\n","    print(\"Patients with only 'N' labels found among 128Hz recordings at indices:\", only_N_128)\n","else:\n","    print(\"No patients with only 'N' labels found among 128Hz recordings.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-01T19:44:04.883404Z","iopub.status.busy":"2024-02-01T19:44:04.883139Z","iopub.status.idle":"2024-02-01T19:44:04.930619Z","shell.execute_reply":"2024-02-01T19:44:04.929788Z","shell.execute_reply.started":"2024-02-01T19:44:04.883381Z"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1706388750649,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"OTcMvUz4yPXR","outputId":"4eac666e-f2d2-4568-a6df-d562f18e985b","trusted":true},"outputs":[],"source":["# Checking if there are any patient with only 'n' label in the dataset with fs=250Hz\n","only_N_250 = []\n","\n","for idx, label in enumerate(labels_250):\n","  unique_labels = set(label)\n","\n","  if len(unique_labels) == 1 and 'N' in unique_labels:\n","      only_N_250.append(idx)\n","\n","if only_N_250:\n","    print(\"Patients with only 'N' labels found among 250Hz recordings at indices:\", only_N_250)\n","else:\n","    print(\"No patients with only 'N' labels found among 250Hz recordings.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-01T19:44:04.932200Z","iopub.status.busy":"2024-02-01T19:44:04.931882Z","iopub.status.idle":"2024-02-01T19:44:04.942088Z","shell.execute_reply":"2024-02-01T19:44:04.941258Z","shell.execute_reply.started":"2024-02-01T19:44:04.932170Z"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1706388750649,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"CYyqxEch3JMg","outputId":"69fb70b2-f729-40cf-b2c7-72f083b86ace","trusted":true},"outputs":[],"source":["# Remove patients from labels, recordings and peak_locations\n","locations_250 = [locations_250[i] for i in range(len(locations_250)) if i not in only_N_250]\n","recordings_250 = [recordings_250[i] for i in range(len(recordings_250)) if i not in only_N_250]\n","labels_250 = [labels_250[i] for i in range(len(labels_250)) if i not in only_N_250]\n","\n","# Check dimensionality\n","print(f\"New peaks locations dim.: {len(locations_250)}\")\n","print(f\"New recordings dim.: {len(recordings_250)}\")\n","print(f\"New labels dim.:{len(labels_250)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-01T19:44:04.943646Z","iopub.status.busy":"2024-02-01T19:44:04.943331Z","iopub.status.idle":"2024-02-01T19:44:04.958026Z","shell.execute_reply":"2024-02-01T19:44:04.957218Z","shell.execute_reply.started":"2024-02-01T19:44:04.943617Z"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1706388750649,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"dEeWkFyHtV3N","outputId":"e707f458-65a8-4fac-869c-ae51ae025226","trusted":true},"outputs":[],"source":["# Check new label distribution in 250Hz samples\n","tot_count_n_250, tot_count_s_250, tot_count_v_250 = calculate_label_distribution(labels_250)\n","print(f\"Signals 250Hz: {tot_count_n_250} N beats, {tot_count_s_250} S beats, {tot_count_v_250} V beats\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":927},"execution":{"iopub.execute_input":"2024-02-01T19:44:04.963272Z","iopub.status.busy":"2024-02-01T19:44:04.963005Z","iopub.status.idle":"2024-02-01T19:44:05.285312Z","shell.execute_reply":"2024-02-01T19:44:05.284425Z","shell.execute_reply.started":"2024-02-01T19:44:04.963241Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706388750649,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"JRehbsYztaVX","outputId":"7843912b-7476-4a1b-fc23-5bcfe58e5b76","trusted":true},"outputs":[],"source":["# Visualize the new label distribution\n","\n","# Plot histogram for 250Hz\n","values_250 = [tot_count_n_250, tot_count_s_250, tot_count_v_250]\n","plot_label_distribution(values_250, 'Histogram of Labels for 250Hz recordings')\n","\n","# Plot histogram for overall distribution\n","values = [tot_count_n_250+tot_count_n_128, tot_count_s_250+tot_count_s_128, tot_count_v_250+tot_count_v_128]\n","plot_label_distribution(values, 'Histogram of Labels for all recordings')"]},{"cell_type":"markdown","metadata":{"id":"BZBdAfqbHMFd"},"source":["# Signal Visualization\n","\n","A rapid signal inspection has shown the presence of many artifacts in along the recordings both for 128Hz and 250Hz samples. A further inspection aimed at assessing the labels associated to the peaks in these noisy portions."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:05.286560Z","iopub.status.busy":"2024-02-01T19:44:05.286305Z","iopub.status.idle":"2024-02-01T19:44:05.292231Z","shell.execute_reply":"2024-02-01T19:44:05.291311Z","shell.execute_reply.started":"2024-02-01T19:44:05.286536Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706388750650,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"ZuRT1bXa80Ig","trusted":true},"outputs":[],"source":["# define function to plot signals over a given time range\n","def plot_signal(signal, seconds, fs, offset=0):\n","  t = np.arange(offset ,offset+seconds,1/fs)\n","  fig, axs = plt.subplots()\n","  #axs.plot(t, signal[:len(t)], color='C0')\n","  axs.plot(t, signal[offset*fs:(offset+seconds)*fs], color='C0')\n","  axs.set_xlabel(\"Time [s]\")\n","  axs.set_ylabel(\"Amplitude [mV]\")\n","  plt.title('PPG recording')\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"execution":{"iopub.execute_input":"2024-02-01T19:44:05.293647Z","iopub.status.busy":"2024-02-01T19:44:05.293393Z","iopub.status.idle":"2024-02-01T19:44:05.508045Z","shell.execute_reply":"2024-02-01T19:44:05.507254Z","shell.execute_reply.started":"2024-02-01T19:44:05.293624Z"},"executionInfo":{"elapsed":1069,"status":"ok","timestamp":1706388751714,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"9wNvrDfj80K2","outputId":"cf7d16d3-8fdd-4ac5-8a20-39adf1f59f3c","trusted":true},"outputs":[],"source":["# Show 128Hz signal\n","plot_signal(recordings_128[0],20,128)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"execution":{"iopub.execute_input":"2024-02-01T19:44:05.509277Z","iopub.status.busy":"2024-02-01T19:44:05.509022Z","iopub.status.idle":"2024-02-01T19:44:05.764153Z","shell.execute_reply":"2024-02-01T19:44:05.763272Z","shell.execute_reply.started":"2024-02-01T19:44:05.509253Z"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1706388751714,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"AyjUWYzB80M_","outputId":"e44100bc-1e70-40f9-9ef4-beb1c077589b","trusted":true},"outputs":[],"source":["# Show 250Hz signal\n","plot_signal(recordings_250[0],20,250)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:05.766835Z","iopub.status.busy":"2024-02-01T19:44:05.765432Z","iopub.status.idle":"2024-02-01T19:44:05.774014Z","shell.execute_reply":"2024-02-01T19:44:05.772931Z","shell.execute_reply.started":"2024-02-01T19:44:05.766804Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706388751714,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"x7OiPBuUfoHU","trusted":true},"outputs":[],"source":["# plot the signal with the corresponding peaks\n","def plot_signal_with_peaks(signal, peak_locations, fs):\n","    # Define the time axis\n","    t = np.arange(0, len(signal) / fs, 1 / fs)\n","\n","    # Plot the signal\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(t, signal, color='C0', label='Signal')\n","\n","    # Plot the peak locations\n","    peak_times = np.array(peak_locations) / fs\n","    plt.scatter(peak_times, signal[peak_locations], color='red', label='Peak Locations')\n","\n","    # Set the x-axis label and title\n","    plt.xlabel('Time (s)')\n","    plt.ylabel('Amplitude')\n","    plt.title('Signal with Peak Locations')\n","\n","    # Show the legend\n","    plt.legend()\n","\n","    # Display the plot\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":552},"execution":{"iopub.execute_input":"2024-02-01T19:44:05.775682Z","iopub.status.busy":"2024-02-01T19:44:05.775329Z","iopub.status.idle":"2024-02-01T19:44:06.060961Z","shell.execute_reply":"2024-02-01T19:44:06.059993Z","shell.execute_reply.started":"2024-02-01T19:44:05.775654Z"},"executionInfo":{"elapsed":963,"status":"ok","timestamp":1706388752672,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"C4yu3YRtf0tE","outputId":"53a2e32e-fbb5-4682-da14-e34c45679b21","trusted":true},"outputs":[],"source":["plot_signal_with_peaks(recordings_250[0][:2000], locations_250[0][:10], 250)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:06.062414Z","iopub.status.busy":"2024-02-01T19:44:06.062127Z","iopub.status.idle":"2024-02-01T19:44:06.069489Z","shell.execute_reply":"2024-02-01T19:44:06.068564Z","shell.execute_reply.started":"2024-02-01T19:44:06.062390Z"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1706388752672,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"Ex6iLu3NgdNe","trusted":true},"outputs":[],"source":["# plot the signal with the labelled peaks\n","def plot_signal_with_labelled_peaks(signal, peak_locations, labels, fs):\n","    # Define the time axis\n","    t = np.arange(0, len(signal) / fs, 1 / fs)\n","\n","    # Plot the signal\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(t, signal, color='blue', label='Signal')\n","\n","    # Plot the peak locations with different colors based on the label\n","    for i, peak_loc in enumerate(peak_locations):\n","        if labels[i] == 'N':\n","            color = 'blue'\n","        elif labels[i] == 'V':\n","            color = 'red'\n","        elif labels[i] == 'S':\n","            color = 'green'\n","        else:\n","            color = 'black'\n","        plt.scatter(t[peak_loc], signal[peak_loc], color=color)\n","\n","    # Set the x-axis label and title\n","    plt.xlabel('Time (s)')\n","    plt.ylabel('Amplitude [mV]')\n","    plt.title('Signal with Labelled Peak Locations')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":558},"execution":{"iopub.execute_input":"2024-02-01T19:44:06.070909Z","iopub.status.busy":"2024-02-01T19:44:06.070623Z","iopub.status.idle":"2024-02-01T19:44:06.431101Z","shell.execute_reply":"2024-02-01T19:44:06.430268Z","shell.execute_reply.started":"2024-02-01T19:44:06.070881Z"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1706388752672,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"w044dXWkgeS_","outputId":"c58e21ca-0dc5-4ce9-a799-c976d919b583","trusted":true},"outputs":[],"source":["plot_signal_with_labelled_peaks(recordings_128[1][:1000], locations_128[1][:10],labels_128[1][:10], 128)"]},{"cell_type":"markdown","metadata":{"id":"AbMd_L0sLdwA"},"source":["# Signal Pre-Processing\n","\n","Given the different sampling frequencies, resampling is performed to equalize them. In the analysis, __downsampling__ the 250Hz signals to 128Hz is performed for three main reasons:\n","1. It allows to decrease the computational costs.\n","2. It allows for lower memory requirements\n","3. The majority of signals are sampled at 128Hz.\n","\n","Note that when downsampling from 250Hz to 128Hz we need to ensure that the original signal does not contain frequencies above 64Hz, in accordance with the sampling theorem. For this reason, the 250Hz signals' periodograms have been analyzed and it was observed that on average most of the frequency content of the signals is contained between 0 and 3 Hz."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:06.432615Z","iopub.status.busy":"2024-02-01T19:44:06.432349Z","iopub.status.idle":"2024-02-01T19:44:06.788961Z","shell.execute_reply":"2024-02-01T19:44:06.788132Z","shell.execute_reply.started":"2024-02-01T19:44:06.432591Z"},"executionInfo":{"elapsed":580,"status":"ok","timestamp":1706388753246,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"AkQyPoy0SXZe","trusted":true},"outputs":[],"source":["from scipy.signal import periodogram\n","\n","# plot a signal and the corresponding periodogram\n","def plot_signal_and_periodogram(signal, fs):\n","  frequencies, Pxx = periodogram(signal.flatten(), fs)\n","\n","  # Plot the signal and its periodogram\n","  plt.figure(figsize=(12, 6))\n","\n","  if 1:\n","    # Plot the signal\n","    plt.subplot(2, 1, 1)\n","    t = np.arange(0, len(signal) / fs, 1 / fs)\n","    plt.plot(t, signal)\n","    plt.title('Original Signal')\n","    plt.xlabel('Time (s)')\n","    plt.ylabel('Amplitude [mV]')\n","\n","  # Plot the periodogram\n","  plt.subplot(2, 1, 2)\n","  plt.plot(frequencies, Pxx)\n","  plt.title('Periodogram of the Signal')\n","  plt.xlabel('Frequency (Hz)')\n","  plt.ylabel('Power/Frequency (dB/Hz)')\n","\n","  # Limit the x-axis to 10\n","  plt.xlim(0, 10)\n","\n","  plt.tight_layout()\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:06.796626Z","iopub.status.busy":"2024-02-01T19:44:06.795932Z","iopub.status.idle":"2024-02-01T19:44:07.209623Z","shell.execute_reply":"2024-02-01T19:44:07.208816Z","shell.execute_reply.started":"2024-02-01T19:44:06.796587Z"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1706388753247,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"M3Osg5eDW40H","trusted":true},"outputs":[],"source":["# downsample 250Hz signals to 128Hz\n","from scipy.signal import resample\n","\n","# Define the target sampling frequency\n","fs = 250\n","target_fs = 128\n","\n","# Downsample the signals\n","downsampled_signals = [resample(signal, int(len(signal) * target_fs / fs)) for signal in recordings_250]\n","# Modify locations_250 to match downsampled_signals\n","downsampled_locations = [np.round(location * target_fs / fs).astype(int) for location in locations_250]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":552},"execution":{"iopub.execute_input":"2024-02-01T19:44:07.211160Z","iopub.status.busy":"2024-02-01T19:44:07.210810Z","iopub.status.idle":"2024-02-01T19:44:07.491190Z","shell.execute_reply":"2024-02-01T19:44:07.490389Z","shell.execute_reply.started":"2024-02-01T19:44:07.211128Z"},"executionInfo":{"elapsed":753,"status":"ok","timestamp":1706388753996,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"yuTwou7mk2H2","outputId":"5bfc8107-d948-47f0-ccd0-f7663b985958","trusted":true},"outputs":[],"source":["# Check the downsampled signal with the corresponding peaks\n","plot_signal_with_peaks(downsampled_signals[0][:2000], downsampled_locations[0][:10], 128)"]},{"cell_type":"markdown","metadata":{"id":"QxvPFClTSgVX"},"source":["# Individual signal check\n","Visualization of all the signals composing the dataset is performed to avoid including any \"outlier\", meaning recordings not showing clear waveforms."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:07.503851Z","iopub.status.busy":"2024-02-01T19:44:07.503587Z","iopub.status.idle":"2024-02-01T19:44:07.514169Z","shell.execute_reply":"2024-02-01T19:44:07.513276Z","shell.execute_reply.started":"2024-02-01T19:44:07.503828Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706388753996,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"1_S9cLT2Q6wL","trusted":true},"outputs":[],"source":["# Plot signals in downsampled_signals\n","fs = 128\n","for i, signal in enumerate(downsampled_signals):\n","    print(f\"Downsampled signal {i}\")\n","    plot_signal(signal, 20, fs, offset = 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:07.515393Z","iopub.status.busy":"2024-02-01T19:44:07.515139Z","iopub.status.idle":"2024-02-01T19:44:07.525424Z","shell.execute_reply":"2024-02-01T19:44:07.524643Z","shell.execute_reply.started":"2024-02-01T19:44:07.515370Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706388753996,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"BAg4rAI2Q8Wc","trusted":true},"outputs":[],"source":["# Plot signals in recordings_128\n","for i, signal in enumerate(recordings_128):\n","  print(f\"Original 128Hz signal {i}\")\n","  plot_signal(signal, 20, fs, offset = 100)"]},{"cell_type":"markdown","metadata":{"id":"ETKR4LSfo0xC"},"source":["## Filtering and artifact removal"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:07.526692Z","iopub.status.busy":"2024-02-01T19:44:07.526437Z","iopub.status.idle":"2024-02-01T19:44:08.059586Z","shell.execute_reply":"2024-02-01T19:44:08.058797Z","shell.execute_reply.started":"2024-02-01T19:44:07.526670Z"},"executionInfo":{"elapsed":538,"status":"ok","timestamp":1706388754530,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"wAjkC7r-FLm-","trusted":true},"outputs":[],"source":["from scipy.signal import butter, filtfilt\n","\n","# Define the filter parameters\n","low_cutoff_frequency = 0.5  # Set the low cutoff frequency to 0.5 Hz\n","high_cutoff_frequency = 5  # Set the high cutoff frequency to 5 Hz\n","nyquist_freq = 0.5 * fs  # Nyquist frequency\n","fs = 128  # Set the sampling frequency to 128 Hz\n","filter_order = 2  # Set the filter order\n","\n","# Calculate the normalized cutoff frequencies\n","normalized_low_cutoff_frequency = low_cutoff_frequency / nyquist_freq\n","normalized_high_cutoff_frequency = high_cutoff_frequency / nyquist_freq\n","\n","# Design the Butterworth bandpass filter\n","b, a = butter(filter_order, [normalized_low_cutoff_frequency, normalized_high_cutoff_frequency], btype='band', analog=False, output='ba')\n","\n","# Recordings_128 is a list of 2D arrays\n","flattened_signals_128 = [np.squeeze(signal) for signal in recordings_128]\n","# Apply the filter to the flattened signals\n","filtered_signals_128 = [filtfilt(b, a, signal) for signal in flattened_signals_128]\n","\n","# Downsampled_signals is a list of 2D arrays\n","flattened_downsampled_signals = [np.squeeze(signal) for signal in downsampled_signals]\n","filtered_signals_downsampled = [filtfilt(b, a, signal) for signal in flattened_downsampled_signals]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:08.060937Z","iopub.status.busy":"2024-02-01T19:44:08.060663Z","iopub.status.idle":"2024-02-01T19:44:08.067493Z","shell.execute_reply":"2024-02-01T19:44:08.066647Z","shell.execute_reply.started":"2024-02-01T19:44:08.060912Z"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706388754531,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"8CDKyl_jz9qR","trusted":true},"outputs":[],"source":["def plot_signals_overlapped(signal_1, signal_2, seconds, fs,  offset=0):\n","    t = np.arange(offset,offset+seconds,1/fs)\n","\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(t, signal_1[offset*fs:(offset+seconds)*fs], label='Signal 1', color='C0')\n","    plt.plot(t, signal_2[offset*fs:(offset+seconds)*fs], label='Signal 2', color='red')\n","    plt.xlabel('Time (s)')\n","    plt.ylabel('Amplitude [mV]')\n","    plt.title('Signal Overlap')\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:08.068738Z","iopub.status.busy":"2024-02-01T19:44:08.068496Z","iopub.status.idle":"2024-02-01T19:44:08.079561Z","shell.execute_reply":"2024-02-01T19:44:08.078775Z","shell.execute_reply.started":"2024-02-01T19:44:08.068716Z"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706388754531,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"OoQ9Q7fBcCYL","trusted":true},"outputs":[],"source":["# show signal vs. filtered signal\n","fs = 128\n","for i, (signal, filtered_signal) in enumerate(zip(recordings_128,filtered_signals_128)):\n","  print(f\"Original vs Filtered 128Hz signal {i}\")\n","  plot_signals_overlapped(signal, filtered_signal, 20, fs, offset=300)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:08.087832Z","iopub.status.busy":"2024-02-01T19:44:08.087563Z","iopub.status.idle":"2024-02-01T19:44:08.091866Z","shell.execute_reply":"2024-02-01T19:44:08.091001Z","shell.execute_reply.started":"2024-02-01T19:44:08.087809Z"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706388754531,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"C-UVPMykVqUm","trusted":true},"outputs":[],"source":["# show signal vs. filtered signal\n","for i, (signal, filtered_signal) in enumerate(zip(downsampled_signals,filtered_signals_downsampled)):\n","  print(f\"Downsampled vs Downsampled and Filtered signal {i}\")\n","  plot_signals_overlapped(signal, filtered_signal, 20, fs)"]},{"cell_type":"markdown","metadata":{"id":"eCS53dW0K354"},"source":["# Artifacts Detection with Derivatives"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":526},"execution":{"iopub.execute_input":"2024-02-01T19:44:08.330708Z","iopub.status.busy":"2024-02-01T19:44:08.330421Z","iopub.status.idle":"2024-02-01T19:44:08.912552Z","shell.execute_reply":"2024-02-01T19:44:08.911529Z","shell.execute_reply.started":"2024-02-01T19:44:08.330684Z"},"executionInfo":{"elapsed":999,"status":"ok","timestamp":1706388755528,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"9CnkmHXFK8eY","outputId":"33de5a0b-6f48-4536-a6f5-7a1b51b18036","trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def remove_noise_derivative(ppg_signal, threshold, distance_threshold):\n","    # Calculate the first derivative of the signal\n","    first_derivative = np.gradient(ppg_signal)\n","\n","\n","    # Identify indices where the absolute value of the derivative exceeds the threshold\n","    noisy_indices = np.where(np.abs(first_derivative) > threshold)[0]\n","\n","    # Create a mask to exclude noisy portions\n","    mask = np.ones_like(ppg_signal, dtype=bool)\n","    mask[noisy_indices] = False\n","\n","    # Set values in noisy portions to 0\n","    cleaned_ppg = np.where(mask, ppg_signal, 0)\n","\n","\n","    # Set values to 0 between two noisy indices if their distance is less than distance_threshold\n","    for i in range(len(noisy_indices) - 1):\n","        if (noisy_indices[i + 1] - noisy_indices[i]) < distance_threshold:\n","            cleaned_ppg[noisy_indices[i]:noisy_indices[i + 1] + 1] = 0\n","\n","    return cleaned_ppg\n","\n","# Set a threshold for noise removal based on the derivative\n","derivative_threshold = 0.4\n","distance_threshold=50 \n","\n","# Remove noisy portions from the PPG signal using the derivative-based method\n","cleaned_ppg = remove_noise_derivative(filtered_signals_128[0], derivative_threshold, distance_threshold)\n","\n","# Plot the original and cleaned signals\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(0,len(filtered_signals_128[0])), filtered_signals_128[0], label='Original PPG')\n","plt.plot(range(0,len(filtered_signals_128[0])), cleaned_ppg, label='Cleaned PPG', linestyle='--')\n","plt.xlabel('Samples')\n","plt.ylabel('Amplitude')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:08.923835Z","iopub.status.busy":"2024-02-01T19:44:08.923511Z","iopub.status.idle":"2024-02-01T19:44:08.931296Z","shell.execute_reply":"2024-02-01T19:44:08.930549Z","shell.execute_reply.started":"2024-02-01T19:44:08.923803Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706388755529,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"gAsUlZsPMJnr","trusted":true},"outputs":[],"source":["offset=0\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(offset,offset+2000), filtered_signals_128[0][offset:offset+2000], label='Original PPG')\n","plt.plot(range(offset,offset+2000), cleaned_ppg[offset:offset+2000], label='Cleaned PPG', linestyle='--')\n","for i in range(40, 70):\n","    plt.scatter(locations_128[0][i], cleaned_ppg[locations_128[0][i]])\n","plt.xlabel('Time')\n","plt.ylabel('Amplitude')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"tQsvJGC99_Qg"},"source":["## Artifacts removal from 128 Hz signals"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-01T19:44:08.932987Z","iopub.status.busy":"2024-02-01T19:44:08.932489Z","iopub.status.idle":"2024-02-01T19:44:11.265160Z","shell.execute_reply":"2024-02-01T19:44:11.264248Z","shell.execute_reply.started":"2024-02-01T19:44:08.932954Z"},"executionInfo":{"elapsed":2955,"status":"ok","timestamp":1706388758479,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"n0jOeGQoV0xe","outputId":"41e10949-ccea-4f05-9bc7-06a10863d971","trusted":true},"outputs":[],"source":["no_noise=[]\n","for i in range(0,len(filtered_signals_128)):\n","  cleaned_ppg = remove_noise_derivative(filtered_signals_128[i], derivative_threshold, distance_threshold)\n","  no_noise.append(cleaned_ppg)\n","\n","\n","len(no_noise)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:11.266659Z","iopub.status.busy":"2024-02-01T19:44:11.266377Z","iopub.status.idle":"2024-02-01T19:44:11.270937Z","shell.execute_reply":"2024-02-01T19:44:11.269916Z","shell.execute_reply.started":"2024-02-01T19:44:11.266634Z"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706388758479,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"xOMfV7hL-dzc","trusted":true},"outputs":[],"source":["#plot cleaned and standardized\n","offset=4000\n","for i in range(0,len(no_noise)):\n","   plt.figure(figsize=(10, 6))\n","   plt.plot(range(offset,offset+2000), filtered_signals_128[i][offset:offset+2000], label='Original PPG')\n","   plt.plot(range(offset,offset+2000), no_noise[i][offset:offset+2000], label='Cleaned PPG', linestyle='--')\n","   plt.xlabel('Time')\n","   plt.ylabel('Amplitude')\n","   plt.legend()\n","   plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Artifacts removal from downsampled signals"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-01T19:44:11.272820Z","iopub.status.busy":"2024-02-01T19:44:11.272237Z","iopub.status.idle":"2024-02-01T19:44:12.506489Z","shell.execute_reply":"2024-02-01T19:44:12.505499Z","shell.execute_reply.started":"2024-02-01T19:44:11.272767Z"},"executionInfo":{"elapsed":1194,"status":"ok","timestamp":1706388759671,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"TytuZ8st9-DN","outputId":"436c414a-dd4a-484b-ff41-a031eba229ac","trusted":true},"outputs":[],"source":["no_noise_downs=[]\n","for i in range(0,len(filtered_signals_downsampled)):\n","   cleaned_ppg = remove_noise_derivative(filtered_signals_downsampled[i], derivative_threshold, distance_threshold)\n","   no_noise_downs.append(cleaned_ppg)\n","\n","\n","len(no_noise_downs)"]},{"cell_type":"markdown","metadata":{},"source":["# Save cleaned PPG"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save(no_noise, \"cleaned_128\")\n","save(no_noise_downs, \"cleaned_down\")\n","save(locations_128, \"clean_peak_128\")\n","save(downsampled_locations, \"clean_peak_down\")\n","save(labels_128, \"clean_label_128\")\n","save(labels_250, \"clean_label_down\")"]},{"cell_type":"markdown","metadata":{"id":"yg91IXmkuqBK"},"source":["# Beat Segmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:12.508917Z","iopub.status.busy":"2024-02-01T19:44:12.508115Z","iopub.status.idle":"2024-02-01T19:44:12.516760Z","shell.execute_reply":"2024-02-01T19:44:12.515875Z","shell.execute_reply.started":"2024-02-01T19:44:12.508878Z"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1706388759671,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"nB-LZP7T_iAM","trusted":true},"outputs":[],"source":["# Define a Patient class so that train-validation-test split can be performed easily later\n","class Patient:\n","    def __init__(self,\n","                 single_beats=None, contiguous_beats=None,\n","                 mean=None, std=None, amplitude=None, peak_value=None,\n","                 pre_PP=None, post_PP=None, avg_PP=None, width=None, FWHM=None,\n","                 skewness=None, pre_skew=None, post_skew=None,\n","                 kurtosis=None, pre_kurt=None, post_kurt=None, \n","                 entropy=None, RMS=None, neg_neg_jump=None, \n","                 pre_pos_pos_jump=None, post_pos_pos_jump=None,\n","                 rise_time=None, fall_time=None, area=None, local_hrv=None,\n","                 energy=None, dominant_frequency=None, \n","                 labels=None, peak_locations=None):\n","\n","        # Segmented beats\n","        self.single_beats = single_beats if single_beats is not None else []\n","        self.contiguous_beats = contiguous_beats if contiguous_beats is not None else []\n","        # Features\n","        self.mean = mean if mean is not None else []\n","        self.std = std if std is not None else []\n","        self.amplitude = amplitude if amplitude is not None else []\n","        self.peak_value = peak_value if peak_value is not None else []\n","        self.pre_PP = pre_PP if pre_PP is not None else []\n","        self.post_PP = post_PP if post_PP is not None else []\n","        self.avg_PP = avg_PP if avg_PP is not None else []\n","        self.width = width if width is not None else []\n","        self.FWHM = FWHM if FWHM is not None else []\n","        self.skewness = skewness if skewness is not None else []\n","        self.pre_skew = pre_skew if pre_skew is not None else []\n","        self.post_skew = post_skew if post_skew is not None else []\n","        self.kurtosis = kurtosis if kurtosis is not None else []\n","        self.pre_kurt = pre_kurt if pre_kurt is not None else []\n","        self.post_kurt = post_kurt if post_kurt is not None else []\n","        self.entropy = entropy if entropy is not None else []\n","        self.RMS = RMS if RMS is not None else []\n","        self.neg_neg_jump = neg_neg_jump if neg_neg_jump is not None else []\n","        self.pre_pos_pos_jump = pre_pos_pos_jump if pre_pos_pos_jump is not None else []\n","        self.post_pos_pos_jump = post_pos_pos_jump if post_pos_pos_jump is not None else []\n","        self.rise_time = rise_time if rise_time is not None else []\n","        self.fall_time = fall_time if fall_time is not None else []\n","        self.area = area if area is not None else []\n","        self.local_hrv = local_hrv if local_hrv is not None else []\n","        self.energy = energy if energy is not None else []\n","        self.dominant_frequency = dominant_frequency if dominant_frequency is not None else []\n","        # Labels and peak locations\n","        self.labels = labels if labels is not None else []\n","        self.peak_locations = peak_locations if peak_locations is not None else []"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:12.518361Z","iopub.status.busy":"2024-02-01T19:44:12.518009Z","iopub.status.idle":"2024-02-01T19:44:12.533553Z","shell.execute_reply":"2024-02-01T19:44:12.532711Z","shell.execute_reply.started":"2024-02-01T19:44:12.518328Z"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1706388759671,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"eQMZVyA3_khR","trusted":true},"outputs":[],"source":["# Initialize the patient instances as empty lists\n","NUM_PATIENTS = len(no_noise) + len(no_noise_downs)\n","\n","patient_instances = [Patient() for _ in range(NUM_PATIENTS)]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:12.535148Z","iopub.status.busy":"2024-02-01T19:44:12.534894Z","iopub.status.idle":"2024-02-01T19:44:12.548701Z","shell.execute_reply":"2024-02-01T19:44:12.547826Z","shell.execute_reply.started":"2024-02-01T19:44:12.535127Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706388759671,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"aOXLmk2l0-C-","trusted":true},"outputs":[],"source":["# Define a function to extract single beats from a signal using a fixed window size\n","def extract_single_beats_statically(signal, peak_locations, window_size=100, start_ratio=0.35, end_ratio=0.65):\n","    \"\"\"\n","    Extracts single beats from a given signal based on peak locations.\n","\n","    Args:\n","        signal (array-like): The input signal.\n","        peak_locations (array-like): The locations of the peaks in the signal.\n","        window_size (int, optional): The size of the window around each peak to extract the beat.\n","            Defaults to 80.\n","        start_ratio (float, optional): The ratio of the window size to use as the starting point of the beat extraction.\n","            Defaults to 0.35.\n","        end_ratio (float, optional): The ratio of the window size to use as the ending point of the beat extraction.\n","            Defaults to 0.65.\n","\n","    Returns:\n","        list: A list of extracted beats.\n","        list: A list of peak positions relative to the start of the window.\n","    \"\"\"\n","    beats = []\n","    peak_positions = []\n","\n","    # Segment the beats\n","    for peak in peak_locations:\n","        start = int(max(0, peak - window_size*start_ratio))\n","        end = int(min(len(signal), peak + window_size*end_ratio))\n","        beat = signal[start:end]\n","        if start > 0:\n","              peak_position = int(window_size * start_ratio)\n","        else: peak_position = peak\n","\n","        if start!=end & peak_position<len(beat):\n","          beats.append(beat)\n","          # Calculate the relative position of the peak within the window\n","          peak_positions.append(peak_position)\n","\n","    return beats, peak_positions\n","\n","# Define a function to extract single beats from a signal using a dynamic window size\n","def extract_single_beats_dynamically(signal, peak_locations, start_ratio=0.35, end_ratio=0.65):\n","    \"\"\"\n","    Extracts beats from a signal based on the locations of the peaks.\n","\n","    Args:\n","        signal (list): The input signal.\n","        peak_locations (list): The locations of the peaks in the signal.\n","        start_ratio (float, optional): The ratio of the window size to use as the starting point of the beat extraction.\n","            Defaults to 0.35.\n","        end_ratio (float, optional): The ratio of the window size to use as the ending point of the beat extraction.\n","            Defaults to 0.65.\n","\n","    Returns:\n","        list: A list of beats extracted from the signal.\n","        list: A list of peak positions relative to the start of the window.\n","    \"\"\"\n","    beats = []\n","    peak_positions = []\n","    for i in range(len(peak_locations)):\n","        if i == 0:\n","            window_size = peak_locations[i+1] - peak_locations[i]\n","        elif i == len(peak_locations) - 1:\n","            window_size = peak_locations[i] - peak_locations[i-1]\n","        else:\n","            window_size = min(peak_locations[i+1] - peak_locations[i], peak_locations[i] - peak_locations[i-1])\n","\n","        start = int(max(0, peak_locations[i] - window_size*start_ratio))\n","        end = int(min(len(signal), peak_locations[i] + window_size*end_ratio))\n","        beat = signal[start:end]\n","        if start!=end:\n","          beats.append(beat)\n","          # Calculate the relative position of the peak within the window\n","          if start > 0:\n","              peak_position = int(window_size * start_ratio)\n","          else:\n","              peak_position = peak_locations[i]\n","          peak_positions.append(peak_position)\n","\n","    return beats, peak_positions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define a function to extract contiguous beats from a signal\n","def extract_contiguous_beats_dynamically(signal, peak_locations):\n","    \"\"\"\n","    Extracts single beats from a given signal based on peak locations.\n","\n","    Args:\n","        signal (array-like): The input signal.\n","        peak_locations (array-like): The locations of the peaks in the signal.\n","\n","    Returns:\n","        list: A list of extracted beats.\n","\n","    \"\"\"\n","    beats = []\n","    for i in range(len(peak_locations)):\n","        if i == 0:\n","            preceding_window_size = peak_locations[i]\n","        else:\n","            preceding_window_size = max(0, peak_locations[i] - peak_locations[i-1])\n","            preceding_window_size = preceding_window_size * (0.25 + 1)\n","\n","        if i == len(peak_locations) - 1:\n","            succeeding_window_size = (len(signal) - peak_locations[i])\n","        else:\n","            succeeding_window_size = min(len(signal), peak_locations[i+1] - peak_locations[i])\n","            succeeding_window_size = succeeding_window_size * (0.25 + 1)\n","\n","        start = int(max(0, peak_locations[i] - preceding_window_size))\n","        end = int(min(len(signal), peak_locations[i] + succeeding_window_size))\n","        beat = signal[start:end]\n","        if start!=end:\n","          beats.append(beat)\n","\n","    return beats\n","\n","def extract_contiguous_beats_statically(signal, peak_locations, window_size=200, start_ratio=0.5, end_ratio=0.5):\n","    \"\"\"\n","    Extracts contiguous beats from a given signal based on peak locations.\n","\n","    Args:\n","        signal (array-like): The input signal.\n","        peak_locations (array-like): The locations of the peaks in the signal.\n","        window_size (int, optional): The size of the window around each peak to extract the beat.\n","            Defaults to 200.\n","        start_ratio (float, optional): The ratio of the window size to use as the starting point of the beat extraction.\n","            Defaults to 0.5.\n","        end_ratio (float, optional): The ratio of the window size to use as the ending point of the beat extraction.\n","            Defaults to 0.5.\n","\n","    Returns:\n","        list: A list of extracted beats.\n","\n","    \"\"\"\n","    beats = []\n","    for peak in peak_locations:\n","        start = int(max(0, peak - window_size*start_ratio))\n","        end = int(min(len(signal), peak + window_size*end_ratio))\n","        beat = signal[start:end]\n","        if start!=end:\n","          beats.append(beat)\n","\n","    return beats"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:12.568872Z","iopub.status.busy":"2024-02-01T19:44:12.568602Z","iopub.status.idle":"2024-02-01T19:44:12.584030Z","shell.execute_reply":"2024-02-01T19:44:12.583136Z","shell.execute_reply.started":"2024-02-01T19:44:12.568849Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706388759671,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"D8pz3cNO_o8V","trusted":true},"outputs":[],"source":["# Define a function for the visualization of beats\n","def plot_beat_with_peak(beats, positions, idx=None):\n","    \"\"\"\n","    Plots a beat with the peak location.\n","\n","    Args:\n","        beats (array-like): The input beats.\n","        positions (array-like): The locations of the peaks in the beats.\n","        idx (int, optional): The index of the beat to plot. Defaults to None.\n","    \"\"\"\n","    if idx is None:\n","        idx = np.random.randint(len(beats))\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(beats[idx])\n","    plt.scatter(positions[idx], beats[idx][positions[idx]], color='red')\n","    plt.xlabel('Time (s)')\n","    plt.ylabel('Amplitude [mV]')\n","    plt.title(f'Beat {idx} with Peak Location')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Feature extraction\n","def get_beat_mean(beats):\n","    \"\"\"\n","    Calculates the mean of the beats.\n","\n","    Args:\n","        beats (array-like): The input beats.\n","\n","    Returns:\n","        array-like: The mean of the beats.\n","    \"\"\"\n","    mean_array = []\n","    for beat in beats:\n","        mean = np.mean(beat)\n","        mean_array.append(mean)\n","    return mean_array\n","\n","def get_beat_std(beats):\n","    \"\"\"\n","    Calculates the standard deviation of the beats.\n","\n","    Args:\n","        beats (array-like): The input beats.\n","\n","    Returns:\n","        array-like: The standard deviation of the beats.\n","    \"\"\"\n","    std_array = []\n","    for beat in beats:\n","        std = np.std(beat)\n","        std_array.append(std)\n","    return std_array\n","\n","def get_beat_amplitude(beats):\n","    \"\"\"\n","    Calculates the amplitude of the beats.\n","\n","    Args:\n","        beats (array-like): The input beats.\n","\n","    Returns:\n","        array-like: The amplitude of the beats.\n","    \"\"\"\n","    amplitudes = []\n","    for beat in beats:\n","        amplitudes.append(np.max(beat) - np.min(beat))\n","    return amplitudes\n","\n","def get_beat_peak_value(beats, peak_locations):\n","    \"\"\"\n","    Calculates the peak value of the beats.\n","\n","    Args:\n","        beats (array-like): The input beats.\n","        peak_locations (array-like): The locations of the peaks in the beats.\n","\n","    Returns:\n","        array-like: The peak value of the beats.\n","    \"\"\"\n","    peak_values = []\n","    for beat, peak_location in zip(beats, peak_locations):\n","        peak_values.append(beat[peak_location])\n","    return peak_values\n","\n","def get_beat_pre_post_PP(peak_locations):\n","    \"\"\"\n","    Calculates the peak to peak distances of the beats.\n","\n","    Args:\n","        peak_locations (array-like): The locations of the peaks in the beats.\n","\n","    Returns:\n","        array-like: The pre-PP and post-PP of the beats in seconds.\n","    \"\"\"\n","    fs = 128\n","    pre_PPs = []\n","    post_PPs = []\n","\n","    for i in range(len(peak_locations)):\n","        if i == 0:\n","            pre_PP = None\n","        else:\n","            pre_PP = float(peak_locations[i] - peak_locations[i-1])/fs\n","\n","        if i == len(peak_locations) - 1:\n","            post_PP = None\n","        else:\n","            post_PP = float(peak_locations[i+1] - peak_locations[i])/fs\n","\n","        if(pre_PP is None):\n","            pre_PP = post_PP\n","        elif(post_PP is None):\n","            post_PP = pre_PP\n","\n","        pre_PPs.append(pre_PP)\n","        post_PPs.append(post_PP)\n","\n","    return pre_PPs, post_PPs\n","\n","def get_beat_width(beats):\n","    \"\"\"\n","    Calculates the duration of the beats.\n","\n","    Args:\n","        beats (array-like): The input beats.\n","\n","    Returns:\n","        array-like: The width value of the beats in seconds.\n","    \"\"\"\n","    fs = 128\n","    widths = []\n","    for beat in beats:\n","        widths.append(len(beat)/fs)\n","    return widths\n","\n","def get_beat_FWHM(beats):\n","    \"\"\"\n","    Calculates the Full Width at Half Maximum (FWHM) of the beats.\n","\n","    Args:\n","        beats (array-like): The input beats.\n","\n","    Returns:\n","        array-like: The FWHM value of the beats in seconds.\n","    \"\"\"\n","    fs = 128\n","    widths = []\n","    for beat in beats:\n","        max_val = np.max(beat)\n","        half_max = max_val / 2.\n","        indices = np.where(beat > half_max)[0]\n","        if len(indices) > 0:  # Check if there are any indices found\n","            fwhm = (indices[-1] - indices[0] + 1) / fs\n","            widths.append(fwhm)\n","        else:\n","            widths.append(0)  # If no indices found, append 0\n","    return widths\n","\n","def compute_rise_times(beats):\n","    \"\"\"\n","    Calculates the rise time of each beat in the list.\n","\n","    Args:\n","        beats (list of array-like): The input beats.\n","\n","    Returns:\n","        list of float: The rise times of the beats in seconds.\n","    \"\"\"\n","    fs = 128  # Sampling frequency\n","    rise_times = []\n","    for beat in beats:\n","        max_val = np.max(beat)\n","        max_index = np.argmax(beat)\n","        low_val = 0.1 * max_val\n","        high_val = 0.9 * max_val\n","\n","        indices = np.where((beat[:max_index] >= low_val) & (beat[:max_index] <= high_val))[0]\n","        if len(indices) > 0:  # Check if there are any indices found\n","            rise_time = (indices[-1] - indices[0] + 1) / fs\n","            rise_times.append(rise_time)\n","        else:\n","            rise_times.append(0)  # If no indices found, append 0\n","    return rise_times\n","\n","def compute_fall_times(beats):\n","    \"\"\"\n","    Calculates the fall time of each beat in the list.\n","\n","    Args:\n","        beats (list of array-like): The input beats.\n","\n","    Returns:\n","        list of float: The fall times of the beats in seconds.\n","    \"\"\"\n","    fs = 128  # Sampling frequency\n","    fall_times = []\n","    for beat in beats:\n","        max_val = np.max(beat)\n","        max_index = np.argmax(beat)\n","        high_val = 0.9 * max_val\n","        low_val = 0.1 * max_val\n","\n","        # Reverse the beat to calculate fall time\n","        reversed_beat = beat[max_index:][::-1]\n","        indices = np.where((reversed_beat >= low_val) & (reversed_beat <= high_val))[0]\n","        if len(indices) > 0:  # Check if there are any indices found\n","            fall_time = (indices[-1] - indices[0] + 1) / fs\n","            fall_times.append(fall_time)\n","        else:\n","            fall_times.append(0)  # If no indices found, append 0\n","    return fall_times\n","\n","def compute_negative_to_negative_peak_jump(beats):\n","    \"\"\"\n","    Calculates the difference between the beat onset and beat end values of each beat in the list.\n","\n","    Args:\n","        beats (list of array-like): The input beats.\n","\n","    Returns:\n","        list of float: The negative to negative peak jump.\n","    \"\"\"\n","    neg_jumps = []\n","    for beat in beats:\n","        max_index = np.argmax(beat)\n","        # Split the beat into two halves\n","        first_half = beat[:max_index]\n","        second_half = beat[max_index:]        \n","        # Find the minimum value in each half\n","        min_first_half = min(first_half) if first_half.size > 0 else beat[0]\n","        min_second_half = min(second_half) if second_half.size > 0 else beat[-1]\n","        # Calculate the negative to negative peak jump\n","        jump = min_second_half - min_first_half\n","        neg_jumps.append(jump)\n","\n","    return neg_jumps\n","\n","def compute_positive_to_positive_peak_jump(beats):\n","    \"\"\"\n","    Calculates the difference between successive peak values for each beat in the list.\n","\n","    Args:\n","        beats (list of array-like): The input beats.\n","\n","    Returns:\n","        list of float: The negative to negative peak jump.\n","    \"\"\"\n","    pre_pos_jumps = []\n","    post_pos_jumps = []\n","    for beat in beats:\n","        # Compute the peak value of the current beat\n","        peak_current = np.max(beats[i])\n","        # Compute the peak value of the previous beat\n","        peak_prev = np.max(beats[i - 1])\n","        # Compute the peak value of the next beat\n","        peak_next = np.max(beats[i + 1])\n","        # Compute the difference in peak values\n","        pre_diff = peak_current - peak_prev\n","        post_diff = peak_next - peak_current\n","        \n","        pre_pos_jumps.append(pre_diff)\n","        post_pos_jumps.append(post_diff)\n","\n","    return pre_pos_jumps, post_pos_jumps\n","\n","def compute_areas(beats):\n","    \"\"\"\n","    Calculates the area under each beat in the list.\n","\n","    Args:\n","        beats (list of array-like): The input beats.\n","\n","    Returns:\n","        list of float: The areas under the beats.\n","    \"\"\"\n","    areas = []\n","    for beat in beats:\n","        area = np.trapz(beat)\n","        areas.append(area)\n","    return areas\n","\n","def compute_energy(beats):\n","    \"\"\"\n","    Calculates the total energy of each beat in the list.\n","\n","    Args:\n","        beats (list of array-like): The input beats.\n","\n","    Returns:\n","        list of float: The total energy of the beats.\n","    \"\"\"\n","    energies = []\n","    for beat in beats:\n","        energy = np.sum(np.square(beat))\n","        energies.append(energy)\n","    return energies\n","\n","from scipy.stats import skew\n","\n","def compute_skewness(beats):\n","    \"\"\"\n","    Calculates the skewness of each beat in the list.\n","\n","    Args:\n","        beats (list of array-like): The input beats.\n","\n","    Returns:\n","        list of float: The skewness of the beats.\n","    \"\"\"\n","    skewness_values = []\n","    for beat in beats:\n","        skewness = skew(beat)\n","        skewness_values.append(skewness)\n","    return skewness_values\n","\n","def compute_skewness_diff(beats):\n","    \"\"\"\n","    Calculates the difference in skewness between each beat and its previous and next beat.\n","\n","    Args:\n","        beats (list of array-like): The input beats.\n","\n","    Returns:\n","        list of tuples: Each tuple contains the difference in skewness with the previous beat and the next beat.\n","    \"\"\"\n","    skewness_values = [skew(beat) for beat in beats]\n","    skewness_diffs_pre = []\n","    skewness_diffs_post = []\n","\n","    for i in range(len(skewness_values)):\n","        if i == 0:  # first beat, no previous beat\n","            prev_diff = None\n","        else:\n","            prev_diff = skewness_values[i] - skewness_values[i-1]\n","\n","        if i == len(skewness_values) - 1:  # last beat, no next beat\n","            next_diff = None\n","        else:\n","            next_diff = skewness_values[i] - skewness_values[i+1]\n","\n","        if(prev_diff is None):\n","            prev_diff = next_diff\n","        elif(next_diff is None):\n","            next_diff = prev_diff\n","        \n","        skewness_diffs_pre.append(prev_diff)\n","        skewness_diffs_post.append(next_diff)\n","\n","    return skewness_diffs_pre, skewness_diffs_post\n","\n","from scipy.stats import kurtosis\n","\n","def compute_kurtosis(beats):\n","    \"\"\"\n","    Calculates the kurtosis of each beat in the list.\n","\n","    Args:\n","        beats (list of array-like): The input beats.\n","\n","    Returns:\n","        list of float: The kurtosis of the beats.\n","    \"\"\"\n","    kurtosis_values = []\n","    for beat in beats:\n","        kurt = kurtosis(beat)\n","        kurtosis_values.append(kurt)\n","    return kurtosis_values\n","\n","def compute_kurtosis_diff(beats):\n","    \"\"\"\n","    Calculates the difference in kurtosis between each beat and its previous and next beat.\n","\n","    Args:\n","        beats (list of array-like): The input beats.\n","\n","    Returns:\n","        Two lists: Each list contains the difference in kurtosis with the previous beat and the next beat.\n","    \"\"\"\n","    kurtosis_values = [kurtosis(beat) for beat in beats]\n","    kurtosis_diffs_pre = []\n","    kurtosis_diffs_post = []\n","\n","    for i in range(len(kurtosis_values)):\n","        if i == 0:  # first beat, no previous beat\n","            prev_diff = None\n","        else:\n","            prev_diff = kurtosis_values[i] - kurtosis_values[i-1]\n","\n","        if i == len(kurtosis_values) - 1:  # last beat, no next beat\n","            next_diff = None\n","        else:\n","            next_diff = kurtosis_values[i] - kurtosis_values[i+1]\n","\n","        if(prev_diff is None):\n","            prev_diff = next_diff\n","        elif(next_diff is None):\n","            next_diff = prev_diff\n","        \n","        kurtosis_diffs_pre.append(prev_diff)\n","        kurtosis_diffs_post.append(next_diff)\n","\n","    return kurtosis_diffs_pre, kurtosis_diffs_post\n","\n","import nolds\n","\n","def compute_entropy(beats):\n","    \"\"\"\n","    Calculates the sample entropy of each beat.\n","\n","    Args:\n","        beats (list of array-like): The input beats.\n","\n","    Returns:\n","        list of float: The sample entropy of each beat.\n","    \"\"\"\n","    entropy_values = []\n","    for beat in beats:\n","        # Compute the sample entropy of the beat\n","        e = nolds.sampen(beat)\n","        entropy_values.append(e)\n","\n","    return entropy_values\n","\n","def compute_rms(beats):\n","    \"\"\"\n","    Calculates the root mean square (RMS) of each beat in the list.\n","\n","    Args:\n","        beats (list of array-like): The input beats.\n","\n","    Returns:\n","        list of float: The RMS of the beats.\n","    \"\"\"\n","    rms_values = []\n","    for beat in beats:\n","        rms = np.sqrt(np.mean(np.square(beat)))\n","        rms_values.append(rms)\n","    return rms_values\n","\n","def calculate_hrv(peak_locations, window_size=4):\n","    \"\"\"\n","    Calculate Heart Rate Variability (HRV) for each peak location within a given window size.\n","\n","    Parameters:\n","    peak_locations (list): List of peak locations.\n","    window_size (int): The size of the window to consider for each peak. Default is 4.\n","\n","    Returns:\n","    list: HRV measures for each peak.\n","    \"\"\"\n","\n","    # Initialize an empty list to store HRV measures\n","    hrv_measures = []\n","\n","    # Calculate half window size for creating centered window\n","    half_window = window_size // 2\n","\n","    # Loop over each peak location\n","    for i in range(len(peak_locations)):\n","        # Define the start and end of the window centered on the current peak\n","        window_start = max(0, i - half_window)\n","        window_end = min(len(peak_locations), i + half_window + 1)\n","\n","        # Get the peak locations within this window\n","        window_peaks = peak_locations[window_start:window_end]\n","        window_peaks_1d = window_peaks.flatten()\n","        # Calculate differences between successive peaks to get PP intervals\n","        rr_intervals = np.diff(window_peaks_1d)\n","\n","        # Calculate HRV measure for this window\n","        # Here we use the standard deviation of PP intervals as the HRV measure\n","        hrv = np.std(rr_intervals)\n","\n","        # Append the calculated HRV measure to the list\n","        hrv_measures.append(hrv)\n","\n","    # Return the list of HRV measures\n","    return hrv_measures\n","\n","def compute_avg_peak_to_peak_distance(peak_locations, window_size=4):\n","    \"\"\"\n","    Calculates the average peak-to-peak distance for each peak within a sliding window.\n","\n","    Args:\n","        peak_locations (list of int): The locations of the peaks.\n","        window_size (int, optional): The size of the sliding window. Defaults to 4.\n","\n","    Returns:\n","        list of float: The average peak-to-peak distance for each peak.\n","    \"\"\"\n","    fs = 128\n","    avg_PP_distances = []\n","    for i in range(len(peak_locations)):\n","        # Determine the start and end of the sliding window\n","        start = max(0, i - window_size)\n","        end = min(i + window_size, len(peak_locations))\n","\n","        # Extract the peak locations within the sliding window\n","        window_peaks = peak_locations[start:end]\n","        window_peaks_1d = window_peaks.flatten()\n","        # Compute the peak-to-peak distances\n","        distances = np.diff(window_peaks_1d) / fs\n","\n","        # Compute the average distance\n","        avg_distance = np.mean(distances) if distances.size else 0\n","        avg_PP_distances.append(avg_distance)\n","\n","    return avg_PP_distances\n","\n","from scipy.fftpack import fft\n","\n","def compute_dominant_frequency(beats, sample_rate):\n","    \"\"\"\n","    Calculates the dominant frequency of each beat in the list.\n","\n","    Args:\n","        beats (list of array-like): The input beats.\n","        sample_rate (float): The sample rate of the beats.\n","\n","    Returns:\n","        list of float: The dominant frequency of the beats.\n","    \"\"\"\n","    dominant_frequencies = []\n","    for beat in beats:\n","        # Compute FFT\n","        fft_vals = fft(beat)\n","\n","        # Compute absolute value of FFT\n","        abs_fft_vals = np.abs(fft_vals)\n","\n","        # Find the frequency where the absolute value of FFT is maximum\n","        dominant_frequency = np.argmax(abs_fft_vals) * sample_rate / len(beat)\n","        dominant_frequencies.append(dominant_frequency)\n","    return dominant_frequencies"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tqdm\n","# Extract beats and features from the signals\n","print(\"Extracting beats from 128Hz signals...\")\n","for i, patient_instance in tqdm.tqdm(enumerate(patient_instances[:len(no_noise)]), total=len(no_noise)):\n","    # Extract single beats from the signal\n","    single_beats, peak_locations = extract_single_beats_statically(no_noise[i],\n","                                                                   locations_128[i],\n","                                                                   window_size=100)\n","    # Extract contiguous beats from the signal\n","    contiguous_beats = extract_contiguous_beats_statically(no_noise[i],\n","                                                            locations_128[i],\n","                                                            window_size=200)\n","    # Store the beats and peak locations in the patient instance\n","    patient_instance.single_beats = single_beats\n","    patient_instance.contiguous_beats = contiguous_beats\n","    patient_instance.peak_locations = peak_locations\n","    # Store the labels in the patient instance\n","    patient_instance.labels = labels_128[i]\n","    # Calculate the features and store them in the patient instance\n","    single_beats_dynamic, peak_locations_ = extract_single_beats_dynamically(no_noise[i],\n","                                                                             locations_128[i])\n","    patient_instance.mean = get_beat_mean(single_beats_dynamic)\n","    patient_instance.std = get_beat_std(single_beats_dynamic)\n","    patient_instance.amplitude = get_beat_amplitude(single_beats_dynamic)\n","    patient_instance.peak_value = get_beat_peak_value(single_beats_dynamic, peak_locations)\n","    patient_instance.pre_PP, patient_instance.post_PP = get_beat_pre_post_PP(locations_128[i])\n","    patient_instance.avg_PP = compute_avg_peak_to_peak_distance(locations_128[i])\n","    patient_instance.width = get_beat_width(single_beats_dynamic)\n","    patient_instance.FWHM = get_beat_FWHM(single_beats_dynamic)\n","    patient_instance.rise_time = compute_rise_times(single_beats_dynamic)\n","    patient_instance.fall_time = compute_fall_times(single_beats_dynamic)\n","    patient_instance.area = compute_areas(single_beats_dynamic)\n","    patient_instance.skewness = compute_skewness(single_beats_dynamic)\n","    patient_instance.pre_skew, patient_instance.post_skew = compute_skewness_diff(single_beats_dynamic)\n","    patient_instance.kurtosis = compute_kurtosis(single_beats_dynamic)\n","    patient_instance.pre_kurt, patient_instance.post_kurt = compute_kurtosis_diff(single_beats_dynamic)\n","    patient_instance.entropy = compute_entropy(single_beats_dynamic)\n","    patient_instance.RMS = compute_rms(single_beats_dynamic)\n","    patient_instance.neg_neg_jump = compute_negative_to_negative_peak_jump(single_beats_dynamic)\n","    patient_instance.pre_pos_pos_jump, patient_instance.post_pos_pos_jump = compute_positive_to_positive_peak_jump(single_beats_dynamic)\n","    patient_instance.local_hrv = calculate_hrv(locations_128[i])\n","    patient_instance.energy = compute_energy(single_beats_dynamic)\n","    patient_instance.dominant_frequency = compute_dominant_frequency(single_beats_dynamic, 128)\n","\n","\n","print(\"Extracting beats from downsampled 250Hz signals...\")\n","for i, patient_instance in tqdm.tqdm(enumerate(patient_instances[len(no_noise):]), total=len(no_noise_downs)):\n","    # Extract single beats from the signal\n","    single_beats, peak_locations = extract_single_beats_statically(no_noise_downs[i],\n","                                                                   downsampled_locations[i],\n","                                                                   window_size=100)\n","    # Extract also beats form non-standardized signals to be used for feature extraction\n","    # single_beats_non_standardized, peak_locations_non_standardized = extract_single_beats_statically(filtered_signals_downsampled[i],\n","    #                                                                                                  downsampled_locations[i],\n","    #                                                                                                  window_size=100)\n","    # Extract contiguous beats from the signal\n","    contiguous_beats = extract_contiguous_beats_statically(no_noise_downs[i],\n","                                                            downsampled_locations[i],\n","                                                            window_size=200)\n","    # Store the beats and peak locations in the patient instance\n","    patient_instance.single_beats = single_beats\n","    patient_instance.contiguous_beats = contiguous_beats\n","    patient_instance.peak_locations = peak_locations\n","    # Store the labels in the patient instance\n","    patient_instance.labels = labels_250[i]\n","    # Calculate the features and store them in the patient instance\n","    single_beats_dynamic, peak_locations_ = extract_single_beats_dynamically(no_noise_downs[i],\n","                                                                             downsampled_locations[i])\n","    patient_instance.mean = get_beat_mean(single_beats_dynamic)\n","    patient_instance.std = get_beat_std(single_beats_dynamic)\n","    patient_instance.amplitude = get_beat_amplitude(single_beats_dynamic)\n","    patient_instance.peak_value = get_beat_peak_value(single_beats_dynamic,peak_locations )\n","    patient_instance.pre_PP, patient_instance.post_PP = get_beat_pre_post_PP(downsampled_locations[i])\n","    patient_instance.avg_PP = compute_avg_peak_to_peak_distance(downsampled_locations[i])\n","    patient_instance.width = get_beat_width(single_beats_dynamic)\n","    patient_instance.FWHM = get_beat_FWHM(single_beats_dynamic)\n","    patient_instance.rise_time = compute_rise_times(single_beats_dynamic)\n","    patient_instance.fall_time = compute_fall_times(single_beats_dynamic)\n","    patient_instance.area = compute_areas(single_beats_dynamic)\n","    patient_instance.skewness = compute_skewness(single_beats_dynamic)\n","    patient_instance.pre_skew, patient_instance.post_skew = compute_skewness_diff(single_beats_dynamic)\n","    patient_instance.kurtosis = compute_kurtosis(single_beats_dynamic)\n","    patient_instance.pre_kurt, patient_instance.post_kurt = compute_kurtosis_diff(single_beats_dynamic)\n","    patient_instance.entropy = compute_entropy(single_beats_dynamic)\n","    patient_instance.RMS = compute_rms(single_beats_dynamic)\n","    patient_instance.neg_neg_jump = compute_negative_to_negative_peak_jump(single_beats_dynamic)\n","    patient_instance.pre_pos_pos_jump, patient_instance.post_pos_pos_jump = compute_positive_to_positive_peak_jump(single_beats_dynamic)\n","    patient_instance.local_hrv = calculate_hrv(downsampled_locations[i])\n","    patient_instance.energy = compute_energy(single_beats_dynamic)\n","    patient_instance.dominant_frequency = compute_dominant_frequency(single_beats_dynamic, 128)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-01T19:44:34.174846Z","iopub.status.busy":"2024-02-01T19:44:34.174544Z","iopub.status.idle":"2024-02-01T19:44:35.232370Z","shell.execute_reply":"2024-02-01T19:44:35.231289Z","shell.execute_reply.started":"2024-02-01T19:44:34.174819Z"},"executionInfo":{"elapsed":513,"status":"ok","timestamp":1706388779624,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"b7vRjgQX1jDh","outputId":"8df5b75c-7ded-44ff-f8b4-4a1a0413aedd","trusted":true},"outputs":[],"source":["# Define a function to check if the peak position is correct\n","def check_peak_position(patients, threshold=10):\n","    \"\"\"\n","    Checks if the peak position is correct.\n","\n","    Args:\n","        patients (list): The list of patient instances.\n","        threhshold (int, optional): The threshold to use to determine if the peak position is correct. Defaults to 5.\n","    Returns:\n","        list: A list of incorrect peak positions.\n","        Each element of the list is a tuple containing the patient index and the incorrect peak positions.\n","    \"\"\"\n","    incorrect_peak_positions = []\n","    for patient_id, patient in enumerate(patients):\n","        for beat_id, (beat, peak_location)in enumerate(zip(patient.single_beats, patient.peak_locations)):\n","            peak_pos = np.argmax(beat)\n","            # Check if the peak is at the beginning or at the end of the beat\n","            if peak_pos in [0, len(beat)-1]:\n","                continue\n","            # Check if the peak is correct\n","            if not (peak_pos - threshold <= peak_location <= peak_pos + threshold):\n","                incorrect_peak_positions.append((patient_id, beat_id))\n","\n","    return incorrect_peak_positions\n","\n","# Check incorrect peak positions\n","incorrect_peak_positions = check_peak_position(patient_instances)\n","# Check dimensionality of mislabelled peaks.\n","# Note that many, but not necessarily all of these will correspond to noisy beats\n","print(f\"Num. mislabelled peaks: {len(incorrect_peak_positions)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:35.233846Z","iopub.status.busy":"2024-02-01T19:44:35.233558Z","iopub.status.idle":"2024-02-01T19:44:35.240041Z","shell.execute_reply":"2024-02-01T19:44:35.239071Z","shell.execute_reply.started":"2024-02-01T19:44:35.233820Z"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1706388779624,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"ckbZt2DB1jAB","trusted":true},"outputs":[],"source":["from collections import defaultdict\n","\n","def group_mislabelled_beats(incorrect_peak_positions):\n","    \"\"\"\n","    Groups the noisy beats based on the first index.\n","\n","    Args:\n","        noisy_beats_N (list of tuples): The noisy beats to group.\n","\n","    Returns:\n","        list of list: The grouped noisy beats.\n","    \"\"\"\n","    groups = defaultdict(list)\n","    for beat in incorrect_peak_positions:\n","        groups[beat[0]].append(beat)\n","\n","    return list(groups.values())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:35.241582Z","iopub.status.busy":"2024-02-01T19:44:35.241215Z","iopub.status.idle":"2024-02-01T19:44:35.262212Z","shell.execute_reply":"2024-02-01T19:44:35.261453Z","shell.execute_reply.started":"2024-02-01T19:44:35.241551Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706388779625,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"wDySo2i11i8-","trusted":true},"outputs":[],"source":["# Group mislabelled  beats by patients\n","grouped_misl_beats = group_mislabelled_beats(incorrect_peak_positions)\n","mislab_list = []\n","for group in grouped_misl_beats:\n","    mislab_list.append([item[1] for item in group])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-01T19:44:35.263608Z","iopub.status.busy":"2024-02-01T19:44:35.263260Z","iopub.status.idle":"2024-02-01T19:44:35.280059Z","shell.execute_reply":"2024-02-01T19:44:35.279073Z","shell.execute_reply.started":"2024-02-01T19:44:35.263581Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706388779625,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"jlroHBlq1i6J","outputId":"68be0bb5-57ba-49e0-9f06-17594661c69e","trusted":true},"outputs":[],"source":["# Check if noisy beats have been found for each patient\n","patient_indexes = []\n","for group in grouped_misl_beats:\n","    idx = [item[0] for item in group]\n","    unique_idx = set(idx)\n","    if unique_idx not in patient_indexes:\n","        patient_indexes.append(unique_idx)\n","\n","print(f\"Patient indexes: {patient_indexes}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:35.281357Z","iopub.status.busy":"2024-02-01T19:44:35.281066Z","iopub.status.idle":"2024-02-01T19:44:42.991694Z","shell.execute_reply":"2024-02-01T19:44:42.990902Z","shell.execute_reply.started":"2024-02-01T19:44:35.281334Z"},"executionInfo":{"elapsed":7786,"status":"ok","timestamp":1706388787408,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"zqfCSlJ-1ixu","trusted":true},"outputs":[],"source":["# Remove mislabelled beats\n","for i, patient in enumerate(patient_instances):\n","    if i in patient_indexes:\n","        patient.single_beats = [beat for j, beat in enumerate(patient.single_beats) if j not in mislab_list[i]]\n","        # Note that in this way the noisy beats is removed from the contiguous beats as well\n","        # Yet the previous and the following beats to the noisy one are not removed (this may need to be done)\n","        patient.contiguous_beats = [beat for j, beat in enumerate(patient.contiguous_beats) if j not in mislab_list[i]]\n","        patient.mean = [mean for j, mean in enumerate(patient.mean) if j not in mislab_list[i]]\n","        patient.std = [std for j, std in enumerate(patient.std) if j not in mislab_list[i]]\n","        patient.amplitude = [amplitude for j, amplitude in enumerate(patient.amplitude) if j not in mislab_list[i]]\n","        patient.peak_value = [peak_value for j, peak_value in enumerate(patient.peak_value) if j not in mislab_list[i]]\n","        patient.pre_PP = [pre_PP for j, pre_PP in enumerate(patient.pre_PP) if j not in mislab_list[i]]\n","        patient.post_PP = [post_PP for j, post_PP in enumerate(patient.post_PP) if j not in mislab_list[i]]\n","        patient.avg_PP = [avg_PP for j, avg_PP in enumerate(patient.avg_PP) if j not in mislab_list[i]]\n","        patient.width = [width for j, width in enumerate(patient.width) if j not in mislab_list[i]]\n","        patient.labels = [label for j, label in enumerate(patient.labels) if j not in mislab_list[i]]\n","        patient.peak_locations = [peak_location for j, peak_location in enumerate(patient.peak_locations) if j not in mislab_list[i]]\n","        patient.FWHM = [FWHM for j, FWHM in enumerate(patient.FWHM) if j not in mislab_list[i]]\n","        patient.skewness = [skewness for j, skewness in enumerate(patient.skewness) if j not in mislab_list[i]]\n","        patient.pre_skew = [pre_skew for j, pre_skew in enumerate(patient.pre_skew) if j not in mislab_list[i]]\n","        patient.post_skew = [post_skew for j, post_skew in enumerate(patient.post_skew) if j not in mislab_list[i]]\n","        patient.kurtosis = [kurtosis for j, kurtosis in enumerate(patient.kurtosis) if j not in mislab_list[i]]\n","        patient.pre_kurt = [pre_kurt for j, pre_kurt in enumerate(patient.pre_kurt) if j not in mislab_list[i]]\n","        patient.post_kurt = [post_kurt for j, post_kurt in enumerate(patient.post_kurt) if j not in mislab_list[i]]\n","        patient.entropy = [entropy for j, entropy in enumerate(patient.entropy) if j not in mislab_list[i]]\n","        patient.RMS = [RMS for j, RMS in enumerate(patient.RMS) if j not in mislab_list[i]]\n","        patient.neg_neg_jump = [neg_neg_jump for j, neg_neg_jump in enumerate(patient.neg_neg_jump) if j not in mislab_list[i]]\n","        patient.pre_pos_pos_jump = [pre_pos_pos_jump for j, pre_pos_pos_jump in enumerate(patient.pre_pos_pos_jump) if j not in mislab_list[i]]\n","        patient.post_pos_pos_jump = [post_pos_pos_jump for j, post_pos_pos_jump in enumerate(patient.post_pos_pos_jump) if j not in mislab_list[i]]\n","        patient.rise_time = [rise_time for j, rise_time in enumerate(patient.rise_time) if j not in mislab_list[i]]\n","        patient.fall_time = [fall_time for j, fall_time in enumerate(patient.fall_time) if j not in mislab_list[i]]\n","        patient.area = [area for j, area in enumerate(patient.area) if j not in mislab_list[i]]\n","        patient.local_hrv = [local_hrv for j, local_hrv in enumerate(patient.local_hrv) if j not in mislab_list[i]]\n","        patient.energy = [energy for j, energy in enumerate(patient.energy) if j not in mislab_list[i]]\n","        patient.dominant_frequency = [dominant_frequency for j, dominant_frequency in enumerate(patient.dominant_frequency) if j not in mislab_list[i]]"]},{"cell_type":"markdown","metadata":{"id":"Me2BYCT334YV"},"source":["# Zero peaks removal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-01T19:44:42.993266Z","iopub.status.busy":"2024-02-01T19:44:42.992906Z","iopub.status.idle":"2024-02-01T19:44:43.706381Z","shell.execute_reply":"2024-02-01T19:44:43.705453Z","shell.execute_reply.started":"2024-02-01T19:44:42.993220Z"},"executionInfo":{"elapsed":501,"status":"ok","timestamp":1706388787906,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"Yh2xyiuu4DAm","outputId":"93b80829-2494-47aa-f142-ea98b999c4cc","trusted":true},"outputs":[],"source":["# Define a function to check if the peak position is correct\n","def check_peak_zero(patients):\n","    \"\"\"\n","    Checks if the peak position is correct.\n","\n","    Args:\n","        patients (list): The list of patient instances.\n","        threhshold (int, optional): The threshold to use to determine if the peak position is correct. Defaults to 5.\n","    Returns:\n","        list: A list of incorrect peak positions.\n","        Each element of the list is a tuple containing the patient index and the incorrect peak positions.\n","    \"\"\"\n","    zero_peak = []\n","    for patient_id, patient in enumerate(patients):\n","        for beat_id, (beat, peak_location)in enumerate(zip(patient.single_beats, patient.peak_locations)):\n","            peak_pos = np.argmax(beat)\n","            # Check if the peak is correct\n","            if beat[peak_location]==0:\n","                zero_peak.append((patient_id, beat_id))\n","\n","    return zero_peak\n","\n","# Check incorrect peak positions\n","zero_peak = check_peak_zero(patient_instances)\n","# Check dimensionality of mislabelled peaks.\n","# Note that many, but not necessarily all of these will correspond to noisy beats\n","print(f\"Num. mislabelled peaks: {len(zero_peak)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:43.707742Z","iopub.status.busy":"2024-02-01T19:44:43.707481Z","iopub.status.idle":"2024-02-01T19:44:43.713082Z","shell.execute_reply":"2024-02-01T19:44:43.712081Z","shell.execute_reply.started":"2024-02-01T19:44:43.707719Z"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1706388787907,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"-C6nlZjl3-dh","trusted":true},"outputs":[],"source":["from collections import defaultdict\n","\n","def group_zero_beats(zero_peak):\n","    \"\"\"\n","    Groups the noisy beats based on the first index.\n","\n","    Args:\n","        noisy_beats_N (list of tuples): The noisy beats to group.\n","\n","    Returns:\n","        list of list: The grouped noisy beats.\n","    \"\"\"\n","    groups = defaultdict(list)\n","    for beat in zero_peak:\n","        groups[beat[0]].append(beat)\n","\n","    return list(groups.values())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:43.714523Z","iopub.status.busy":"2024-02-01T19:44:43.714214Z","iopub.status.idle":"2024-02-01T19:44:43.728881Z","shell.execute_reply":"2024-02-01T19:44:43.728036Z","shell.execute_reply.started":"2024-02-01T19:44:43.714499Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706388787907,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"ni4RJRcB3-Z_","trusted":true},"outputs":[],"source":["# Group N noisy beats by patients\n","zero_peak_grouped = group_zero_beats(zero_peak)\n","zero_list = []\n","for group in zero_peak_grouped:\n","    zero_list.append([item[1] for item in group])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-01T19:44:43.730415Z","iopub.status.busy":"2024-02-01T19:44:43.729972Z","iopub.status.idle":"2024-02-01T19:44:43.742512Z","shell.execute_reply":"2024-02-01T19:44:43.741652Z","shell.execute_reply.started":"2024-02-01T19:44:43.730383Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706388787907,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"V4agQcEU3-Xh","outputId":"f2d46059-cedc-497e-8097-0a3b70923142","trusted":true},"outputs":[],"source":["# Check if zero_peak beats have been found for each patient\n","patient_indexes = []\n","for group in zero_peak_grouped:\n","    idx = [item[0] for item in group]\n","    unique_idx = set(idx)\n","    if unique_idx not in patient_indexes:\n","        patient_indexes.append(unique_idx)\n","\n","print(f\"Patient indexes: {patient_indexes}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:43.743730Z","iopub.status.busy":"2024-02-01T19:44:43.743477Z","iopub.status.idle":"2024-02-01T19:44:46.886933Z","shell.execute_reply":"2024-02-01T19:44:46.886095Z","shell.execute_reply.started":"2024-02-01T19:44:43.743708Z"},"executionInfo":{"elapsed":3398,"status":"ok","timestamp":1706388791302,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"fCmj_UOi3-VG","trusted":true},"outputs":[],"source":["# Remove zero peak beats\n","for i, patient in enumerate(patient_instances):\n","    if i in patient_indexes:\n","        patient.single_beats = [beat for j, beat in enumerate(patient.single_beats) if j not in zero_list[i]]\n","        # Note that in this way the noisy beats is removed from the contiguous beats as well\n","        # Yet the previous and the following beats to the noisy one are not removed (this may need to be done)\n","        patient.contiguous_beats = [beat for j, beat in enumerate(patient.contiguous_beats) if j not in zero_list[i]]\n","        patient.mean = [mean for j, mean in enumerate(patient.mean) if j not in zero_list[i]]\n","        patient.std = [std for j, std in enumerate(patient.std) if j not in zero_list[i]]\n","        patient.amplitude = [amplitude for j, amplitude in enumerate(patient.amplitude) if j not in zero_list[i]]\n","        patient.peak_value = [peak_value for j, peak_value in enumerate(patient.peak_value) if j not in zero_list[i]]\n","        patient.pre_PP = [pre_PP for j, pre_PP in enumerate(patient.pre_PP) if j not in zero_list[i]]\n","        patient.post_PP = [post_PP for j, post_PP in enumerate(patient.post_PP) if j not in zero_list[i]]\n","        patient.avg_PP = [avg_PP for j, avg_PP in enumerate(patient.avg_PP) if j not in zero_list[i]]\n","        patient.width = [width for j, width in enumerate(patient.width) if j not in zero_list[i]]\n","        patient.labels = [label for j, label in enumerate(patient.labels) if j not in zero_list[i]]\n","        patient.peak_locations = [peak_location for j, peak_location in enumerate(patient.peak_locations) if j not in zero_list[i]]\n","        patient.FWHM = [FWHM for j, FWHM in enumerate(patient.FWHM) if j not in zero_list[i]]\n","        patient.skewness = [skewness for j, skewness in enumerate(patient.skewness) if j not in zero_list[i]]\n","        patient.pre_skew = [pre_skew for j, pre_skew in enumerate(patient.pre_skew) if j not in zero_list[i]]\n","        patient.post_skew = [post_skew for j, post_skew in enumerate(patient.post_skew) if j not in zero_list[i]]\n","        patient.kurtosis = [kurtosis for j, kurtosis in enumerate(patient.kurtosis) if j not in zero_list[i]]\n","        patient.pre_kurt = [pre_kurt for j, pre_kurt in enumerate(patient.pre_kurt) if j not in zero_list[i]]\n","        patient.post_kurt = [post_kurt for j, post_kurt in enumerate(patient.post_kurt) if j not in zero_list[i]]\n","        patient.entropy = [entropy for j, entropy in enumerate(patient.entropy) if j not in zero_list[i]]\n","        patient.RMS = [RMS for j, RMS in enumerate(patient.RMS) if j not in zero_list[i]]\n","        patient.neg_neg_jump = [neg_neg_jump for j, neg_neg_jump in enumerate(patient.neg_neg_jump) if j not in zero_list[i]]\n","        patient.pre_pos_pos_jump = [pre_pos_pos_jump for j, pre_pos_pos_jump in enumerate(patient.pre_pos_pos_jump) if j not in zero_list[i]]\n","        patient.post_pos_pos_jump = [post_pos_pos_jump for j, post_pos_pos_jump in enumerate(patient.post_pos_pos_jump) if j not in zero_list[i]]\n","        patient.rise_time = [rise_time for j, rise_time in enumerate(patient.rise_time) if j not in zero_list[i]]\n","        patient.fall_time = [fall_time for j, fall_time in enumerate(patient.fall_time) if j not in zero_list[i]]\n","        patient.area = [area for j, area in enumerate(patient.area) if j not in zero_list[i]]\n","        patient.local_hrv = [local_hrv for j, local_hrv in enumerate(patient.local_hrv) if j not in zero_list[i]]\n","        patient.energy = [energy for j, energy in enumerate(patient.energy) if j not in zero_list[i]]\n","        patient.dominant_frequency = [dominant_frequency for j, dominant_frequency in enumerate(patient.dominant_frequency) if j not in zero_list[i]]"]},{"cell_type":"markdown","metadata":{"id":"0jToGIzybpeY"},"source":["# Signal Standardization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:44:47.546486Z","iopub.status.busy":"2024-02-01T19:44:47.545926Z","iopub.status.idle":"2024-02-01T19:45:02.740889Z","shell.execute_reply":"2024-02-01T19:45:02.739937Z","shell.execute_reply.started":"2024-02-01T19:44:47.546451Z"},"executionInfo":{"elapsed":14100,"status":"ok","timestamp":1706388805887,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"5zTXSHCB2vtd","trusted":true},"outputs":[],"source":["# Perform standardization based on the signal mean and standard deviation\n","def standardize_signals(signal):\n","    mean = np.mean(signal)\n","    std = np.std(signal)\n","    standardized_signal = (signal - mean) / std\n","    return standardized_signal\n","\n","for i, patient in enumerate(patient_instances):\n","  patient.single_beats = [standardize_signals(beat) for j, beat in enumerate(patient.single_beats)]\n","  patient.contiguous_beats = [standardize_signals(beat) for j, beat in enumerate(patient.contiguous_beats)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["j=0\n","for i, patient in enumerate(patient_instances):\n","  j=j+1\n","  if j<5:\n","    for beat in patient.single_beats[0:5]:\n","      plt.plot(range(0,len(beat)), beat, label='V PPG')\n","\n","      plt.xlabel('Samples')\n","      plt.ylabel('Amplitude')\n","      plt.legend()\n","      plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Train Test split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def calculate_class_distribution(patient_instances):\n","    \"\"\"\n","    Calculates the class distribution of the labels.\n","\n","    Args:\n","        patient_instances (list): The list of patient instances.\n","    \"\"\"\n","    tot_count_n = 0\n","    tot_count_s = 0\n","    tot_count_v = 0\n","    for patient in patient_instances:\n","        count_n, count_s, count_v = calculate_label_distribution(patient.labels)\n","        tot_count_n += count_n\n","        tot_count_s += count_s\n","        tot_count_v += count_v\n","    print(f\"Label Distribution: {tot_count_n} N beats, {tot_count_s} S beats, {tot_count_v} V beats\")\n","\n","    \n","# Check class distribution\n","calculate_class_distribution(patient_instances)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define a function to compute class proportions\n","def calculate_class_proportions(patient_instances):\n","    \"\"\"\n","    Calculates the class proportions of the labels.\n","\n","    Args:\n","        patient_instances (list): The list of patient instances.\n","    \"\"\"\n","    tot_count_n = 0\n","    tot_count_s = 0\n","    tot_count_v = 0\n","    for patient in patient_instances:\n","        count_n, count_s, count_v = calculate_label_distribution(patient.labels)\n","        tot_count_n += count_n\n","        tot_count_s += count_s\n","        tot_count_v += count_v\n","    n_ratio = tot_count_n / (tot_count_n + tot_count_v + tot_count_s)\n","    v_ratio = tot_count_v / (tot_count_n + tot_count_v + tot_count_s)\n","    s_ratio = tot_count_s / (tot_count_n + tot_count_v + tot_count_s)\n","    print(f\"Label proportions: {round(n_ratio, 4)} N beats, {round(v_ratio, 4)} V beats, {round(s_ratio, 4)} S beats\")\n","    return n_ratio, v_ratio, s_ratio\n","\n","# Check class proportions\n","n_ratio, v_ratio, s_ratio = calculate_class_proportions(patient_instances)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Initialize variables\n","n_ratio_train = 0\n","s_ratio_train = 0\n","v_ratio_train = 0\n","\n","n_ratio_val = 0\n","s_ratio_val = 0\n","v_ratio_val = 0\n","\n","n_ratio_test = 0\n","s_ratio_test = 0\n","v_ratio_test = 0\n","random_state = 999\n","max_iterations = 100\n","iteration = 0\n","\n","# Initialize variables for the best split\n","best_diff = float('inf')\n","best_split = None\n","\n","# Loop until desired conditions are met\n","while((abs(n_ratio_train - n_ratio) > 0.001 or abs(s_ratio_train - s_ratio) > 0.001 or abs(v_ratio_train - v_ratio) > 0.001 or\n","       abs(n_ratio_val - n_ratio) > 0.001 or abs(s_ratio_val - s_ratio) > 0.001 or abs(v_ratio_val - v_ratio) > 0.001 or\n","       abs(n_ratio_test - n_ratio) > 0.001 or abs(s_ratio_test - s_ratio) > 0.001 or abs(v_ratio_test - v_ratio) > 0.001) and iteration < max_iterations):\n","\n","    # Split the data into train, validation and test sets\n","    X_train_val, X_test = train_test_split(patient_instances, test_size=0.15, random_state=random_state)\n","    X_train, X_val = train_test_split(X_train_val, test_size=len(X_test), random_state=random_state)\n","\n","    # Check label distribution in train set\n","    print(\"Train set:\")\n","    calculate_class_distribution(X_train)\n","    n_ratio_train, v_ratio_train, s_ratio_train = calculate_class_proportions(X_train)\n","\n","    # Check label distribution in validation set\n","    print(\"Validation set:\")\n","    calculate_class_distribution(X_val)\n","    n_ratio_val, v_ratio_val, s_ratio_val = calculate_class_proportions(X_val)\n","\n","    # Check label distribution in test set\n","    print(\"Test set:\")\n","    calculate_class_distribution(X_test)\n","    n_ratio_test, v_ratio_test, s_ratio_test = calculate_class_proportions(X_test)\n","\n","    # Calculate the total difference between the ratios\n","    total_diff = abs(n_ratio_train - n_ratio) + abs(s_ratio_train - s_ratio) + abs(v_ratio_train - v_ratio) + \\\n","                abs(n_ratio_val - n_ratio) + abs(s_ratio_val - s_ratio) + abs(v_ratio_val - v_ratio) + \\\n","                abs(n_ratio_test - n_ratio) + abs(s_ratio_test - s_ratio) + abs(v_ratio_test - v_ratio)\n","\n","    # If this split is better than the previous best, update the best split\n","    if total_diff < best_diff:\n","        best_diff = total_diff\n","        best_split = (X_train, X_val, X_test)\n","\n","    random_state += 1\n","    iteration += 1\n","\n","# After the loop, best_split contains the best split found\n","if(iteration >= max_iterations):\n","    print(\"Max iterations reached\")\n","    X_train, X_val, X_test = best_split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check the class proportions of the sets\n","print(\"Train set:\")\n","calculate_class_proportions(X_train)\n","print(\"Validation set:\")\n","calculate_class_proportions(X_val)\n","print(\"Test set:\")\n","calculate_class_proportions(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Build train, validation and test sets\n","X_train_single_beats = [beat for patient in X_train for beat in patient.single_beats]\n","X_val_single_beats = [beat for patient in X_val for beat in patient.single_beats]\n","X_test_single_beats = [beat for patient in X_test for beat in patient.single_beats]\n","\n","X_train_contiguous_beats = [beat for patient in X_train for beat in patient.contiguous_beats]\n","X_val_contiguous_beats = [beat for patient in X_val for beat in patient.contiguous_beats]\n","X_test_contiguous_beats = [beat for patient in X_test for beat in patient.contiguous_beats]\n","\n","# Build train, validation and test labels\n","y_train = [label for patient in X_train for label in patient.labels]\n","y_val = [label for patient in X_val for label in patient.labels]\n","y_test = [label for patient in X_test for label in patient.labels]\n","\n","# Check dimensionality of train, validation and test sets\n","print(\"-> Single Beats\")\n","print(f\"Train dim.: {len(X_train_single_beats)}\")\n","print(f\"Validation dim.: {len(X_val_single_beats)}\")\n","print(f\"Test dim.: {len(X_test_single_beats)}\")\n","print(\"-> Contiguous Beats\")\n","print(f\"Train dim.: {len(X_train_contiguous_beats)}\")\n","print(f\"Validation dim.: {len(X_val_contiguous_beats)}\")\n","print(f\"Test dim.: {len(X_test_contiguous_beats)}\")\n","\n","# Check dimensionality of labels\n","print(\"-> Labels\")\n","print(f\"Train labels dim.: {len(y_train)}\")\n","print(f\"Validation labels dim.: {len(y_val)}\")\n","print(f\"Test labels dim.: {len(y_test)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def separate_short_beats(beats, labels, target_len=100):\n","    # Initialize empty lists for each label\n","    short_beats_N = []\n","    short_beats_S = []\n","    short_beats_V = []\n","\n","    # Iterate over the beats and append them to the corresponding list based on their label\n","    for i, beat in enumerate(beats):\n","        label = labels[i]\n","        if len(beat) < target_len:\n","            if label == 'N':\n","                print(f\"Beat {i} is a {len(beat)} N beat\")\n","                short_beats_N.append(i)\n","            elif label == 'S':\n","                print(f\"Beat {i} is a {len(beat)} S beat\")\n","                short_beats_S.append(i)\n","            elif label == 'V':\n","                print(f\"Beat {i} is a {len(beat)} V beat\")\n","                short_beats_V.append(i)\n","\n","    # # Print the number of short beats for each label\n","    # print(f\"Number of short 'N' beats: {len(short_beats_N)}\")\n","    # print(f\"Number of short 'S' beats: {len(short_beats_S)}\")\n","    # print(f\"Number of short 'V' beats: {len(short_beats_V)}\")\n","\n","    return short_beats_N, short_beats_S, short_beats_V\n","\n","# Check the number of short beats for each label\n","print(\"--> Single Beats\")\n","print(\"Train set:\")\n","short_beats_N_train, short_beats_S_train, short_beats_V_train = separate_short_beats(X_train_single_beats, y_train)\n","print(\"Validation set:\")\n","short_beats_N_val, short_beats_S_val, short_beats_V_val = separate_short_beats(X_val_single_beats, y_val)\n","print(\"Test set:\")\n","short_beats_N_test, short_beats_S_test, short_beats_V_test = separate_short_beats(X_test_single_beats, y_test)\n","\n","print(\"--> Contiguous Beats\")\n","print(\"Train set:\")\n","short_contiguous_beats_N_train, short_contiguous_beats_S_train, short_contiguous_beats_V_train = separate_short_beats(X_train_contiguous_beats, y_train, target_len=200)\n","print(\"Validation set:\")\n","short_contiguous_beats_N_val, short_contiguous_beats_S_val, short_contiguous_beats_V_val = separate_short_beats(X_val_contiguous_beats, y_val, target_len=200)\n","print(\"Test set:\")\n","short_contiguous_beats_N_test, short_contiguous_beats_S_test, short_contiguous_beats_V_test = separate_short_beats(X_test_contiguous_beats, y_test, target_len=200)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T19:45:02.892915Z","iopub.status.busy":"2024-02-01T19:45:02.892664Z","iopub.status.idle":"2024-02-01T19:45:02.919618Z","shell.execute_reply":"2024-02-01T19:45:02.918723Z","shell.execute_reply.started":"2024-02-01T19:45:02.892893Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706388805888,"user":{"displayName":"Giorgia Monzio Compagnoni","userId":"05946820986144194348"},"user_tz":-60},"id":"j8gE_yA9l8Oq","trusted":true},"outputs":[],"source":["# Define a function to pad the sequence by repeating its last value\n","def pad_sequence(seq, target_length):\n","    pad_size = target_length - len(seq)\n","    if pad_size <= 0:\n","        return seq\n","    else:\n","        return np.pad(seq, (0, pad_size), 'constant', constant_values=seq[-1])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Apply padding to the short V sequence in Single beats\n","X_train_beats = [pad_sequence(beat, 100) if i in short_beats_V_train else beat for i, beat in enumerate(X_train_single_beats)]\n","X_val_beats = [pad_sequence(beat, 100) if i in short_beats_V_val else beat for i, beat in enumerate(X_val_single_beats)]\n","X_test_beats = [pad_sequence(beat, 100) if i in short_beats_V_test else beat for i, beat in enumerate(X_test_single_beats)]\n","\n","# Apply padding to the short V sequence in Contiguous beats\n","X_train_beats_cont = [pad_sequence(beat, 200) if i in short_contiguous_beats_V_train else beat for i, beat in enumerate(X_train_contiguous_beats)]\n","X_val_beats_cont = [pad_sequence(beat, 200) if i in short_contiguous_beats_V_val else beat for i, beat in enumerate(X_val_contiguous_beats)]\n","X_test_beats_cont = [pad_sequence(beat, 200) if i in short_contiguous_beats_V_test else beat for i, beat in enumerate(X_test_contiguous_beats)]\n","\n","# Apply padding to the short S sequence in Contiguous beats\n","X_train_beats_cont = [pad_sequence(beat, 200) if i in short_contiguous_beats_S_train else beat for i, beat in enumerate(X_train_beats_cont)]\n","X_val_beats_cont = [pad_sequence(beat, 200) if i in short_contiguous_beats_S_val else beat for i, beat in enumerate(X_val_beats_cont)]\n","X_test_beats_cont = [pad_sequence(beat, 200) if i in short_contiguous_beats_S_test else beat for i, beat in enumerate(X_test_beats_cont)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Remove the short N beats from all the sets\n","# Single Beats\n","X_train_single = [beat for i, beat in enumerate(X_train_beats) if i not in short_beats_N_train]\n","X_val_single = [beat for i, beat in enumerate(X_val_beats) if i not in short_beats_N_val]\n","X_test_single = [beat for i, beat in enumerate(X_test_beats) if i not in short_beats_N_test]\n","# Contiguous Beats\n","X_train_contiguous = [beat for i, beat in enumerate(X_train_beats_cont) if i not in short_contiguous_beats_N_train]\n","X_val_contiguous = [beat for i, beat in enumerate(X_val_beats_cont) if i not in short_contiguous_beats_N_val]\n","X_test_contiguous = [beat for i, beat in enumerate(X_test_beats_cont) if i not in short_contiguous_beats_N_test]\n","# Single Beats labels\n","y_train_single = [label for i, label in enumerate(y_train) if i not in short_beats_N_train]\n","y_val_single = [label for i, label in enumerate(y_val) if i not in short_beats_N_val]\n","y_test_single = [label for i, label in enumerate(y_test) if i not in short_beats_N_test]\n","# Contiguous Beats labels\n","y_train_contiguous = [label for i, label in enumerate(y_train) if i not in short_contiguous_beats_N_train]\n","y_val_contiguous = [label for i, label in enumerate(y_val) if i not in short_contiguous_beats_N_val]\n","y_test_contiguous = [label for i, label in enumerate(y_test) if i not in short_contiguous_beats_N_test]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Single Beats\n","# Convert to NumPy array\n","X_train_single = np.array(X_train_single)\n","X_val_single = np.array(X_val_single)\n","X_test_single = np.array(X_test_single)\n","# Check dimensionality\n","print(\"Single Beats\")\n","print(X_train_single.shape,X_val_single.shape,X_test_single.shape)\n","# Contiguous Beats\n","# Convert to NumPy array\n","X_train_contiguous = np.array(X_train_contiguous)\n","X_val_contiguous = np.array(X_val_contiguous)\n","X_test_contiguous = np.array(X_test_contiguous)\n","# Check dimensionality\n","print(\"Contiguous Beats\")\n","print(X_train_contiguous.shape,X_val_contiguous.shape,X_test_contiguous.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Single Beats\n","# Convert to NumPy array\n","y_train_single = np.array(y_train_single)\n","y_val_single = np.array(y_val_single)\n","y_test_single = np.array(y_test_single)\n","# Check dimensionality\n","print(\"Single Beats\")\n","print(y_train_single.shape,y_val_single.shape,y_test_single.shape)\n","# Contiguous Beats\n","# Convert to NumPy array\n","y_train_contiguous = np.array(y_train_contiguous)\n","y_val_contiguous = np.array(y_val_contiguous)\n","y_test_contiguous = np.array(y_test_contiguous)\n","# Check dimensionality\n","print(\"Contiguous Beats\")\n","print(y_train_contiguous.shape,y_val_contiguous.shape,y_test_contiguous.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","import tensorflow as tf\n","# One hot encode labels\n","num_classes = 3\n","encoder = LabelEncoder()\n","\n","# Single Beats\n","y_train_single_encoded = encoder.fit_transform(y_train_single)\n","y_val_single_encoded = encoder.transform(y_val_single)\n","y_test_single_encoded = encoder.transform(y_test_single)\n","\n","y_train_single = tf.keras.utils.to_categorical(y_train_single_encoded, num_classes)\n","y_val_single = tf.keras.utils.to_categorical(y_val_single_encoded, num_classes)\n","y_test_single = tf.keras.utils.to_categorical(y_test_single_encoded, num_classes)\n","\n","# Contiguous Beats\n","y_train_contiguous_encoded = encoder.fit_transform(y_train_contiguous)\n","y_val_contiguous_encoded = encoder.transform(y_val_contiguous)\n","y_test_contiguous_encoded = encoder.transform(y_test_contiguous)\n","\n","y_train_contiguous = tf.keras.utils.to_categorical(y_train_contiguous_encoded, num_classes)\n","y_val_contiguous = tf.keras.utils.to_categorical(y_val_contiguous_encoded, num_classes)\n","y_test_contiguous = tf.keras.utils.to_categorical(y_test_contiguous_encoded, num_classes)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4339674,"sourceId":7455419,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
