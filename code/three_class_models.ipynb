{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TN9rKvunAXF"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle as pkl\n",
        "\n",
        "with open('X_train_feat', 'rb') as f:\n",
        "    X_train_feat = pkl.load(f)\n",
        "with open('X_test_feat', 'rb') as f:\n",
        "    X_test_feat = pkl.load(f)\n",
        "with open('y_train_encoded', 'rb') as f:\n",
        "    y_train_encoded = pkl.load(f)\n",
        "with open('y_test_encoded', 'rb') as f:\n",
        "    y_test_encoded = pkl.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKB6anCc0Opd"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GNxrjiv0Qko"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_confusion_matrix(model, X, y, normalize=True):\n",
        "    # Predict the values from the validation dataset\n",
        "    y_pred = model.predict(X)\n",
        "    print(f\"Number of predictions: {y_pred.shape}\")\n",
        "    # Convert predictions classes to one hot vectors\n",
        "    y_pred_classes = np.argmax(y_pred, axis = 1)\n",
        "    # Convert validation observations to one hot vectors\n",
        "    y_true = np.argmax(y, axis = 1)\n",
        "    # compute the confusion matrix\n",
        "    confusion_mtx = confusion_matrix(y_true, y_pred_classes, normalize='true' if normalize else None)\n",
        "\n",
        "    # plot the confusion matrix\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.heatmap(confusion_mtx, annot=True, fmt='.2f' if normalize else 'd', cmap='Blues')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def compute_metrics(model, X, y):\n",
        "    # Predict the values from the validation dataset\n",
        "    y_pred = model.predict(X)\n",
        "    print(f\"Number of predictions: {y_pred.shape}\")\n",
        "    # Convert predictions classes to one hot vectors\n",
        "    y_pred_classes = np.argmax(y_pred, axis = 1)\n",
        "    # Convert validation observations to one hot vectors\n",
        "    y_true = np.argmax(y, axis = 1)\n",
        "\n",
        "    # Compute and print the classification report\n",
        "    print(classification_report(y_true, y_pred_classes))\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "def plot_confusionMatrix(test_labels, test_predicted, clf):\n",
        "  cm = confusion_matrix(test_labels, test_predicted, normalize='true', labels=clf.classes_)\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                                display_labels=['N', 'S','V'])\n",
        "  disp.plot()\n",
        "  plt.show()\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Metric\n",
        "\n",
        "Our metric is designed for model selection. The idea is to select the model that is the most capable of identifying the S and V beats in order for it to be used as a diagnostic tool. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "def penalized_sensitivity_threeclass_metric(y_true, y_pred, threshold=0.9, penalty=None):\n",
        "    # Calculate recall for each class\n",
        "    recall = recall_score(y_true, y_pred, average=None)\n",
        "\n",
        "    # Compute the average of the 'S' and 'V' recalls\n",
        "    avg_recall_sv = round((recall[1] + recall[2]) / 2,2)\n",
        "\n",
        "    # If the recall for 'N' beats is below the threshold, return penalized normalized recall for 'S' and 'V'\n",
        "    if recall[0] < threshold:\n",
        "        if penalty is None:\n",
        "            penalty = recall[0]\n",
        "        return round(avg_recall_sv*penalty,2)\n",
        "\n",
        "    # Otherwise, return the normalized sum of recall for 'S' and 'V' beats\n",
        "    return avg_recall_sv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# Create a custom scorer\n",
        "custom_scorer = make_scorer(penalized_sensitivity_threeclass_metric, greater_is_better=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bayesian Search on XgBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from skopt.space import Real, Integer\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Define the parameter space\n",
        "param_space = {\n",
        "    \"gamma\": Real(0, 0.5),\n",
        "    \"learning_rate\": Real(0.01, 0.3), # default 0.1 \n",
        "    \"max_depth\": Integer(2, 6), # default 3\n",
        "    \"n_estimators\": Integer(100, 500), # default 100\n",
        "    \"subsample\": Real(0.5, 1.0),\n",
        "    \"lambda\": Real(0.5, 1.5)\n",
        "}\n",
        "\n",
        "# Initialize the XGBClassifier\n",
        "clf_xgb = xgb.XGBClassifier()\n",
        "\n",
        "# Define the cross-validation strategy\n",
        "cv = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Initialize the BayesSearchCV\n",
        "opt_xgb = BayesSearchCV(\n",
        "    clf_xgb,\n",
        "    param_space,\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    scoring=custom_scorer,\n",
        "    return_train_score=True,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "# Run Bayesian optimization over the train data\n",
        "opt_xgb.fit(X_train_feat, y_train_encoded)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params_xgb_custom = opt_xgb.best_params_\n",
        "print(best_params_xgb_custom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# As refit=True, the BayesSearch API retrains the model on the whole dataset with the set of best found parameters\n",
        "model_xgb_custom = opt_xgb.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Prediction\n",
        "y_pred_feat_train_xgb_custom = model_xgb_custom.predict(X_train_feat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assess Model performance\n",
        "plot_confusionMatrix(y_train_encoded, y_pred_feat_train_xgb_custom, model_xgb_custom)\n",
        "print(classification_report(y_train_encoded, y_pred_feat_train_xgb_custom))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Prediction\n",
        "y_pred_feat_xgb_custom = model_xgb_custom.predict(X_test_feat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assess Model performance\n",
        "plot_confusionMatrix(y_test_encoded, y_pred_feat_xgb_custom, model_xgb_custom)\n",
        "print(classification_report(y_test_encoded, y_pred_feat_xgb_custom))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bayesian Search on Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the parameter search space\n",
        "param_space = {\n",
        "    'n_estimators': (100, 500),\n",
        "    'max_depth': (3,8),\n",
        "    'min_samples_split': (4, 10),\n",
        "    'min_samples_leaf': (4, 8),\n",
        "    'max_features': (4,10),\n",
        "    'bootstrap': [True],\n",
        "    'class_weight':['balanced']\n",
        "}\n",
        "\n",
        "# Create a StratifiedKFold object\n",
        "cv = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Create the BayesSearchCV object\n",
        "opt_rf = BayesSearchCV(\n",
        "    RandomForestClassifier(),\n",
        "    param_space,\n",
        "    cv=cv,\n",
        "    n_iter=50,\n",
        "    n_jobs=-1,\n",
        "    return_train_score=True\n",
        ")\n",
        "np.int = int\n",
        "# Perform the search\n",
        "opt_rf.fit(X_train_feat, y_train_encoded)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters found: \", opt_rf.best_params_)\n",
        "print(\"Best score found: \", opt_rf.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# As refit=True, the BayesSearch API retrains the model on the whole dataset with the set of best found parameters\n",
        "model_rf_custom = opt_rf.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Prediction\n",
        "y_pred_feat_train_rf_custom = model_rf_custom.predict(X_train_feat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assess Model performance\n",
        "plot_confusionMatrix(y_train_encoded, y_pred_feat_train_rf_custom, model_rf_custom)\n",
        "print(classification_report(y_train_encoded, y_pred_feat_train_rf_custom))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Prediction\n",
        "y_pred_feat_rf_custom = model_rf_custom.predict(X_test_feat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assess Model performance\n",
        "plot_confusionMatrix(y_test_encoded, y_pred_feat_rf_custom, model_rf_custom)\n",
        "print(classification_report(y_test_encoded, y_pred_feat_rf_custom))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bayesian Search on Adaboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Define the parameter space\n",
        "param_space = {\n",
        "    \"base_estimator__max_depth\": Integer(1, 3),\n",
        "    \"n_estimators\": Integer(100, 500),\n",
        "    \"learning_rate\": Real(0.01, 1.0)\n",
        "}\n",
        "\n",
        "# Initialize the weak learner\n",
        "weak_learner = DecisionTreeClassifier(class_weight='balanced')\n",
        "\n",
        "# Initialize the AdaBoostClassifier\n",
        "ada = AdaBoostClassifier(base_estimator=weak_learner)\n",
        "\n",
        "# Define the cross-validation strategy\n",
        "cv = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Initialize the BayesSearchCV\n",
        "opt_ada = BayesSearchCV(\n",
        "    ada,\n",
        "    param_space,\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    scoring=custom_scorer,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "# Run Bayesian optimization over the train data\n",
        "opt_ada.fit(X_train_feat, y_train_encoded)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params_ada_custom = opt_ada.best_params_\n",
        "print(best_params_ada_custom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model\n",
        "model_bayes_ada = opt_ada.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Prediction\n",
        "y_pred_bayes_train_ada = model_bayes_ada.predict(X_train_feat)\n",
        "# Assess Model performance\n",
        "plot_confusionMatrix(y_train_encoded, y_pred_bayes_train_ada, model_bayes_ada)\n",
        "print(classification_report(y_train_encoded, y_pred_bayes_train_ada))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Prediction\n",
        "y_pred_bayes_ada = model_bayes_ada.predict(X_test_feat)\n",
        "# Assess Model performance\n",
        "plot_confusionMatrix(y_test_encoded, y_pred_bayes_ada, model_bayes_ada)\n",
        "print(classification_report(y_test_encoded, y_pred_bayes_ada))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bayesian Search on LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "from skopt.space import Categorical\n",
        "\n",
        "# Define the parameter space\n",
        "param_space = {\n",
        "    \"learning_rate\": Real(0.01, 1.0),\n",
        "    \"n_estimators\": Integer(50, 300),\n",
        "    \"max_depth\": Integer(1, 10),\n",
        "    \"num_leaves\": Integer(2, 50),\n",
        "    \"min_child_samples\": Integer(20, 500),\n",
        "    \"subsample\": Real(0.5, 1.0),\n",
        "    \"colsample_bytree\": Real(0.5, 1.0),\n",
        "    \"class_weight\": Categorical(['balanced'])\n",
        "}\n",
        "\n",
        "# Initialize the LGBMClassifier\n",
        "lgbm = LGBMClassifier()\n",
        "\n",
        "# Define the cross-validation strategy\n",
        "cv = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Initialize the BayesSearchCV\n",
        "opt_lgbm = BayesSearchCV(\n",
        "    lgbm,\n",
        "    param_space,\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    scoring=custom_scorer,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "# Run Bayesian optimization over the train data\n",
        "opt_lgbm.fit(X_train_feat, y_train_encoded)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params_lgbm = opt_lgbm.best_params_\n",
        "print(best_params_lgbm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model\n",
        "model_bayes_lgbm = opt_lgbm.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Prediction\n",
        "y_pred_bayes_train_lgbm = model_bayes_lgbm.predict(X_train_feat)\n",
        "# Assess Model performance\n",
        "plot_confusionMatrix(y_train_encoded, y_pred_bayes_train_lgbm, model_bayes_lgbm)\n",
        "print(classification_report(y_train_encoded, y_pred_bayes_train_lgbm))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Prediction\n",
        "y_pred_bayes_lgbm = model_bayes_lgbm.predict(X_test_feat)\n",
        "# Assess Model performance\n",
        "plot_confusionMatrix(y_test_encoded, y_pred_bayes_lgbm, model_bayes_lgbm)\n",
        "print(classification_report(y_test_encoded, y_pred_bayes_lgbm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bayesian Search on SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Define the parameter space\n",
        "param_space = {\n",
        "    \"C\": Real(1e-2, 1e+2),\n",
        "    \"gamma\": Categorical(['scale', 'auto']),\n",
        "    \"degree\": Integer(2,4),\n",
        "    \"kernel\": Categorical(['linear', 'poly', 'rbf', 'sigmoid']),\n",
        "    \"class_weight\": Categorical(['balanced'])\n",
        "}\n",
        "\n",
        "# Initialize the SVC\n",
        "svc = SVC()\n",
        "\n",
        "# Define the cross-validation strategy\n",
        "cv = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Initialize the BayesSearchCV\n",
        "opt_svc = BayesSearchCV(\n",
        "    svc,\n",
        "    param_space,\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    scoring=custom_scorer,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "# Run Bayesian optimization over the train data\n",
        "opt_svc.fit(X_train_feat, y_train_encoded)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params_svc = opt_svc.best_params_\n",
        "print(best_params_svc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model\n",
        "model_bayes_svc = opt_svc.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Prediction\n",
        "y_pred_bayes_train_svc = model_bayes_svc.predict(X_train_feat)\n",
        "# Assess Model performance\n",
        "plot_confusionMatrix(y_train_encoded, y_pred_bayes_train_svc, model_bayes_svc)\n",
        "print(classification_report(y_train_encoded, y_pred_bayes_train_svc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Prediction\n",
        "y_pred_bayes_svc = model_bayes_svc.predict(X_test_feat)\n",
        "# Assess Model performance\n",
        "plot_confusionMatrix(y_test_encoded, y_pred_bayes_svc, model_bayes_svc)\n",
        "print(classification_report(y_test_encoded, y_pred_bayes_svc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparison custom metric vs. macro sensitivity\n",
        "\n",
        "This is performed on the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, recall_score\n",
        "\n",
        "# Define the custom scorer\n",
        "macro_sensitivity = make_scorer(recall_score, average='macro')\n",
        "\n",
        "\n",
        "# Define the parameter space\n",
        "param_space = {\n",
        "    \"learning_rate\": Real(0.01, 1.0),\n",
        "    \"n_estimators\": Integer(50, 300),\n",
        "    \"max_depth\": Integer(1, 10),\n",
        "    \"num_leaves\": Integer(2, 50),\n",
        "    \"min_child_samples\": Integer(20, 500),\n",
        "    \"subsample\": Real(0.5, 1.0),\n",
        "    \"colsample_bytree\": Real(0.5, 1.0),\n",
        "    \"class_weight\": Categorical(['balanced'])\n",
        "}\n",
        "\n",
        "# Initialize the BayesSearchCV with the custom scorer\n",
        "opt_lgbm_comparison = BayesSearchCV(\n",
        "    LGBMClassifier(),\n",
        "    param_space,\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    scoring=macro_sensitivity,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "# Run Bayesian optimization over the train data\n",
        "opt_lgbm_comparison.fit(X_train_feat, y_train_encoded)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params_lgbm_comparison = opt_lgbm_comparison.best_params_\n",
        "print(best_params_lgbm_comparison)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model\n",
        "model_bayes_lgbm_comparison = opt_lgbm_comparison.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Prediction\n",
        "y_pred_bayes_train_lgbm_comparison = model_bayes_lgbm_comparison.predict(X_train_feat)\n",
        "# Assess Model performance\n",
        "plot_confusionMatrix(y_train_encoded, y_pred_bayes_train_lgbm_comparison, model_bayes_lgbm_comparison)\n",
        "print(classification_report(y_train_encoded, y_pred_bayes_train_lgbm_comparison))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Prediction\n",
        "y_pred_bayes_lgbm_comparison = model_bayes_lgbm_comparison.predict(X_test_feat)\n",
        "# Assess Model performance\n",
        "plot_confusionMatrix(y_test_encoded, y_pred_bayes_lgbm_comparison, model_bayes_lgbm_comparison)\n",
        "print(classification_report(y_test_encoded, y_pred_bayes_lgbm_comparison))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
